# -*- coding: utf-8 -*-
"""
LLM æœåŠ¡å±‚
è´Ÿè´£ LLM å®¢æˆ·ç«¯çš„åˆå§‹åŒ–å’Œç®¡ç†
"""
import httpx
from typing import Dict, Any
from llama_index.llms.openai import OpenAI
from llama_index.core.llms import LLMMetadata
from config import Settings
from utils.logger import logger


class CustomOpenAILike(OpenAI):
    """è‡ªå®šä¹‰ OpenAI å…¼å®¹ LLM å®¢æˆ·ç«¯"""

    # ä½¿ç”¨ç±»å˜é‡å­˜å‚¨æ‰€æœ‰å®ä¾‹çš„æ€è€ƒæ¨¡å¼è®¾ç½®ï¼ˆé¿å… Pydantic éªŒè¯é”™è¯¯ï¼‰
    _thinking_modes: Dict[int, bool] = {}
    
    def __init__(self, *args, **kwargs):
        """åˆå§‹åŒ–æ—¶è®¾ç½® default_headers æ¥ä¼ é€’ enable_thinking"""
        super().__init__(*args, **kwargs)
        # ğŸ”¥ å…³é”®ä¿®å¤ï¼šä¸ºæ¯ä¸ªå®ä¾‹è®¾ç½®é»˜è®¤çš„æ€è€ƒæ¨¡å¼ä¸º False
        self._thinking_modes[id(self)] = False
        # å°è¯•åœ¨åˆå§‹åŒ–æ—¶å°±è®¾ç½® extra_bodyï¼ˆå¦‚æœ OpenAI SDK æ”¯æŒï¼‰
        logger.info(f"CustomOpenAILike åˆå§‹åŒ–å®Œæˆï¼Œæ¨¡å‹: {kwargs.get('model')}, é»˜è®¤ enable_thinking=False")

    @property
    def metadata(self) -> LLMMetadata:
        return LLMMetadata(
            context_window=Settings.LLM_CONTEXT_WINDOW,
            num_output=Settings.LLM_MAX_TOKENS,
            is_chat_model=True,
            model_name=self.model,
        )

    def _get_enable_thinking(self) -> bool:
        """è·å–å½“å‰å®ä¾‹çš„æ€è€ƒæ¨¡å¼è®¾ç½®"""
        return self._thinking_modes.get(id(self), False)

    def _set_enable_thinking(self, value: bool):
        """è®¾ç½®å½“å‰å®ä¾‹çš„æ€è€ƒæ¨¡å¼"""
        self._thinking_modes[id(self)] = value

    def _get_model_kwargs(self, **kwargs: Any) -> Dict[str, Any]:
        """
        æ‹¦æˆªå¹¶å¤„ç† enable_thinking å‚æ•°
        è¿™ä¸ªæ–¹æ³•åœ¨ OpenAI åŸºç±»ä¸­è¢«è°ƒç”¨ï¼Œç”¨äºå‡†å¤‡æ¨¡å‹å‚æ•°
        """
        # æå– enable_thinking å‚æ•°
        enable_thinking = kwargs.pop("enable_thinking", None)
        if enable_thinking is not None:
            self._set_enable_thinking(enable_thinking)
            logger.info(f"âœ“ æ€è€ƒæ¨¡å¼å·²è®¾ç½®: enable_thinking={enable_thinking}")

        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•
        if hasattr(super(), '_get_model_kwargs'):
            return super()._get_model_kwargs(**kwargs)
        return {}

    def _get_completion_kwargs(self, **kwargs: Any) -> Dict[str, Any]:
        """é‡å†™ä»¥å¼ºåˆ¶è®¾ç½® max_tokensã€top_p å¹¶ç§»é™¤ stop å‚æ•°ï¼Œæ”¯æŒ enable_thinking"""
        # å…ˆæå– enable_thinkingï¼Œé¿å…ä¼ é€’ç»™çˆ¶ç±»
        enable_thinking = kwargs.pop("enable_thinking", None)
        if enable_thinking is not None:
            self._set_enable_thinking(enable_thinking)

        completion_kwargs = super()._get_completion_kwargs(**kwargs)
        completion_kwargs["max_tokens"] = Settings.LLM_MAX_TOKENS
        completion_kwargs["top_p"] = Settings.TOP_P
        completion_kwargs["top_k"] = Settings.TOP_K
        completion_kwargs.pop("stop", None)

        # è·å–å½“å‰å®ä¾‹çš„æ€è€ƒæ¨¡å¼è®¾ç½®
        current_enable_thinking = self._get_enable_thinking()

        # é€šè¿‡ additional_kwargs æˆ– extra_body ä¼ é€’ enable_thinking
        # æ³¨æ„ï¼šå§‹ç»ˆä¼ é€’è¯¥å‚æ•°ï¼ˆTrue æˆ– Falseï¼‰ï¼Œä»¥æ˜ç¡®å‘Šè¯‰å¤§æ¨¡å‹æ˜¯å¦å¼€å¯æ€è€ƒ
        # å°è¯•ä½¿ç”¨ additional_kwargsï¼ˆllama-index çš„æ–¹å¼ï¼‰
        if "additional_kwargs" not in completion_kwargs:
            completion_kwargs["additional_kwargs"] = {}
        completion_kwargs["additional_kwargs"]["enable_thinking"] = current_enable_thinking

        # åŒæ—¶å°è¯• extra_bodyï¼ˆOpenAI SDK 1.0+ çš„æ–¹å¼ï¼‰
        if "extra_body" not in completion_kwargs:
            completion_kwargs["extra_body"] = {}
        completion_kwargs["extra_body"]["enable_thinking"] = current_enable_thinking

        logger.info(f"âœ“ å·²å°† enable_thinking={current_enable_thinking} æ·»åŠ åˆ°è¯·æ±‚å‚æ•°")

        return completion_kwargs

    def _get_chat_kwargs(self, **kwargs: Any) -> Dict[str, Any]:
        """é‡å†™ä»¥å¼ºåˆ¶è®¾ç½® max_tokensã€top_p å¹¶ç§»é™¤ stop å‚æ•°ï¼Œæ”¯æŒ enable_thinking"""
        # å…ˆæå– enable_thinkingï¼Œé¿å…ä¼ é€’ç»™çˆ¶ç±»
        enable_thinking = kwargs.pop("enable_thinking", None)
        if enable_thinking is not None:
            self._set_enable_thinking(enable_thinking)

        chat_kwargs = super()._get_chat_kwargs(**kwargs)
        chat_kwargs["max_tokens"] = Settings.LLM_MAX_TOKENS
        chat_kwargs["top_p"] = Settings.TOP_P
        chat_kwargs["top_k"] = Settings.TOP_K
        chat_kwargs.pop("stop", None)

        # è·å–å½“å‰å®ä¾‹çš„æ€è€ƒæ¨¡å¼è®¾ç½®
        current_enable_thinking = self._get_enable_thinking()

        # é€šè¿‡ additional_kwargs æˆ– extra_body ä¼ é€’ enable_thinking
        # æ³¨æ„ï¼šå§‹ç»ˆä¼ é€’è¯¥å‚æ•°ï¼ˆTrue æˆ– Falseï¼‰ï¼Œä»¥æ˜ç¡®å‘Šè¯‰å¤§æ¨¡å‹æ˜¯å¦å¼€å¯æ€è€ƒ
        # å°è¯•ä½¿ç”¨ additional_kwargsï¼ˆllama-index çš„æ–¹å¼ï¼‰
        if "additional_kwargs" not in chat_kwargs:
            chat_kwargs["additional_kwargs"] = {}
        chat_kwargs["additional_kwargs"]["enable_thinking"] = current_enable_thinking

        # åŒæ—¶å°è¯• extra_bodyï¼ˆOpenAI SDK 1.0+ çš„æ–¹å¼ï¼‰
        if "extra_body" not in chat_kwargs:
            chat_kwargs["extra_body"] = {}
        chat_kwargs["extra_body"]["enable_thinking"] = current_enable_thinking

        logger.info(f"âœ“ å·²å°† enable_thinking={current_enable_thinking} æ·»åŠ åˆ°è¯·æ±‚å‚æ•°")

        return chat_kwargs


class LLMService:
    """LLM æœåŠ¡ç®¡ç†å™¨"""

    def __init__(self):
        self.clients: Dict[str, CustomOpenAILike] = {}
        self.http_client = None

    def initialize(self) -> Dict[str, CustomOpenAILike]:
        """
        åˆå§‹åŒ–æ‰€æœ‰ LLM å®¢æˆ·ç«¯

        Returns:
            æ¨¡å‹ ID åˆ° LLM å®¢æˆ·ç«¯çš„æ˜ å°„
        """
        logger.info("åˆå§‹åŒ– LLM å®¢æˆ·ç«¯...")

        self.http_client = httpx.Client(
            verify=False,
            timeout=Settings.LLM_REQUEST_TIMEOUT
        )

        for model_id, config in Settings.LLM_ENDPOINTS.items():
            logger.info(
                f"åˆ›å»ºæ¨¡å‹å®¢æˆ·ç«¯: '{model_id}' "
                f"(API: {config['api_base_url']})"
            )

            api_key = config.get("access_token") or "not-needed"

            llm_client = CustomOpenAILike(
                model=config["llm_model_name"],
                api_key=api_key,
                api_base=config["api_base_url"],
                http_client=self.http_client,
                is_chat_model=True,
                context_window=Settings.LLM_CONTEXT_WINDOW
            )

            self.clients[model_id] = llm_client
            logger.info(
                f"æ¨¡å‹ '{model_id}' ä¸Šä¸‹æ–‡çª—å£: "
                f"{Settings.LLM_CONTEXT_WINDOW}"
            )

        logger.info(f"æˆåŠŸåˆå§‹åŒ– {len(self.clients)} ä¸ª LLM å®¢æˆ·ç«¯")
        return self.clients

    def get_client(self, model_id: str) -> CustomOpenAILike:
        """
        è·å–æŒ‡å®šçš„ LLM å®¢æˆ·ç«¯

        Args:
            model_id: æ¨¡å‹ ID

        Returns:
            LLM å®¢æˆ·ç«¯å®ä¾‹

        Raises:
            KeyError: å¦‚æœæ¨¡å‹ ID ä¸å­˜åœ¨
        """
        if model_id not in self.clients:
            logger.warning(
                f"è¯·æ±‚çš„æ¨¡å‹ '{model_id}' ä¸å­˜åœ¨ï¼Œ"
                f"ä½¿ç”¨é»˜è®¤æ¨¡å‹ '{Settings.DEFAULT_LLM_ID}'"
            )
            model_id = Settings.DEFAULT_LLM_ID

        return self.clients[model_id]
