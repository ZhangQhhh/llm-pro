# -*- coding: utf-8 -*-
"""
å¯¹è¯ç®¡ç†å™¨
è´Ÿè´£å¤šè½®å¯¹è¯çš„å­˜å‚¨ã€æ£€ç´¢å’Œä¸Šä¸‹æ–‡æ„å»º
"""
from typing import List, Dict, Optional
from datetime import datetime, timedelta
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct, Filter, FieldCondition, MatchValue, FilterSelector, Range
from llama_index.core.embeddings import BaseEmbedding
from llama_index.core.llms import ChatMessage, MessageRole
from config import Settings as AppSettings
from utils.logger import logger
from prompts import (
    get_conversation_summary_system,
    get_conversation_summary_user,
    get_conversation_summary_context_prefix
)
import uuid
import time


class ConversationManager:
    """å¤šè½®å¯¹è¯ç®¡ç†å™¨"""

    def __init__(self, embed_model: BaseEmbedding, qdrant_client: QdrantClient, llm_client=None):
        self.embed_model = embed_model
        self.qdrant_client = qdrant_client
        self.llm_client = llm_client  # ç”¨äºå¯¹è¯æ€»ç»“çš„ LLM å®¢æˆ·ç«¯
        self.collection_name = AppSettings.CONVERSATION_COLLECTION

        # ç¼“å­˜æœ€è¿‘å¯¹è¯ï¼ˆå‡å°‘ Qdrant æŸ¥è¯¢ï¼‰
        self._recent_cache = {}  # {session_id: {"conversations": [...], "timestamp": float}}
        self._cache_ttl = 300  # ç¼“å­˜æœ‰æ•ˆæœŸ 5 åˆ†é’Ÿ

        # ç¼“å­˜å†å²å¯¹è¯æ€»ç»“
        self._summary_cache = {}  # {session_id: {"summary": str, "summarized_until": int, "timestamp": float}}
        self._summary_cache_ttl = 600  # æ€»ç»“ç¼“å­˜æœ‰æ•ˆæœŸ 10 åˆ†é’Ÿ

        # ç¡®ä¿å¯¹è¯é›†åˆå­˜åœ¨
        self._ensure_collection()

    def _ensure_collection(self):
        """ç¡®ä¿ Qdrant å¯¹è¯é›†åˆå­˜åœ¨"""
        try:
            collections = self.qdrant_client.get_collections().collections
            collection_exists = any(c.name == self.collection_name for c in collections)

            if not collection_exists:
                logger.warning(f"å¯¹è¯é›†åˆ {self.collection_name} ä¸å­˜åœ¨ï¼Œæ­£åœ¨åˆ›å»º...")
                # è·å– embedding ç»´åº¦
                test_embedding = self.embed_model.get_text_embedding("test")
                vector_size = len(test_embedding)

                self.qdrant_client.create_collection(
                    collection_name=self.collection_name,
                    vectors_config=VectorParams(
                        size=vector_size,
                        distance=Distance.COSINE
                    )
                )
                logger.info(f" æˆåŠŸåˆ›å»ºå¯¹è¯é›†åˆ {self.collection_name}ï¼ˆç»´åº¦: {vector_size}ï¼‰")

            return True
        except Exception as e:
            logger.error(f" åˆ›å»ºå¯¹è¯é›†åˆå¤±è´¥: {e}", exc_info=True)
            raise  # æŠ›å‡ºå¼‚å¸¸è€Œä¸æ˜¯é™é»˜å¤±è´¥

    def _check_and_create_collection(self):
        """æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»ºï¼ˆç”¨äºè¿è¡Œæ—¶æ£€æŸ¥ï¼‰"""
        try:
            # å°è¯•è·å–é›†åˆä¿¡æ¯
            self.qdrant_client.get_collection(self.collection_name)
            return True
        except Exception:
            # é›†åˆä¸å­˜åœ¨ï¼Œå°è¯•åˆ›å»º
            logger.warning(f"æ£€æµ‹åˆ°é›†åˆ {self.collection_name} ä¸å­˜åœ¨ï¼Œå°è¯•é‡æ–°åˆ›å»º...")
            return self._ensure_collection()

    def add_conversation_turn(
        self,
        session_id: str,
        user_query: str,
        assistant_response: str,
        context_docs: Optional[List[str]] = None,
        turn_id: Optional[str] = None,
        parent_turn_id: Optional[str] = None
    ):
        """
        å­˜å‚¨ä¸€è½®å¯¹è¯åˆ°å‘é‡åº“

        Args:
            session_id: ä¼šè¯ID
            user_query: ç”¨æˆ·é—®é¢˜
            assistant_response: åŠ©æ‰‹å›ç­”
            context_docs: ä½¿ç”¨çš„ä¸Šä¸‹æ–‡æ–‡æ¡£(å¯é€‰)
            turn_id: å¯¹è¯è½®æ¬¡ID(å¯é€‰ï¼Œç”¨äºå¯¹è¯åˆ†æ”¯)
            parent_turn_id: çˆ¶å¯¹è¯è½®æ¬¡ID(å¯é€‰ï¼Œç”¨äºå¯¹è¯åˆ†æ”¯)
        """
        start_time = time.time()

        try:
            # ç¡®ä¿é›†åˆå­˜åœ¨
            self._check_and_create_collection()

            # æ„å»ºå¯¹è¯æ–‡æœ¬(ç”¨äºå‘é‡åŒ–)
            conversation_text = f"ç”¨æˆ·: {user_query}\nåŠ©æ‰‹: {assistant_response}"

            # ç»Ÿè®¡ token æ•°é‡ï¼ˆç²—ç•¥ä¼°ç®—ï¼šä¸­æ–‡æŒ‰å­—ç¬¦æ•°ï¼Œè‹±æ–‡æŒ‰ç©ºæ ¼åˆ†è¯ï¼‰
            token_count = len(user_query) + len(assistant_response)

            # ç”Ÿæˆ embedding
            embedding_start = time.time()
            embedding = self.embed_model.get_text_embedding(conversation_text)
            embedding_time = time.time() - embedding_start

            # æ„å»º payloadï¼ˆåŒ…å«ç›‘æ§å­—æ®µå’Œåˆ†æ”¯å­—æ®µï¼‰
            payload = {
                "session_id": session_id,
                "user_query": user_query,
                "assistant_response": assistant_response,
                "timestamp": datetime.now().isoformat(),
                "context_docs": context_docs or [],
                "token_count": token_count,
                "turn_id": turn_id or str(uuid.uuid4()),  # è‡ªåŠ¨ç”Ÿæˆæˆ–ä½¿ç”¨æä¾›çš„
                "parent_turn_id": parent_turn_id or None  # ç”¨äºå¯¹è¯åˆ†æ”¯
            }

            # å­˜å‚¨åˆ° Qdrant
            point_id = str(uuid.uuid4())
            self.qdrant_client.upsert(
                collection_name=self.collection_name,
                points=[
                    PointStruct(
                        id=point_id,
                        vector=embedding,
                        payload=payload
                    )
                ]
            )

            # æ¸…é™¤è¯¥ä¼šè¯çš„ç¼“å­˜
            if session_id in self._recent_cache:
                del self._recent_cache[session_id]

            total_time = time.time() - start_time

            logger.info(
                f"ä¼šè¯ {session_id} å¯¹è¯å·²å­˜å‚¨ | "
                f"Tokenæ•°: {token_count} | "
                f"Embeddingè€—æ—¶: {embedding_time:.2f}s | "
                f"æ€»è€—æ—¶: {total_time:.2f}s"
            )

            # å¼‚å¸¸æ£€æµ‹ï¼šå¦‚æœ token æ•°è¿‡å¤šï¼Œå‘å‡ºè­¦å‘Š
            if token_count > 4000:
                logger.warning(
                    f"âš ï¸ ä¼šè¯ {session_id} å•è½®å¯¹è¯ token æ•°è¿‡å¤š: {token_count}ï¼Œ"
                    f"å¯èƒ½å¯¼è‡´ä¸Šä¸‹æ–‡è¶…é™"
                )

        except Exception as e:
            logger.error(f"å­˜å‚¨å¯¹è¯å¤±è´¥: {e}", exc_info=True)

    def retrieve_relevant_history(
        self,
        session_id: str,
        current_query: str,
        top_k: int = 3
    ) -> List[Dict]:
        """
        æ£€ç´¢ç›¸å…³å†å²å¯¹è¯ï¼ˆä¼˜åŒ–ç‰ˆï¼‰

        æ”¹è¿›ç‚¹ï¼š
        1. åªç”¨ç”¨æˆ·é—®é¢˜çš„æ–‡æœ¬ç”Ÿæˆå‘é‡ï¼ˆä¸åŒ…å«åŠ©æ‰‹å›ç­”ï¼‰
        2. æ·»åŠ æ—¶é—´è¡°å‡æƒé‡ï¼ˆè¶Šè¿‘çš„å¯¹è¯æƒé‡è¶Šé«˜ï¼‰
        3. å¢åŠ å…³é”®è¯åŒ¹é…ä½œä¸ºè¡¥å……

        Args:
            session_id: ä¼šè¯ID
            current_query: å½“å‰é—®é¢˜
            top_k: æ£€ç´¢æ•°é‡

        Returns:
            ç›¸å…³å¯¹è¯åˆ—è¡¨
        """
        start_time = time.time()

        try:
            # ç”ŸæˆæŸ¥è¯¢ embedding
            query_embedding = self.embed_model.get_text_embedding(current_query)

            # å‘é‡æ£€ç´¢(ä»…é™å½“å‰ä¼šè¯) - å¢åŠ æ£€ç´¢æ•°é‡ä»¥ä¾¿åç»­è¿‡æ»¤
            search_limit = top_k * 2  # æ£€ç´¢2å€æ•°é‡ï¼Œåç»­æ ¹æ®æ—¶é—´è¡°å‡ç­›é€‰
            search_result = self.qdrant_client.search(
                collection_name=self.collection_name,
                query_vector=query_embedding,
                query_filter={
                    "must": [
                        {"key": "session_id", "match": {"value": session_id}}
                    ]
                },
                limit=search_limit,
                with_payload=True
            )

            # æå–ç›¸å…³å¯¹è¯å¹¶æ·»åŠ æ—¶é—´è¡°å‡æƒé‡
            relevant_history = []
            current_time = datetime.now()

            for hit in search_result:
                timestamp_str = hit.payload["timestamp"]
                timestamp = datetime.fromisoformat(timestamp_str)

                # è®¡ç®—æ—¶é—´å·®ï¼ˆå°æ—¶ï¼‰
                time_diff_hours = (current_time - timestamp).total_seconds() / 3600

                # æ—¶é—´è¡°å‡å› å­ï¼š24å°æ—¶å†…æƒé‡1.0ï¼Œä¹‹åæ¯24å°æ—¶è¡°å‡0.1
                time_decay = max(0.5, 1.0 - (time_diff_hours / 24) * 0.1)

                # ç»¼åˆå¾—åˆ† = å‘é‡ç›¸ä¼¼åº¦ * æ—¶é—´è¡°å‡
                adjusted_score = hit.score * time_decay

                relevant_history.append({
                    "user_query": hit.payload["user_query"],
                    "assistant_response": hit.payload["assistant_response"],
                    "timestamp": timestamp_str,
                    "score": hit.score,  # åŸå§‹ç›¸ä¼¼åº¦åˆ†æ•°
                    "adjusted_score": adjusted_score,  # è°ƒæ•´åçš„åˆ†æ•°
                    "turn_id": hit.payload.get("turn_id"),
                    "time_diff_hours": round(time_diff_hours, 2)
                })

            # æŒ‰è°ƒæ•´åçš„åˆ†æ•°æ’åºï¼Œå–top_k
            relevant_history.sort(key=lambda x: x["adjusted_score"], reverse=True)
            relevant_history = relevant_history[:top_k]

            elapsed_time = time.time() - start_time
            logger.info(
                f"æ£€ç´¢åˆ° {len(relevant_history)} æ¡ç›¸å…³å†å²ï¼ˆåº”ç”¨æ—¶é—´è¡°å‡ï¼‰ | "
                f"è€—æ—¶: {elapsed_time:.2f}s"
            )

            # è°ƒè¯•æ—¥å¿—ï¼šæ˜¾ç¤ºè°ƒæ•´åçš„åˆ†æ•°
            if relevant_history:
                logger.debug(
                    f"ç›¸å…³å†å²å¾—åˆ†ï¼ˆå‰3æ¡ï¼‰: " +
                    ", ".join([
                        f"{h['adjusted_score']:.3f}(åŸ:{h['score']:.3f},æ—¶å·®:{h['time_diff_hours']}h)"
                        for h in relevant_history[:3]
                    ])
                )

            return relevant_history

        except Exception as e:
            logger.error(f"æ£€ç´¢å†å²å¯¹è¯å¤±è´¥: {e}", exc_info=True)
            return []

    def get_recent_history(
        self,
        session_id: str,
        limit: int = 5
    ) -> List[Dict]:
        """
        è·å–æœ€è¿‘ N è½®å¯¹è¯ï¼ˆå¸¦ç¼“å­˜ä¼˜åŒ–ï¼‰

        Args:
            session_id: ä¼šè¯ID
            limit: è·å–æ•°é‡

        Returns:
            æœ€è¿‘å¯¹è¯åˆ—è¡¨(æŒ‰æ—¶é—´å‡åº)
        """
        # æ£€æŸ¥ç¼“å­˜
        current_time = time.time()
        if session_id in self._recent_cache:
            cache_entry = self._recent_cache[session_id]
            if current_time - cache_entry["timestamp"] < self._cache_ttl:
                logger.debug(f"ä½¿ç”¨ç¼“å­˜çš„æœ€è¿‘å¯¹è¯ (session: {session_id})")
                return cache_entry["conversations"][:limit]

        try:
            # ç¡®ä¿é›†åˆå­˜åœ¨
            self._check_and_create_collection()

            # ä½¿ç”¨ scroll è·å–æ‰€æœ‰å¯¹è¯,ç„¶åæŒ‰æ—¶é—´æ’åº
            scroll_result = self.qdrant_client.scroll(
                collection_name=self.collection_name,
                scroll_filter={
                    "must": [
                        {"key": "session_id", "match": {"value": session_id}}
                    ]
                },
                limit=100,  # å‡è®¾å•ä¼šè¯ä¸è¶…è¿‡100è½®
                with_payload=True
            )

            # æå–å¹¶æ’åº
            all_turns = []
            total_tokens = 0
            for point in scroll_result[0]:
                all_turns.append({
                    "user_query": point.payload["user_query"],
                    "assistant_response": point.payload["assistant_response"],
                    "timestamp": point.payload["timestamp"],
                    "turn_id": point.payload.get("turn_id"),  # æ·»åŠ  turn_id
                    "parent_turn_id": point.payload.get("parent_turn_id")  # æ·»åŠ  parent_turn_id
                })
                total_tokens += point.payload.get("token_count", 0)

            # æŒ‰æ—¶é—´é™åºæ’åº,å–æœ€è¿‘çš„ limit æ¡
            recent_turns = sorted(
                all_turns,
                key=lambda x: x["timestamp"],
                reverse=True
            )[:limit]

            # åè½¬ä¸ºæ—¶é—´å‡åº(æ—§â†’æ–°)
            recent_turns.reverse()

            # æ›´æ–°ç¼“å­˜
            self._recent_cache[session_id] = {
                "conversations": recent_turns,
                "timestamp": current_time
            }

            logger.info(
                f"è·å–åˆ° {len(recent_turns)} æ¡æœ€è¿‘å¯¹è¯ | "
                f"ä¼šè¯æ€»è½®æ¬¡: {len(all_turns)} | "
                f"ç´¯è®¡Tokenæ•°: {total_tokens}"
            )

            # å¦‚æœç´¯è®¡ token æ•°è¿‡å¤šï¼Œå‘å‡ºè­¦å‘Š
            if total_tokens > 10000:
                logger.warning(
                    f"âš ï¸ ä¼šè¯ {session_id} ç´¯è®¡ token æ•°è¿‡å¤š: {total_tokens}ï¼Œ"
                    f"å»ºè®®è€ƒè™‘æ¸…ç†å†å²æˆ–å¢åŠ æ‘˜è¦æœºåˆ¶"
                )

            return recent_turns

        except Exception as e:
            logger.error(f"è·å–æœ€è¿‘å¯¹è¯å¤±è´¥: {e}", exc_info=True)
            return []

    def summarize_old_conversations(
        self,
        session_id: str,
        conversations: List[Dict],
        context_docs: Optional[List[str]] = None
    ) -> Optional[str]:
        """
        ä½¿ç”¨ LLM æ€»ç»“æ—§çš„å¯¹è¯å†å²

        Args:
            session_id: ä¼šè¯ID
            conversations: éœ€è¦æ€»ç»“çš„å¯¹è¯åˆ—è¡¨
            context_docs: å¯¹è¯ä¸­ä½¿ç”¨çš„ä¸Šä¸‹æ–‡æ–‡æ¡£åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰

        Returns:
            æ€»ç»“æ–‡æœ¬ï¼Œå¦‚æœæ€»ç»“å¤±è´¥è¿”å› None
        """
        if not self.llm_client:
            logger.warning("æœªæä¾› LLM å®¢æˆ·ç«¯ï¼Œæ— æ³•æ€»ç»“å¯¹è¯å†å²")
            return None

        if not conversations:
            return None

        try:
            start_time = time.time()

            # æ„å»ºå¯¹è¯å†å²æ–‡æœ¬
            conversation_text = ""
            for idx, conv in enumerate(conversations, 1):
                conversation_text += f"ç¬¬{idx}è½®ï¼š\n"
                conversation_text += f"ç”¨æˆ·: {conv['user_query']}\n"
                conversation_text += f"åŠ©æ‰‹: {conv['assistant_response']}\n"
                # å¦‚æœæœ‰ä¸Šä¸‹æ–‡æ–‡æ¡£ä¿¡æ¯ï¼Œä¹ŸåŒ…å«è¿›æ¥
                if conv.get('context_docs'):
                    conversation_text += f"(å‚è€ƒæ–‡æ¡£: {len(conv['context_docs'])}ä¸ª)\n"
                conversation_text += "\n"

            # æ„å»ºæ€»ç»“æç¤ºè¯
            system_prompt = '\n'.join(get_conversation_summary_system())
            user_prompt_template = '\n'.join(get_conversation_summary_user())
            user_prompt = user_prompt_template.format(conversation_history=conversation_text)

            # æ„å»ºæ¶ˆæ¯
            messages = [
                ChatMessage(role=MessageRole.SYSTEM, content=system_prompt),
                ChatMessage(role=MessageRole.USER, content=user_prompt)
            ]

            # è°ƒç”¨ LLM ç”Ÿæˆæ€»ç»“
            logger.info(f"æ­£åœ¨æ€»ç»“ä¼šè¯ {session_id} çš„ {len(conversations)} è½®å¯¹è¯...")
            response = self.llm_client.chat(messages)
            summary = response.message.content.strip()

            elapsed_time = time.time() - start_time
            logger.info(
                f"âœ… ä¼šè¯ {session_id} å¯¹è¯æ€»ç»“å®Œæˆ | "
                f"åŸå¯¹è¯è½®æ•°: {len(conversations)} | "
                f"æ€»ç»“é•¿åº¦: {len(summary)} å­— | "
                f"è€—æ—¶: {elapsed_time:.2f}s"
            )

            return summary

        except Exception as e:
            logger.error(f"æ€»ç»“å¯¹è¯å†å²å¤±è´¥: {e}", exc_info=True)
            return None

    def build_context_messages(
        self,
        session_id: str,
        current_query: str,
        system_prompt: str,
        knowledge_context: Optional[str] = None,
        context_prefixes: Optional[Dict[str, str]] = None,
        recent_turns: int = 3,
        relevant_turns: int = 2,
        enable_summary: bool = True
    ) -> List[Dict[str, str]]:
        """
        æ„å»ºå®Œæ•´çš„ä¸Šä¸‹æ–‡ messages æ•°ç»„

        æ–°å¢åŠŸèƒ½ï¼šå½“å†å²å¯¹è¯è¶…è¿‡ recent_turns æ—¶ï¼Œè‡ªåŠ¨æ€»ç»“æ—§å¯¹è¯å¹¶æ³¨å…¥ä¸Šä¸‹æ–‡

        Args:
            session_id: ä¼šè¯ID
            current_query: å½“å‰ç”¨æˆ·é—®é¢˜
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            knowledge_context: çŸ¥è¯†åº“æ£€ç´¢çš„ä¸Šä¸‹æ–‡(å¯é€‰)
            context_prefixes: ä¸Šä¸‹æ–‡å‰ç¼€å­—å…¸(å¯é€‰)
            recent_turns: ä¿ç•™çš„æœ€è¿‘å¯¹è¯è½®æ•°ï¼ˆé»˜è®¤3è½®ï¼‰
            relevant_turns: æ£€ç´¢çš„ç›¸å…³å¯¹è¯è½®æ•°
            enable_summary: æ˜¯å¦å¯ç”¨å†å²å¯¹è¯æ€»ç»“åŠŸèƒ½ï¼ˆé»˜è®¤å¯ç”¨ï¼‰

        Returns:
            messages æ•°ç»„
        """
        messages = [{"role": "system", "content": system_prompt}]

        # é»˜è®¤å‰ç¼€
        if context_prefixes is None:
            context_prefixes = {
                "relevant_history": "ä»¥ä¸‹æ˜¯ç›¸å…³çš„å†å²å¯¹è¯ï¼Œå¯ä½œä¸ºèƒŒæ™¯å‚è€ƒï¼š\n",
                "recent_history": "ä»¥ä¸‹æ˜¯æœ€è¿‘çš„å¯¹è¯å†å²ï¼š\n",
                "regulations": "ä¸šåŠ¡è§„å®šå¦‚ä¸‹ï¼š\n",
                "summary": get_conversation_summary_context_prefix()
            }

        # 1. è·å–æ‰€æœ‰å†å²å¯¹è¯
        try:
            scroll_result = self.qdrant_client.scroll(
                collection_name=self.collection_name,
                scroll_filter={
                    "must": [
                        {"key": "session_id", "match": {"value": session_id}}
                    ]
                },
                limit=100,
                with_payload=True
            )

            all_conversations = []
            for point in scroll_result[0]:
                all_conversations.append({
                    "user_query": point.payload["user_query"],
                    "assistant_response": point.payload["assistant_response"],
                    "timestamp": point.payload["timestamp"],
                    "context_docs": point.payload.get("context_docs", [])
                })

            # æŒ‰æ—¶é—´æ’åº
            all_conversations.sort(key=lambda x: x["timestamp"])

            total_turns = len(all_conversations)
            logger.info(f"ä¼šè¯ {session_id} å…±æœ‰ {total_turns} è½®å¯¹è¯")

        except Exception as e:
            logger.error(f"è·å–å†å²å¯¹è¯å¤±è´¥: {e}", exc_info=True)
            all_conversations = []
            total_turns = 0

        # 2. å¦‚æœå†å²å¯¹è¯è¶…è¿‡ recent_turns ä¸”å¯ç”¨æ€»ç»“ï¼Œåˆ™æ€»ç»“æ—§å¯¹è¯
        old_conversation_summary = None
        if enable_summary and total_turns > recent_turns:
            old_conversations = all_conversations[:-recent_turns]  # æ—§å¯¹è¯

            # æ£€æŸ¥æ€»ç»“ç¼“å­˜
            current_time = time.time()
            cache_key = session_id
            if cache_key in self._summary_cache:
                cache_entry = self._summary_cache[cache_key]
                # å¦‚æœç¼“å­˜æœ‰æ•ˆä¸”æ€»ç»“çš„å¯¹è¯æ•°é‡ä¸€è‡´ï¼Œä½¿ç”¨ç¼“å­˜
                if (current_time - cache_entry["timestamp"] < self._summary_cache_ttl and
                    cache_entry["summarized_until"] == len(old_conversations)):
                    old_conversation_summary = cache_entry["summary"]
                    logger.info(f"ä½¿ç”¨ç¼“å­˜çš„å¯¹è¯æ€»ç»“ (session: {session_id})")

            # å¦‚æœæ²¡æœ‰ç¼“å­˜æˆ–ç¼“å­˜å¤±æ•ˆï¼Œç”Ÿæˆæ–°æ€»ç»“
            if not old_conversation_summary:
                old_conversation_summary = self.summarize_old_conversations(
                    session_id=session_id,
                    conversations=old_conversations
                )

                # æ›´æ–°ç¼“å­˜
                if old_conversation_summary:
                    self._summary_cache[cache_key] = {
                        "summary": old_conversation_summary,
                        "summarized_until": len(old_conversations),
                        "timestamp": current_time
                    }

        # 3. è·å–æœ€è¿‘å¯¹è¯
        recent_history = all_conversations[-recent_turns:] if total_turns > 0 else []

        # 4. è·å–å‘é‡æ£€ç´¢çš„ç›¸å…³å†å²ï¼ˆæ’é™¤æœ€è¿‘å¯¹è¯ï¼‰
        relevant_history = self.retrieve_relevant_history(
            session_id,
            current_query,
            top_k=relevant_turns
        )

        # å»é‡ï¼šæ’é™¤å·²åœ¨æœ€è¿‘å¯¹è¯ä¸­çš„å†…å®¹
        recent_queries = {h["user_query"] for h in recent_history}
        unique_relevant = [
            h for h in relevant_history
            if h["user_query"] not in recent_queries
        ]

        # 5. æ„å»º messages
        # 5.1 å…ˆåŠ å…¥å†å²å¯¹è¯æ€»ç»“ï¼ˆå¦‚æœæœ‰ï¼‰
        if old_conversation_summary:
            messages.append({
                "role": "system",
                "content": context_prefixes.get("summary", "") + old_conversation_summary
            })
            logger.info(f"å·²æ³¨å…¥å†å²å¯¹è¯æ€»ç»“åˆ°ä¸Šä¸‹æ–‡ (æ€»ç»“äº† {total_turns - recent_turns} è½®å¯¹è¯)")

        # 5.2 åŠ å…¥ç›¸å…³å†å²(å¦‚æœæœ‰)
        if unique_relevant:
            messages.append({
                "role": "system",
                "content": context_prefixes["relevant_history"]
            })
            for turn in unique_relevant:
                messages.append({
                    "role": "user",
                    "content": turn["user_query"]
                })
                messages.append({
                    "role": "assistant",
                    "content": turn["assistant_response"]
                })

        # 5.3 åŠ å…¥æœ€è¿‘å¯¹è¯
        if recent_history:
            messages.append({
                "role": "system",
                "content": context_prefixes["recent_history"]
            })
            for turn in recent_history:
                messages.append({
                    "role": "user",
                    "content": turn["user_query"]
                })
                messages.append({
                    "role": "assistant",
                    "content": turn["assistant_response"]
                })

        # 5.4 åŠ å…¥çŸ¥è¯†åº“ä¸Šä¸‹æ–‡(å¦‚æœæœ‰)
        if knowledge_context:
            messages.append({
                "role": "system",
                "content": context_prefixes["regulations"] + knowledge_context
            })

        # 5.5 æœ€ååŠ å…¥å½“å‰é—®é¢˜
        messages.append({
            "role": "user",
            "content": current_query
        })

        logger.info(
            f"ä¸Šä¸‹æ–‡æ„å»ºå®Œæˆ | "
            f"æ€»æ¶ˆæ¯æ•°: {len(messages)} | "
            f"åŒ…å«æ€»ç»“: {'æ˜¯' if old_conversation_summary else 'å¦'} | "
            f"æœ€è¿‘å¯¹è¯: {len(recent_history)}è½® | "
            f"ç›¸å…³å†å²: {len(unique_relevant)}è½®"
        )

        return messages

    def clear_session(self, session_id: str) -> bool:
        """
        æ¸…ç©ºæŒ‡å®šä¼šè¯çš„æ‰€æœ‰å†å²å¯¹è¯

        Args:
            session_id: ä¼šè¯ID

        Returns:
            æ˜¯å¦æˆåŠŸæ¸…ç©º
        """
        try:
            logger.info(f"å¼€å§‹æ¸…ç©ºä¼šè¯ {session_id} çš„å†å²å¯¹è¯...")

            # åˆ é™¤ Qdrant ä¸­è¯¥ä¼šè¯çš„æ‰€æœ‰ç‚¹
            self.qdrant_client.delete(
                collection_name=self.collection_name,
                points_selector=FilterSelector(
                    filter=Filter(
                        must=[
                            FieldCondition(
                                key="session_id",
                                match=MatchValue(value=session_id)
                            )
                        ]
                    )
                )
            )

            # æ¸…é™¤ç¼“å­˜
            if session_id in self._recent_cache:
                del self._recent_cache[session_id]

            logger.info(f"âœ… ä¼šè¯ {session_id} å†å²å¯¹è¯å·²æ¸…ç©º")
            return True

        except Exception as e:
            logger.error(f"æ¸…ç©ºä¼šè¯å¤±è´¥: {e}", exc_info=True)
            return False

    def get_session_statistics(self, session_id: str) -> Dict:
        """
        è·å–ä¼šè¯ç»Ÿè®¡ä¿¡æ¯

        Args:
            session_id: ä¼šè¯ID

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        try:
            scroll_result = self.qdrant_client.scroll(
                collection_name=self.collection_name,
                scroll_filter={
                    "must": [
                        {"key": "session_id", "match": {"value": session_id}}
                    ]
                },
                limit=1000,
                with_payload=True
            )

            total_turns = len(scroll_result[0])
            total_tokens = sum(
                point.payload.get("token_count", 0)
                for point in scroll_result[0]
            )

            if total_turns > 0:
                first_turn = min(
                    scroll_result[0],
                    key=lambda p: p.payload["timestamp"]
                )
                last_turn = max(
                    scroll_result[0],
                    key=lambda p: p.payload["timestamp"]
                )

                return {
                    "session_id": session_id,
                    "total_turns": total_turns,
                    "total_tokens": total_tokens,
                    "avg_tokens_per_turn": total_tokens / total_turns,
                    "first_conversation": first_turn.payload["timestamp"],
                    "last_conversation": last_turn.payload["timestamp"]
                }
            else:
                return {
                    "session_id": session_id,
                    "total_turns": 0,
                    "total_tokens": 0,
                    "avg_tokens_per_turn": 0
                }

        except Exception as e:
            logger.error(f"è·å–ä¼šè¯ç»Ÿè®¡å¤±è´¥: {e}", exc_info=True)
            return {"error": str(e)}

    def clear_cache(self):
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        self._recent_cache.clear()
        logger.info("å¯¹è¯ç¼“å­˜å·²æ¸…ç©º")

    def get_user_sessions(
        self,
        user_id: int,
        limit: int = 20,
        offset: int = 0,
        sort_by: str = "last_update"
    ) -> Dict:
        """
        è·å–ç”¨æˆ·çš„æ‰€æœ‰ä¼šè¯åˆ—è¡¨

        Args:
            user_id: ç”¨æˆ·ID
            limit: æ¯é¡µè¿”å›çš„ä¼šè¯æ•°é‡
            offset: åˆ†é¡µåç§»é‡
            sort_by: æ’åºæ–¹å¼ ("last_update" æˆ– "create_time")

        Returns:
            åŒ…å«ä¼šè¯åˆ—è¡¨å’Œæ€»æ•°çš„å­—å…¸
        """
        try:
            logger.info(f"ğŸ” è·å–ç”¨æˆ· {user_id} çš„ä¼šè¯åˆ—è¡¨ (limit={limit}, offset={offset})")

            # âœ… éªŒè¯ç”¨æˆ·IDæœ‰æ•ˆæ€§
            if not user_id or user_id <= 0:
                logger.error(f"âŒ æ— æ•ˆçš„ç”¨æˆ·ID: {user_id}")
                return {
                    "total": 0,
                    "sessions": [],
                    "error": "æ— æ•ˆçš„ç”¨æˆ·ID"
                }

            # ç¡®ä¿ user_id æ˜¯å­—ç¬¦ä¸²ç±»å‹ç”¨äºå‰ç¼€åŒ¹é…
            user_id_str = str(user_id)
            logger.info(f"ğŸ”‘ æŸ¥è¯¢ç”¨æˆ·ID: {user_id_str}")

            # ç¡®ä¿é›†åˆå­˜åœ¨
            self._check_and_create_collection()

            # ğŸ”¥ ä½¿ç”¨ Qdrant Filter åœ¨æ•°æ®åº“å±‚é¢è¿‡æ»¤
            # ç”±äº Qdrant ä¸æ”¯æŒå‰ç¼€åŒ¹é… payload å­—æ®µï¼Œæˆ‘ä»¬éœ€è¦å…ˆæ·»åŠ  user_id å­—æ®µ
            # ä½œä¸ºä¸´æ—¶æ–¹æ¡ˆï¼Œæˆ‘ä»¬å…ˆè·å–æ‰€æœ‰æ•°æ®ï¼Œä½†æ·»åŠ æ›´ä¸¥æ ¼çš„è¿‡æ»¤å’Œæ—¥å¿—

            scroll_result = self.qdrant_client.scroll(
                collection_name=self.collection_name,
                limit=10000,  # å‡è®¾ä¸ä¼šè¶…è¿‡è¿™ä¸ªæ•°é‡
                with_payload=True,
                with_vectors=False  # ä¸éœ€è¦å‘é‡æ•°æ®ï¼ŒèŠ‚çœå¸¦å®½
            )

            logger.info(f"ğŸ“Š ä» Qdrant è·å–åˆ° {len(scroll_result[0])} æ¡å¯¹è¯è®°å½•")

            # æŒ‰ session_id åˆ†ç»„
            sessions_data = {}
            skipped_count = 0
            matched_count = 0

            for point in scroll_result[0]:
                session_id = point.payload.get("session_id")

                # ğŸ” è°ƒè¯•æ—¥å¿—ï¼šè®°å½•æ¯ä¸ª session_id
                if not session_id:
                    logger.debug(f"âš ï¸ è·³è¿‡æ²¡æœ‰ session_id çš„è®°å½•: point_id={point.id}")
                    skipped_count += 1
                    continue

                # âœ… ä¸¥æ ¼éªŒè¯ session_id æ˜¯å¦å±äºè¯¥ç”¨æˆ·
                # session_id æ ¼å¼ä¸º: {user_id}_{uuid}
                if not session_id.startswith(f"{user_id_str}_"):
                    logger.debug(f"ğŸš« ä¼šè¯ {session_id} ä¸å±äºç”¨æˆ· {user_id_str}ï¼Œè·³è¿‡")
                    skipped_count += 1
                    continue

                # âœ… åŒé‡éªŒè¯ï¼šæ£€æŸ¥ä¸‹åˆ’çº¿åˆ†éš”åçš„ç¬¬ä¸€éƒ¨åˆ†æ˜¯å¦ç¡®å®åŒ¹é…ç”¨æˆ·ID
                try:
                    parts = session_id.split('_', 1)  # åªåˆ†å‰²ä¸€æ¬¡ï¼Œé¿å… UUID ä¸­çš„ä¸‹åˆ’çº¿å½±å“
                    if len(parts) < 2:
                        logger.warning(f"âš ï¸ ä¼šè¯ID {session_id} æ ¼å¼å¼‚å¸¸ï¼ˆç¼ºå°‘ä¸‹åˆ’çº¿ï¼‰ï¼Œè·³è¿‡")
                        skipped_count += 1
                        continue

                    session_user_id = parts[0]
                    if session_user_id != user_id_str:
                        logger.warning(f"âš ï¸ ä¼šè¯ID {session_id} çš„ç”¨æˆ·IDéƒ¨åˆ† ({session_user_id}) ä¸åŒ¹é…ç›®æ ‡ç”¨æˆ· ({user_id_str})ï¼Œè·³è¿‡")
                        skipped_count += 1
                        continue
                except (IndexError, ValueError) as e:
                    logger.warning(f"âš ï¸ ä¼šè¯ID {session_id} è§£æå¤±è´¥ï¼Œè·³è¿‡: {e}")
                    skipped_count += 1
                    continue

                # âœ… éªŒè¯é€šè¿‡ï¼Œæ·»åŠ åˆ°ä¼šè¯æ•°æ®
                matched_count += 1

                if session_id not in sessions_data:
                    sessions_data[session_id] = {
                        "turns": [],
                        "total_tokens": 0
                    }

                sessions_data[session_id]["turns"].append({
                    "user_query": point.payload.get("user_query"),
                    "assistant_response": point.payload.get("assistant_response"),
                    "timestamp": point.payload.get("timestamp"),
                    "token_count": point.payload.get("token_count", 0)
                })
                sessions_data[session_id]["total_tokens"] += point.payload.get("token_count", 0)

            logger.info(f"âœ… åŒ¹é…åˆ° {matched_count} æ¡å±äºç”¨æˆ· {user_id} çš„è®°å½•ï¼Œè·³è¿‡ {skipped_count} æ¡")

            # æ„å»ºä¼šè¯åˆ—è¡¨
            sessions = []
            for session_id, data in sessions_data.items():
                turns = data["turns"]
                if not turns:
                    continue

                # æŒ‰æ—¶é—´æ’åº
                turns_sorted = sorted(turns, key=lambda x: x["timestamp"])
                first_turn = turns_sorted[0]
                last_turn = turns_sorted[-1]

                # ç”Ÿæˆä¼šè¯æ ‡é¢˜ï¼ˆä»ç¬¬ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯æå–ï¼‰
                title = self._generate_session_title(first_turn["user_query"])

                sessions.append({
                    "session_id": session_id,
                    "user_id": user_id,
                    "title": title,
                    "first_message": first_turn["user_query"][:50] + "..." if len(first_turn["user_query"]) > 50 else first_turn["user_query"],
                    "last_message": last_turn["user_query"][:50] + "..." if len(last_turn["user_query"]) > 50 else last_turn["user_query"],
                    "message_count": len(turns),
                    "total_tokens": data["total_tokens"],
                    "create_time": first_turn["timestamp"],
                    "last_update_time": last_turn["timestamp"]
                })

            # æ’åº
            if sort_by == "last_update":
                sessions.sort(key=lambda x: x["last_update_time"], reverse=True)
            else:  # create_time
                sessions.sort(key=lambda x: x["create_time"], reverse=True)

            # åˆ†é¡µ
            total = len(sessions)
            sessions_page = sessions[offset:offset + limit]

            logger.info(f"ğŸ“‹ ç”¨æˆ· {user_id} å…±æœ‰ {total} ä¸ªä¼šè¯ï¼Œè¿”å›ç¬¬ {offset+1}-{offset+len(sessions_page)} ä¸ª")

            return {
                "total": total,
                "sessions": sessions_page,
                "limit": limit,
                "offset": offset
            }

        except Exception as e:
            logger.error(f"âŒ è·å–ç”¨æˆ·ä¼šè¯åˆ—è¡¨å¤±è´¥: {e}", exc_info=True)
            return {
                "total": 0,
                "sessions": [],
                "error": str(e)
            }

    def _generate_session_title(self, first_message: str, max_length: int = 30) -> str:
        """
        ä»ç¬¬ä¸€æ¡æ¶ˆæ¯ç”Ÿæˆä¼šè¯æ ‡é¢˜

        Args:
            first_message: ç¬¬ä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
            max_length: æ ‡é¢˜æœ€å¤§é•¿åº¦

        Returns:
            ä¼šè¯æ ‡é¢˜
        """
        # æ¸…ç†æ¶ˆæ¯
        title = first_message.strip()

        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        title = " ".join(title.split())

        # æˆªæ–­åˆ°åˆé€‚é•¿åº¦
        if len(title) > max_length:
            title = title[:max_length] + "..."

        return title if title else "æ–°å¯¹è¯"

    def get_session_full_history(
        self,
        session_id: str,
        limit: int = 100,
        offset: int = 0,
        order: str = "asc"
    ) -> Dict:
        """
        è·å–ä¼šè¯çš„å®Œæ•´å†å²è®°å½•ï¼ˆæ”¯æŒåˆ†é¡µï¼‰

        Args:
            session_id: ä¼šè¯ID
            limit: æ¯é¡µè¿”å›çš„æ¶ˆæ¯æ•°é‡
            offset: åˆ†é¡µåç§»é‡
            order: æ’åºé¡ºåº ("asc"=ä»æ—§åˆ°æ–°, "desc"=ä»æ–°åˆ°æ—§)

        Returns:
            åŒ…å«æ¶ˆæ¯åˆ—è¡¨å’Œæ€»æ•°çš„å­—å…¸
        """
        try:
            logger.info(f"è·å–ä¼šè¯ {session_id} çš„å®Œæ•´å†å² (limit={limit}, offset={offset}, order={order})")

            # ç¡®ä¿é›†åˆå­˜åœ¨
            self._check_and_create_collection()

            # è·å–è¯¥ä¼šè¯çš„æ‰€æœ‰å¯¹è¯
            scroll_result = self.qdrant_client.scroll(
                collection_name=self.collection_name,
                scroll_filter={
                    "must": [
                        {"key": "session_id", "match": {"value": session_id}}
                    ]
                },
                limit=1000,  # å‡è®¾å•ä¼šè¯ä¸è¶…è¿‡1000è½®
                with_payload=True
            )

            # æå–æ‰€æœ‰æ¶ˆæ¯
            messages = []
            for point in scroll_result[0]:
                messages.append({
                    "turn_id": point.payload.get("turn_id"),
                    "parent_turn_id": point.payload.get("parent_turn_id"),
                    "user_query": point.payload.get("user_query"),
                    "assistant_response": point.payload.get("assistant_response"),
                    "timestamp": point.payload.get("timestamp"),
                    "context_docs": point.payload.get("context_docs", []),
                    "token_count": point.payload.get("token_count", 0)
                })

            # æŒ‰æ—¶é—´æ’åº
            messages.sort(
                key=lambda x: x["timestamp"],
                reverse=(order == "desc")
            )

            # åˆ†é¡µ
            total = len(messages)
            messages_page = messages[offset:offset + limit]

            logger.info(f"ä¼šè¯ {session_id} å…±æœ‰ {total} æ¡æ¶ˆæ¯ï¼Œè¿”å›ç¬¬ {offset}-{offset+len(messages_page)} æ¡")

            return {
                "session_id": session_id,
                "total_messages": total,
                "messages": messages_page,
                "limit": limit,
                "offset": offset,
                "order": order
            }

        except Exception as e:
            logger.error(f"è·å–ä¼šè¯å†å²å¤±è´¥: {e}", exc_info=True)
            return {
                "session_id": session_id,
                "total_messages": 0,
                "messages": [],
                "error": str(e)
            }

    def delete_session(self, session_id: str) -> bool:
        """
        åˆ é™¤æŒ‡å®šä¼šè¯ï¼ˆç‰©ç†åˆ é™¤ï¼‰

        Args:
            session_id: ä¼šè¯ID

        Returns:
            æ˜¯å¦æˆåŠŸåˆ é™¤
        """
        try:
            logger.info(f"å¼€å§‹åˆ é™¤ä¼šè¯ {session_id}...")

            # åˆ é™¤ Qdrant ä¸­è¯¥ä¼šè¯çš„æ‰€æœ‰ç‚¹
            self.qdrant_client.delete(
                collection_name=self.collection_name,
                points_selector=FilterSelector(
                    filter=Filter(
                        must=[
                            FieldCondition(
                                key="session_id",
                                match=MatchValue(value=session_id)
                            )
                        ]
                    )
                )
            )

            # æ¸…é™¤ç¼“å­˜
            if session_id in self._recent_cache:
                del self._recent_cache[session_id]

            logger.info(f"âœ… ä¼šè¯ {session_id} å·²åˆ é™¤")
            return True

        except Exception as e:
            logger.error(f"åˆ é™¤ä¼šè¯å¤±è´¥: {e}", exc_info=True)
            return False

    def get_session_info(self, session_id: str) -> Optional[Dict]:
        """
        è·å–å•ä¸ªä¼šè¯çš„è¯¦ç»†ä¿¡æ¯

        Args:
            session_id: ä¼šè¯ID

        Returns:
            ä¼šè¯ä¿¡æ¯å­—å…¸ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å› None
        """
        try:
            scroll_result = self.qdrant_client.scroll(
                collection_name=self.collection_name,
                scroll_filter={
                    "must": [
                        {"key": "session_id", "match": {"value": session_id}}
                    ]
                },
                limit=1000,
                with_payload=True
            )

            if not scroll_result[0]:
                return None

            # æå–æ‰€æœ‰å¯¹è¯è½®æ¬¡
            turns = []
            total_tokens = 0
            for point in scroll_result[0]:
                turns.append({
                    "timestamp": point.payload.get("timestamp"),
                    "user_query": point.payload.get("user_query"),
                    "token_count": point.payload.get("token_count", 0)
                })
                total_tokens += point.payload.get("token_count", 0)

            # æ’åº
            turns.sort(key=lambda x: x["timestamp"])
            first_turn = turns[0]
            last_turn = turns[-1]

            # æå– user_id
            user_id = None
            if "_" in session_id:
                try:
                    user_id = int(session_id.split("_", 1)[0])
                except ValueError:
                    pass

            return {
                "session_id": session_id,
                "user_id": user_id,
                "title": self._generate_session_title(first_turn["user_query"]),
                "message_count": len(turns),
                "total_tokens": total_tokens,
                "create_time": first_turn["timestamp"],
                "last_update_time": last_turn["timestamp"],
                "first_message": first_turn["user_query"]
            }

        except Exception as e:
            logger.error(f"è·å–ä¼šè¯ä¿¡æ¯å¤±è´¥: {e}", exc_info=True)
            return None

    def delete_old_conversations(self, days: int) -> Dict[str, any]:
        """
        åˆ é™¤æŒ‡å®šå¤©æ•°ä¹‹å‰çš„è¿‡æœŸå¯¹è¯

        Args:
            days: æŒ‡å®šå¤©æ•°ï¼Œåˆ é™¤è¯¥å¤©æ•°ä¹‹å‰çš„å¯¹è¯

        Returns:
            åˆ é™¤ç»“æœå­—å…¸ï¼ŒåŒ…å«åˆ é™¤çš„ä¼šè¯æ•°é‡å’Œè¯¦ç»†ä¿¡æ¯
        """
        try:
            # è®¡ç®—é˜ˆå€¼æ—¥æœŸ
            threshold_date = datetime.now() - timedelta(days=days)
            threshold_iso = threshold_date.isoformat()

            logger.info(f"å¼€å§‹åˆ é™¤ {days} å¤©å‰ï¼ˆ{threshold_iso}ï¼‰çš„è¿‡æœŸå¯¹è¯...")

            # å…ˆç»Ÿè®¡æœ‰å¤šå°‘æ¡è¿‡æœŸå¯¹è¯
            scroll_result = self.qdrant_client.scroll(
                collection_name=self.collection_name,
                scroll_filter=Filter(
                    must=[
                        FieldCondition(
                            key="timestamp",
                            range=Range(lt=threshold_iso)  # å°äºé˜ˆå€¼æ—¥æœŸçš„å¯¹è¯
                        )
                    ]
                ),
                limit=10000,  # å‡è®¾ä¸ä¼šè¶…è¿‡1ä¸‡æ¡è¿‡æœŸå¯¹è¯
                with_payload=True
            )

            expired_points = scroll_result[0]
            expired_count = len(expired_points)

            if expired_count == 0:
                logger.info("æ²¡æœ‰æ‰¾åˆ°éœ€è¦åˆ é™¤çš„è¿‡æœŸå¯¹è¯")
                return {
                    "success": True,
                    "deleted_count": 0,
                    "message": "æ²¡æœ‰è¿‡æœŸå¯¹è¯éœ€è¦åˆ é™¤"
                }

            # ç»Ÿè®¡å—å½±å“çš„ä¼šè¯
            affected_sessions = set()
            total_tokens = 0
            for point in expired_points:
                affected_sessions.add(point.payload.get("session_id"))
                total_tokens += point.payload.get("token_count", 0)

            # åˆ é™¤è¿‡æœŸå¯¹è¯
            self.qdrant_client.delete(
                collection_name=self.collection_name,
                points_selector=FilterSelector(
                    filter=Filter(
                        must=[
                            FieldCondition(
                                key="timestamp",
                                range=Range(lt=threshold_iso)
                            )
                        ]
                    )
                )
            )

            # æ¸…ç©ºæ‰€æœ‰ç¼“å­˜ï¼ˆå› ä¸ºå¯èƒ½æ¶‰åŠå¤šä¸ªä¼šè¯ï¼‰
            self.clear_cache()

            result = {
                "success": True,
                "deleted_count": expired_count,
                "affected_sessions": len(affected_sessions),
                "total_tokens_removed": total_tokens,
                "threshold_date": threshold_iso
            }

            logger.info(
                f"âœ… æˆåŠŸåˆ é™¤ {expired_count} æ¡è¿‡æœŸå¯¹è¯ | "
                f"æ¶‰åŠ {len(affected_sessions)} ä¸ªä¼šè¯ | "
                f"é‡Šæ”¾ {total_tokens} tokens"
            )

            return result

        except Exception as e:
            logger.error(f"åˆ é™¤è¿‡æœŸå¯¹è¯å¤±è´¥: {e}", exc_info=True)
            return {
                "success": False,
                "error": str(e),
                "deleted_count": 0
            }

    def cleanup_expired_conversations(self) -> Dict[str, any]:
        """
        è‡ªåŠ¨æ¸…ç†è¿‡æœŸå¯¹è¯ï¼ˆä½¿ç”¨é…ç½®çš„è¿‡æœŸå¤©æ•°ï¼‰

        Returns:
            æ¸…ç†ç»“æœå­—å…¸
        """
        expire_days = AppSettings.CONVERSATION_EXPIRE_DAYS
        logger.info(f"å¼€å§‹è‡ªåŠ¨æ¸…ç†è¿‡æœŸå¯¹è¯ï¼ˆè¿‡æœŸå¤©æ•°: {expire_days}ï¼‰...")
        result = self.delete_old_conversations(expire_days)
        logger.info(f"è‡ªåŠ¨æ¸…ç†å®Œæˆ: {result}")
        return result
