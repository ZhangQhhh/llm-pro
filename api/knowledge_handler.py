# -*- coding: utf-8 -*-
"""
çŸ¥è¯†é—®ç­”å¤„ç†å™¨
å¤„ç†çŸ¥è¯†åº“é—®ç­”çš„ä¸šåŠ¡é€»è¾‘
"""
import json
import os
from datetime import datetime
from typing import Generator, Dict, Any, Optional, List
from llama_index.core import QueryBundle
from config import Settings
from utils import logger, clean_for_sse_text
from pathlib import Path
from prompts import (
    get_knowledge_assistant_context_prefix,
    get_knowledge_system_rag_simple,
    get_knowledge_system_rag_advanced,
    get_knowledge_system_no_rag_think,
    get_knowledge_system_no_rag_simple,
    get_knowledge_user_rag_simple,
    get_knowledge_user_rag_advanced,
    get_knowledge_user_no_rag_think,
    get_knowledge_user_no_rag_simple,
    get_conversation_system_rag_with_history,
    get_conversation_system_general_with_history,
    get_conversation_context_prefix_relevant_history,
    get_conversation_context_prefix_recent_history,
    get_conversation_context_prefix_regulations,
    get_conversation_user_rag_query,
    get_conversation_user_general_query,
    get_conversation_summary_system,
    get_conversation_summary_user,
    get_conversation_summary_context_prefix
)


class KnowledgeHandler:
    """çŸ¥è¯†é—®ç­”å¤„ç†å™¨"""

    def __init__(
        self, 
        retriever, 
        reranker, 
        llm_wrapper, 
        llm_service=None,
        # å…ç­¾çŸ¥è¯†åº“ç›¸å…³ç»„ä»¶ï¼ˆå¯é€‰ï¼‰
        visa_free_retriever=None,
        # èˆªå¸çŸ¥è¯†åº“ç›¸å…³ç»„ä»¶ï¼ˆå¯é€‰ï¼‰
        airline_retriever=None,
        # å¤šåº“æ£€ç´¢å™¨å’Œæ„å›¾åˆ†ç±»å™¨
        multi_kb_retriever=None,
        intent_classifier=None,
        # å­é—®é¢˜åˆ†è§£å™¨ï¼ˆå¯é€‰ï¼‰
        sub_question_decomposer=None
    ):
        # é€šç”¨çŸ¥è¯†åº“ç»„ä»¶
        self.retriever = retriever
        self.reranker = reranker
        self.llm_wrapper = llm_wrapper
        self.llm_service = llm_service
        self.insert_block_filter = None
        
        # å…ç­¾çŸ¥è¯†åº“ç»„ä»¶
        self.visa_free_retriever = visa_free_retriever
        # èˆªå¸çŸ¥è¯†åº“ç»„ä»¶
        self.airline_retriever = airline_retriever
        # å¤šåº“æ£€ç´¢å™¨å’Œæ„å›¾åˆ†ç±»å™¨
        self.multi_kb_retriever = multi_kb_retriever
        self.intent_classifier = intent_classifier
        # å­é—®é¢˜åˆ†è§£å™¨
        self.sub_question_decomposer = sub_question_decomposer
        
        # å­é—®é¢˜ç­”æ¡ˆåˆæˆï¼ˆç”¨äºä¼ é€’åˆ°æç¤ºè¯ï¼‰
        self._last_synthesized_answer = None

        # å¦‚æœæä¾›äº† llm_serviceï¼Œåˆå§‹åŒ– InsertBlock è¿‡æ»¤å™¨
        if llm_service:
            from core.node_filter import InsertBlockFilter
            self.insert_block_filter = InsertBlockFilter(llm_service)
            logger.info("InsertBlock è¿‡æ»¤å™¨å·²åˆå§‹åŒ–")
        
        # æ—¥å¿—ï¼šçŸ¥è¯†åº“åŠŸèƒ½çŠ¶æ€
        enabled_features = []
        if self.multi_kb_retriever and self.intent_classifier:
            enabled_features.append("å¤šåº“æ£€ç´¢+æ„å›¾åˆ†ç±»")
        if self.visa_free_retriever:
            enabled_features.append("å…ç­¾åº“")
        if self.airline_retriever:
            enabled_features.append("èˆªå¸åº“")
        if self.sub_question_decomposer:
            enabled_features.append("å­é—®é¢˜åˆ†è§£")
        
        if enabled_features:
            logger.info(f"âœ“ çŸ¥è¯†åº“åŠŸèƒ½å·²å¯ç”¨: {', '.join(enabled_features)}")
        else:
            logger.info("âŠ˜ ä»…ä½¿ç”¨é€šç”¨çŸ¥è¯†åº“")

    def process(
        self,
        question: str,
        enable_thinking: bool,
        rerank_top_n: int,
        llm,
        client_ip: str = "unknown",
        use_insert_block: bool = False,
        insert_block_llm_id: Optional[str] = None
    ) -> Generator[str, None, None]:
        """
        å¤„ç†çŸ¥è¯†é—®ç­”

        Args:
            question: é—®é¢˜å†…å®¹
            enable_thinking: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼
            rerank_top_n: é‡æ’åºåè¿”å›çš„æ–‡æ¡£æ•°é‡
            llm: LLM å®ä¾‹
            client_ip: å®¢æˆ·ç«¯ IP
            use_insert_block: æ˜¯å¦ä½¿ç”¨ InsertBlock è¿‡æ»¤æ¨¡å¼
            insert_block_llm_id: InsertBlock ä½¿ç”¨çš„ LLM ID

        Yields:
            SSE æ ¼å¼çš„å“åº”æµ
        """
        full_response = ""
        
        # æ¸…ç©ºä¸Šä¸€æ¬¡çš„å­é—®é¢˜ç­”æ¡ˆå’Œåˆæˆç­”æ¡ˆï¼Œé˜²æ­¢ä¸²é¢˜
        self._last_sub_answers = None
        self._last_synthesized_answer = None

        try:
            logger.info(
                f"å¤„ç†çŸ¥è¯†é—®ç­”: '{question}' | "
                f"æ€è€ƒæ¨¡å¼: {enable_thinking} | "
                f"å‚è€ƒæ–‡ä»¶æ•°: {rerank_top_n} | "
                f"InsertBlock: {use_insert_block}"
            )

            # 1. æ™ºèƒ½è·¯ç”±æ£€ç´¢ï¼ˆæ ¹æ®æ„å›¾é€‰æ‹©çŸ¥è¯†åº“ï¼‰
            # å¦‚æœå‰ç«¯è®¾ç½®å‚è€ƒæ•°é‡ä¸º 0ï¼Œè·³è¿‡æ£€ç´¢
            if rerank_top_n == 0:
                logger.info("[æ£€ç´¢è·³è¿‡] å‰ç«¯è®¾ç½®å‚è€ƒæ•°é‡ä¸º 0ï¼Œè·³è¿‡æ£€ç´¢å’Œå­é—®é¢˜åˆ†è§£")
                final_nodes = []
                result = None
            else:
                yield ('CONTENT', "æ­£åœ¨è¿›è¡Œæ··åˆæ£€ç´¢...\n")
                full_response += "æ­£åœ¨è¿›è¡Œæ··åˆæ£€ç´¢...\n"
                
                # è°ƒç”¨æ£€ç´¢ï¼Œè·å–èŠ‚ç‚¹å’Œå…ƒæ•°æ®
                result = self._smart_retrieve_and_rerank(question, rerank_top_n)
            
            # æ£€æŸ¥æ˜¯å¦è¿”å›äº†å…ƒæ•°æ®ï¼ˆå­é—®é¢˜åˆ†è§£ï¼‰
            if result and isinstance(result, tuple) and len(result) == 2:
                final_nodes, retrieval_metadata = result
                
                # å¦‚æœæœ‰å­é—®é¢˜ï¼Œè¾“å‡ºåˆ°å‰ç«¯
                if retrieval_metadata.get('decomposed') and retrieval_metadata.get('sub_questions'):
                    sub_questions = retrieval_metadata['sub_questions']
                    sub_answers = retrieval_metadata.get('sub_answers', [])
                    
                    # æ„å»ºå®Œæ•´çš„å­é—®é¢˜æ•°æ®
                    sub_questions_data = {
                        'sub_questions': sub_questions,
                        'count': len(sub_questions),
                        'sub_answers': sub_answers  # åŒ…å«æ¯ä¸ªå­é—®é¢˜çš„ç­”æ¡ˆæ‘˜è¦
                    }
                    
                    yield ('SUB_QUESTIONS', sub_questions_data)
                    logger.info(
                        f"[å‰ç«¯è¾“å‡º] å·²å‘é€å­é—®é¢˜åˆ°å‰ç«¯ | "
                        f"å­é—®é¢˜æ•°: {len(sub_questions)} | "
                        f"ç­”æ¡ˆæ•°: {len(sub_answers)}"
                    )
            else:
                # å…¼å®¹æ—§ç‰ˆæœ¬ï¼ˆåªè¿”å›èŠ‚ç‚¹ï¼‰
                final_nodes = result
                retrieval_metadata = None


            # 2. å¦‚æœå¯ç”¨ InsertBlock æ¨¡å¼ï¼Œè¿›è¡Œæ™ºèƒ½è¿‡æ»¤
            filtered_results = None
            filtered_map = None
            nodes_for_prompt = final_nodes  # é»˜è®¤ä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ

            if use_insert_block and final_nodes and self.insert_block_filter:
                yield ('CONTENT', f"æ­£åœ¨ä½¿ç”¨ InsertBlock æ™ºèƒ½è¿‡æ»¤ {len(final_nodes)} ä¸ªèŠ‚ç‚¹...")
                full_response += f"æ­£åœ¨ä½¿ç”¨ InsertBlock æ™ºèƒ½è¿‡æ»¤ {len(final_nodes)} ä¸ªèŠ‚ç‚¹...\n"

                # å®šä¹‰è¿›åº¦å›è°ƒå‡½æ•°
                def progress_callback(processed, total):
                    progress_msg = f"[ç²¾å‡†æ£€ç´¢è¿›åº¦] {processed}/{total} ä¸ªèŠ‚ç‚¹å·²å¤„ç†"
                    logger.info(progress_msg)
                    # ä¸å‘é€åˆ°å‰ç«¯ï¼Œé¿å…åˆ·å±ï¼Œåªè®°å½•æ—¥å¿—

                filtered_results = self.insert_block_filter.filter_nodes(
                    question=question,
                    nodes=final_nodes,
                    llm_id=insert_block_llm_id,
                    progress_callback=progress_callback
                )

                if filtered_results:
                    yield ('CONTENT', f"æ‰¾åˆ° {len(filtered_results)} ä¸ªå¯å›ç­”çš„èŠ‚ç‚¹")
                    full_response += f"æ‰¾åˆ° {len(filtered_results)} ä¸ªå¯å›ç­”çš„èŠ‚ç‚¹\n"
                    # InsertBlock æˆåŠŸï¼šåªä½¿ç”¨è¿‡æ»¤åçš„èŠ‚ç‚¹
                    nodes_for_prompt = None  # ä¸å†ä¼ å…¥åŸå§‹èŠ‚ç‚¹
                    filtered_map = {}
                    for result in filtered_results:
                        key = f"{result['file_name']}_{result['reranked_score']}"
                        filtered_map[key] = result
                else:
                    yield ('CONTENT', "æœªæ‰¾åˆ°å¯ç›´æ¥å›ç­”çš„èŠ‚ç‚¹ï¼Œå°†ä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ")
                    full_response += "æœªæ‰¾åˆ°å¯ç›´æ¥å›ç­”çš„èŠ‚ç‚¹ï¼Œå°†ä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ\n"
                    # InsertBlock å¤±è´¥ï¼šç»§ç»­ä½¿ç”¨åŸå§‹èŠ‚ç‚¹ï¼Œæ¸…ç©ºè¿‡æ»¤ç»“æœ
                    filtered_results = None

            # 3. æ„é€ æç¤ºè¯
            prompt_parts = self._build_prompt(
                question,
                enable_thinking,
                nodes_for_prompt,  # æ ¹æ® InsertBlock ç»“æœå†³å®šä¼ å…¥å“ªäº›èŠ‚ç‚¹
                filtered_results=filtered_results
            )

            # 4. è¾“å‡ºçŠ¶æ€
            status_msg = (
                "å·²æ‰¾åˆ°ç›¸å…³èµ„æ–™ï¼Œæ­£åœ¨ç”Ÿæˆå›ç­”..."
                if final_nodes
                else "æœªæ‰¾åˆ°é«˜ç›¸å…³æ€§èµ„æ–™ï¼ŒåŸºäºé€šç”¨çŸ¥è¯†å›ç­”..."
            )
            yield ('CONTENT', status_msg)
            full_response += status_msg + "\n"

            # 5. è°ƒç”¨ LLM
            for result in self._call_llm(llm, prompt_parts, enable_thinking=enable_thinking):
                # result æ˜¯å…ƒç»„ (prefix_type, content)
                prefix_type, chunk = result
                if prefix_type == 'THINK':
                    yield ('THINK', chunk)
                    # æ€è€ƒå†…å®¹ä¸è®¡å…¥ full_response
                elif prefix_type == 'CONTENT':
                    yield ('CONTENT', chunk)
                    full_response += chunk

            # 6. è¾“å‡ºå‚è€ƒæ¥æº
            reference_entries = self._build_reference_log_entries(final_nodes, filtered_map)

            if use_insert_block and filtered_results:
                # InsertBlock æ¨¡å¼ï¼šè¿”å›æ‰€æœ‰åŸå§‹èŠ‚ç‚¹ï¼Œä½†æ ‡æ³¨å“ªäº›è¢«é€‰ä¸­
                yield ('CONTENT', "\n\n**å‚è€ƒæ¥æºï¼ˆå…¨éƒ¨æ£€ç´¢ç»“æœï¼‰:**")
                full_response += "\n\nå‚è€ƒæ¥æºï¼ˆå…¨éƒ¨æ£€ç´¢ç»“æœï¼‰:"

                # éå†æ‰€æœ‰åŸå§‹èŠ‚ç‚¹ï¼Œæ ‡æ³¨å“ªäº›è¢«é€‰ä¸­
                for i, node in enumerate(final_nodes):
                    file_name = node.node.metadata.get('file_name', 'æœªçŸ¥')
                    initial_score = node.node.metadata.get('initial_score', 0.0)
                    key = f"{file_name}_{node.score}"

                    # æ£€æŸ¥è¯¥èŠ‚ç‚¹æ˜¯å¦åœ¨è¿‡æ»¤ç»“æœä¸­
                    filtered_info = filtered_map.get(key)

                    # æå–æ£€ç´¢å…ƒæ•°æ®
                    retrieval_sources = node.node.metadata.get('retrieval_sources', [])
                    vector_score = node.node.metadata.get('vector_score', 0.0)
                    bm25_score = node.node.metadata.get('bm25_score', 0.0)
                    vector_rank = node.node.metadata.get('vector_rank')
                    bm25_rank = node.node.metadata.get('bm25_rank')
                    
                    source_data = {
                        "id": i + 1,
                        "fileName": file_name,
                        "initialScore": f"{initial_score:.4f}",
                        "rerankedScore": f"{node.score:.4f}",
                        "content": node.node.text.strip(),
                        # æ£€ç´¢å…ƒæ•°æ®
                        "retrievalSources": retrieval_sources,
                        "vectorScore": f"{vector_score:.4f}",
                        "bm25Score": f"{bm25_score:.4f}",
                        # InsertBlock ç‰¹æœ‰å­—æ®µ
                        "canAnswer": filtered_info is not None,
                        "reasoning": filtered_info.get('reasoning', '') if filtered_info else '',
                        "keyPassage": filtered_info.get('key_passage', '') if filtered_info else ''
                    }
                    
                    # æ·»åŠ æ’åä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    if vector_rank is not None:
                        source_data['vectorRank'] = vector_rank
                    if bm25_rank is not None:
                        source_data['bm25Rank'] = bm25_rank
                    
                    # æ·»åŠ åŒ¹é…çš„å…³é”®è¯ï¼ˆå¦‚æœæ˜¯å…³é”®è¯æ£€ç´¢ï¼‰
                    if 'keyword' in retrieval_sources:
                        matched_keywords = node.node.metadata.get('bm25_matched_keywords', [])
                        if matched_keywords:
                            source_data['matchedKeywords'] = matched_keywords

                    yield ('SOURCE', json.dumps(source_data, ensure_ascii=False))

                    full_response += (
                        f"\n[{source_data['id']}] æ–‡ä»¶: {source_data['fileName']}, "
                        f"é‡æ’åˆ†: {source_data['rerankedScore']}, "
                        f"å¯å›ç­”: {source_data['canAnswer']}"
                    )

            elif final_nodes:
                # æ™®é€šæ¨¡å¼ï¼šæ˜¾ç¤ºæ‰€æœ‰æ£€ç´¢ç»“æœ
                yield ('CONTENT', "\n\n**å‚è€ƒæ¥æº:**")
                full_response += "\n\nå‚è€ƒæ¥æº:"

                for source_msg in self._format_sources(final_nodes):
                    yield source_msg
                    if isinstance(source_msg, tuple) and source_msg[0] == "SOURCE":
                        data = json.loads(source_msg[1])
                        full_response += (
                            f"\n[{data['id']}] æ–‡ä»¶: {data['fileName']}, "
                            f"åˆå§‹åˆ†: {data['initialScore']}, "
                            f"é‡æ’åˆ†: {data['rerankedScore']}"
                        )

            self._log_reference_details(
                question=question,
                references=reference_entries,
                mode="single"
            )

            yield ('DONE', '')

            # 7. ä¿å­˜æ—¥å¿—
            self._save_log(
                question,
                full_response,
                client_ip,
                bool(final_nodes),
                use_insert_block=use_insert_block
            )

        except Exception as e:
            error_msg = f"å¤„ç†é”™è¯¯: {str(e)}"
            logger.error(f"çŸ¥è¯†é—®ç­”å¤„ç†å‡ºé”™: {e}", exc_info=True)
            yield ('ERROR', error_msg)

    def _retrieve_and_rerank(self, question: str, rerank_top_n: int, conversation_history: Optional[List[Dict]] = None):
        """
        æ£€ç´¢å’Œé‡æ’åºï¼ˆæ”¯æŒå­é—®é¢˜åˆ†è§£ï¼‰
        
        Args:
            question: ç”¨æˆ·æŸ¥è¯¢
            rerank_top_n: é‡æ’åºè¿”å›æ•°é‡
            conversation_history: å¯¹è¯å†å²ï¼ˆç”¨äºå¤šè½®åœºæ™¯ï¼‰
            
        Returns:
            æ£€ç´¢èŠ‚ç‚¹åˆ—è¡¨
        """
        # å¦‚æœå¯ç”¨äº†å­é—®é¢˜åˆ†è§£å™¨ï¼Œå°è¯•ä½¿ç”¨åˆ†è§£æ£€ç´¢
        if self.sub_question_decomposer and self.sub_question_decomposer.enabled:
            logger.info("[æ£€ç´¢ç­–ç•¥] å°è¯•ä½¿ç”¨å­é—®é¢˜åˆ†è§£æ£€ç´¢ï¼ˆå¤šè½®ï¼‰")
            try:
                # æ³¨æ„ï¼šå¤šè½®åœºæ™¯ä½¿ç”¨é»˜è®¤retrieverï¼Œå› ä¸ºæ²¡æœ‰æ„å›¾åˆ†ç±»
                # å¦‚æœéœ€è¦æ”¯æŒå¤šè½®+æ„å›¾è·¯ç”±ï¼Œéœ€è¦åœ¨è¿™é‡Œä¹Ÿæ·»åŠ æ„å›¾åˆ†ç±»é€»è¾‘
                nodes, metadata = self.sub_question_decomposer.retrieve_with_decomposition(
                    query=question,
                    rerank_top_n=rerank_top_n,
                    conversation_history=conversation_history
                )
                
                # è®°å½•åˆ†è§£å…ƒæ•°æ®
                if metadata.get('decomposed'):
                    logger.info(
                        f"[å­é—®é¢˜æ£€ç´¢] åˆ†è§£æ£€ç´¢å®Œæˆ | "
                        f"å­é—®é¢˜æ•°: {len(metadata['sub_questions'])} | "
                        f"è¿”å›èŠ‚ç‚¹æ•°: {len(nodes)}"
                    )
                    # è®°å½•è¯¦ç»†çš„å­é—®é¢˜ä¿¡æ¯åˆ°æ—¥å¿—
                    for i, sub_result in enumerate(metadata['sub_results'], 1):
                        logger.info(
                            f"  å­é—®é¢˜{i}: {sub_result['sub_question']} | "
                            f"èŠ‚ç‚¹æ•°: {sub_result['node_count']} | "
                            f"æœ€é«˜åˆ†: {sub_result['top_score']:.4f}"
                        )
                    
                    # å¯é€‰ï¼šç”Ÿæˆå­é—®é¢˜ç­”æ¡ˆåˆæˆï¼ˆå¦‚æœæœ‰sub_answersï¼‰
                    if metadata.get('sub_answers') and len(metadata['sub_answers']) > 0:
                        try:
                            synthesized_answer = self.sub_question_decomposer.synthesize_answer(
                                original_query=question,
                                sub_answers=metadata['sub_answers']
                            )
                            if synthesized_answer:
                                # å°†åˆæˆç­”æ¡ˆæ·»åŠ åˆ°metadataï¼Œä¾›åç»­ä½¿ç”¨
                                metadata['synthesized_answer'] = synthesized_answer
                                # å­˜å‚¨ä¸ºå®ä¾‹å˜é‡ï¼Œä¾›_build_promptä½¿ç”¨
                                self._last_synthesized_answer = synthesized_answer
                                logger.info(f"[ç­”æ¡ˆåˆæˆ] å·²ç”Ÿæˆåˆæˆç­”æ¡ˆ | é•¿åº¦: {len(synthesized_answer)}")
                        except Exception as synth_e:
                            logger.warning(f"[ç­”æ¡ˆåˆæˆ] åˆæˆå¤±è´¥: {synth_e}")
                else:
                    logger.info("[å­é—®é¢˜æ£€ç´¢] æœªåˆ†è§£ï¼Œä½¿ç”¨æ ‡å‡†æ£€ç´¢")
                
                return nodes
                
            except Exception as e:
                logger.error(f"[å­é—®é¢˜æ£€ç´¢] åˆ†è§£æ£€ç´¢å¤±è´¥: {e}", exc_info=True)
                logger.info("[å­é—®é¢˜æ£€ç´¢] å›é€€åˆ°æ ‡å‡†æ£€ç´¢æµç¨‹")
                # ç»§ç»­æ‰§è¡Œæ ‡å‡†æ£€ç´¢
        
        # æ ‡å‡†æ£€ç´¢æµç¨‹
        logger.info(f"[å•çŸ¥è¯†åº“æ£€ç´¢] å¼€å§‹æ£€ç´¢é—®é¢˜: {question}")
        logger.info(f"ğŸ” [DEBUG] ä½¿ç”¨çš„æ£€ç´¢å™¨å¯¹è±¡ID: {id(self.retriever)}")
        logger.info(f"ğŸ” [DEBUG] æ£€ç´¢å™¨ç±»å‹: {type(self.retriever).__name__}")
        retrieved_nodes = self.retriever.retrieve(question)
        
        # ğŸ” DEBUG: è®°å½•åˆå§‹æ£€ç´¢å¾—åˆ†
        if retrieved_nodes:
            initial_scores = [f"{n.score:.4f}" for n in retrieved_nodes[:5]]
            logger.info(f"[DEBUG] å•çŸ¥è¯†åº“åˆå§‹æ£€ç´¢Top5å¾—åˆ†: {', '.join(initial_scores)}")

        # å–å‰ N ä¸ªé€å…¥é‡æ’
        reranker_input_top_n = Settings.RERANKER_INPUT_TOP_N
        logger.info(f"[å•çŸ¥è¯†åº“æ£€ç´¢] é…ç½®æ£€æŸ¥ - RERANKER_INPUT_TOP_N: {reranker_input_top_n}")
        
        # è¯¦ç»†æ£€æŸ¥ retrieved_nodes
        logger.info(f"[å•çŸ¥è¯†åº“æ£€ç´¢] retrieved_nodes ç±»å‹: {type(retrieved_nodes)}")
        logger.info(f"[å•çŸ¥è¯†åº“æ£€ç´¢] retrieved_nodes é•¿åº¦: {len(retrieved_nodes) if retrieved_nodes else 'None'}")
        
        if retrieved_nodes and len(retrieved_nodes) > 0:
            logger.info(f"[å•çŸ¥è¯†åº“æ£€ç´¢] ç¬¬ä¸€ä¸ªèŠ‚ç‚¹é¢„è§ˆ: {retrieved_nodes[0].node.get_content()[:100]}...")
        
        reranker_input = retrieved_nodes[:reranker_input_top_n]

        logger.info(
            f"[å•çŸ¥è¯†åº“æ£€ç´¢] åˆæ£€ç´¢æ‰¾åˆ° {len(retrieved_nodes)} ä¸ªèŠ‚ç‚¹, "
            f"é€‰å–å‰ {len(reranker_input)} ä¸ªé€å…¥é‡æ’"
        )
        
        # å¦‚æœåˆå§‹æ£€ç´¢ä¸ºç©ºï¼Œæ‰“å°è­¦å‘Š
        if len(retrieved_nodes) == 0:
            logger.warning(
                f"[å•çŸ¥è¯†åº“æ£€ç´¢] âš ï¸ åˆå§‹æ£€ç´¢ç»“æœä¸ºç©ºï¼\n"
                f"  é—®é¢˜: {question}\n"
                f"  æ£€ç´¢å™¨çŠ¶æ€: {self.retriever is not None}\n"
                f"  å¯èƒ½åŸå› : çŸ¥è¯†åº“ä¸ºç©ºã€ç´¢å¼•æŸåã€æˆ–é—®é¢˜ä¸çŸ¥è¯†åº“å®Œå…¨ä¸ç›¸å…³"
            )

        # é‡æ’åº
        logger.info(f"[å•çŸ¥è¯†åº“æ£€ç´¢] å‡†å¤‡é‡æ’åº - reranker_input é•¿åº¦: {len(reranker_input)}")
        
        if reranker_input:
            logger.info(f"[å•çŸ¥è¯†åº“æ£€ç´¢] âœ“ è¿›å…¥é‡æ’åºåˆ†æ”¯ï¼Œå¼€å§‹è°ƒç”¨ Reranker æ¨¡å‹")
            logger.info(f"ğŸ” [DEBUG] Reranker å¯¹è±¡ID: {id(self.reranker)}")
            logger.info(f"ğŸ” [DEBUG] Reranker ç±»å‹: {type(self.reranker).__name__}")
            logger.info(f"ğŸ” [DEBUG] Reranker top_n: {self.reranker.top_n}")
            logger.info(f"ğŸ” [DEBUG] é—®é¢˜é•¿åº¦: {len(question)} å­—ç¬¦")
            logger.info(f"ğŸ” [DEBUG] é—®é¢˜å†…å®¹: {question[:100]}...")
            
            # ğŸ§ª ä¸´æ—¶å®éªŒï¼šé‡æ–°åˆ›å»º Reranker æ¥éªŒè¯æ˜¯å¦æ˜¯çŠ¶æ€æ±¡æŸ“é—®é¢˜
            logger.warning("ğŸ§ª [å®éªŒ] ä¸´æ—¶é‡æ–°åˆ›å»º Reranker æ¥æµ‹è¯•...")
            from llama_index.core.postprocessor import SentenceTransformerRerank
            temp_reranker = SentenceTransformerRerank(
                model=Settings.RERANKER_MODEL_PATH,
                top_n=Settings.RERANK_TOP_N,
                device=Settings.DEVICE
            )
            logger.info(f"ğŸ§ª [å®éªŒ] ä¸´æ—¶ Reranker å¯¹è±¡ID: {id(temp_reranker)}")
            
            reranked_nodes = temp_reranker.postprocess_nodes(
                reranker_input,
                query_bundle=QueryBundle(question)
            )
            logger.info("ğŸ§ª [å®éªŒ] ä½¿ç”¨ä¸´æ—¶ Reranker å®Œæˆé‡æ’åº")
            logger.info(f"[å•çŸ¥è¯†åº“æ£€ç´¢] âœ“ Reranker å¤„ç†å®Œæˆï¼Œå¾—åˆ° {len(reranked_nodes)} ä¸ªèŠ‚ç‚¹")
            # ğŸ” DEBUG: è®°å½•é‡æ’åºåå¾—åˆ†
            if reranked_nodes:
                rerank_scores = [f"{n.score:.4f}" for n in reranked_nodes[:5]]
                logger.info(f"[DEBUG] å•çŸ¥è¯†åº“é‡æ’åºåTop5å¾—åˆ†: {', '.join(rerank_scores)}")
        else:
            logger.warning(f"[å•çŸ¥è¯†åº“æ£€ç´¢] âš ï¸ reranker_input ä¸ºç©ºï¼Œè·³è¿‡é‡æ’åºï¼")
            reranked_nodes = []

        # é˜ˆå€¼è¿‡æ»¤
        threshold = Settings.RERANK_SCORE_THRESHOLD
        final_nodes = [
            node for node in reranked_nodes
            if node.score >= threshold
        ]
        
        # ğŸ” DEBUG: è®°å½•è¿‡æ»¤åå¾—åˆ†
        if final_nodes:
            final_scores = [f"{n.score:.4f}" for n in final_nodes[:5]]
            logger.info(f"[DEBUG] å•çŸ¥è¯†åº“é˜ˆå€¼è¿‡æ»¤åTop5å¾—åˆ†: {', '.join(final_scores)}")

        logger.info(
            f"[å•çŸ¥è¯†åº“æ£€ç´¢] é‡æ’åºåæœ‰ {len(reranked_nodes)} ä¸ªèŠ‚ç‚¹, "
            f"ç»è¿‡é˜ˆå€¼ {threshold} è¿‡æ»¤åå‰©ä¸‹ {len(final_nodes)} ä¸ª"
        )
        
        # å¦‚æœé˜ˆå€¼è¿‡æ»¤åä¸ºç©ºï¼Œæ‰“å°è¯¦ç»†ä¿¡æ¯
        if len(reranked_nodes) > 0 and len(final_nodes) == 0:
            max_score = max(node.score for node in reranked_nodes) if reranked_nodes else 0.0
            logger.warning(
                f"[å•çŸ¥è¯†åº“æ£€ç´¢] âš ï¸ é˜ˆå€¼è¿‡æ»¤åç»“æœä¸ºç©ºï¼\n"
                f"  é‡æ’åºèŠ‚ç‚¹æ•°: {len(reranked_nodes)}\n"
                f"  æœ€é«˜åˆ†æ•°: {max_score:.4f}\n"
                f"  é˜ˆå€¼: {threshold}\n"
                f"  å»ºè®®: é™ä½ RERANK_SCORE_THRESHOLD æˆ–æ£€æŸ¥ Reranker æ¨¡å‹"
            )

        # åº”ç”¨æœ€ç»ˆæ•°é‡é™åˆ¶
        result = final_nodes[:rerank_top_n]
        logger.info(f"[å•çŸ¥è¯†åº“æ£€ç´¢] æœ€ç»ˆè¿”å› {len(result)} ä¸ªèŠ‚ç‚¹")
        return result


    def _build_prompt(
        self,
        question: str,
        enable_thinking: bool,
        final_nodes,
        filtered_results=None
    ):
        """æ„é€ æç¤ºè¯"""
        # å¦‚æœæœ‰ InsertBlock è¿‡æ»¤ç»“æœï¼Œä¼˜å…ˆä½¿ç”¨
        if filtered_results:
            # åŒæ—¶ä½¿ç”¨å…³é”®æ®µè½å’Œå®Œæ•´å†…å®¹æ„å»ºä¸Šä¸‹æ–‡
            context_blocks = []
            block_index = 1  # ç”¨äºç¼–å·å®é™…æ·»åŠ çš„å—
            
            for result in filtered_results:
                file_name = result['file_name']
                key_passage = result.get('key_passage', '')
                full_content = result['node'].node.text.strip()
                can_answer = result.get('can_answer', False)

                # ä¸¥æ ¼è¿‡æ»¤ï¼šåªæœ‰ can_answer=True ä¸” key_passage ä¸ä¸ºç©ºæ‰æ³¨å…¥ä¸Šä¸‹æ–‡
                if not can_answer:
                    logger.warning(f"[ç²¾å‡†æ£€ç´¢è¿‡æ»¤] è·³è¿‡ä¸å¯å›ç­”çš„èŠ‚ç‚¹: {file_name}")
                    continue
                
                if not key_passage or key_passage.strip() == "":
                    logger.warning(f"[ç²¾å‡†æ£€ç´¢è¿‡æ»¤] è·³è¿‡æ— å…³é”®æ®µè½çš„èŠ‚ç‚¹: {file_name} | can_answer={can_answer}")
                    continue

                # æ„å»ºåŒ…å«å…³é”®æ®µè½å’Œå®Œæ•´å†…å®¹çš„å—
                block = (
                    f"### ä¸šåŠ¡è§„å®š {block_index} - {file_name}:\n"
                    # f"**ã€å…³é”®æ®µè½ã€‘**\n> {key_passage}\n\n"
                    f"**ã€å®Œæ•´å†…å®¹ã€‘**\n> {full_content}"
                )
                context_blocks.append(block)
                block_index += 1
                logger.info(f"[ç²¾å‡†æ£€ç´¢é€šè¿‡] èŠ‚ç‚¹å·²æ³¨å…¥ä¸Šä¸‹æ–‡: {file_name} | å…³é”®æ®µè½é•¿åº¦: {len(key_passage)}")

            formatted_context = "\n\n".join(context_blocks) if context_blocks else None
            has_rag = bool(context_blocks)

            logger.info(
                f"ä½¿ç”¨ InsertBlock ç»“æœæ„å»ºä¸Šä¸‹æ–‡: {len(context_blocks)} ä¸ªæ®µè½ "
                f"(åŒ…å«å…³é”®æ®µè½+å®Œæ•´å†…å®¹)"
            )
        elif final_nodes:
            # æ ¼å¼åŒ–ä¸Šä¸‹æ–‡ - ç›´æ¥æ˜¾ç¤ºæ–‡ä»¶åï¼Œå¹¶ä¸ºæ¯ä¸ªæ¥æºç¼–å·
            context_blocks = []
            for i, node in enumerate(final_nodes):
                file_name = node.node.metadata.get('file_name', 'æœªçŸ¥æ–‡ä»¶')
                content = node.node.get_content().strip()
                block = f"### ä¸šåŠ¡è§„å®š {i + 1} - {file_name}:\n> {content}"
                context_blocks.append(block)

            formatted_context = "\n\n".join(context_blocks)
            has_rag = True
        else:
            formatted_context = None
            has_rag = False

        # æ£€æŸ¥æ˜¯å¦æœ‰å­é—®é¢˜ç­”æ¡ˆæˆ–åˆæˆç­”æ¡ˆéœ€è¦æ³¨å…¥
        has_sub_answers = hasattr(self, '_last_sub_answers') and self._last_sub_answers
        has_synthesis = hasattr(self, '_last_synthesized_answer') and self._last_synthesized_answer
        
        # å¦‚æœæœ‰æ£€ç´¢æ–‡æ¡£æˆ–æœ‰å­é—®é¢˜ç­”æ¡ˆï¼Œéƒ½éœ€è¦æ„å»ºä¸Šä¸‹æ–‡
        if has_rag or has_sub_answers or has_synthesis:
            # è·å–å‰ç¼€
            assistant_prefix = get_knowledge_assistant_context_prefix()
            
            # æ„å»ºåŸºç¡€ä¸Šä¸‹æ–‡
            if has_rag:
                assistant_context = assistant_prefix + formatted_context
            else:
                # å³ä½¿æ²¡æœ‰æ£€ç´¢æ–‡æ¡£ï¼Œä¹Ÿåˆ›å»ºä¸Šä¸‹æ–‡ç”¨äºæ³¨å…¥å­é—®é¢˜ç­”æ¡ˆ
                assistant_context = assistant_prefix + "**æ³¨æ„**: æœªæ£€ç´¢åˆ°ç›¸å…³ä¸šåŠ¡è§„å®šæ–‡æ¡£ï¼Œè¯·åŸºäºä»¥ä¸‹å­é—®é¢˜åˆ†æå›ç­”ã€‚\n"
                logger.info("[æç¤ºè¯æ„å»º] æ— æ£€ç´¢æ–‡æ¡£ï¼Œä½†æœ‰å­é—®é¢˜ç­”æ¡ˆï¼Œåˆ›å»ºä¸Šä¸‹æ–‡ç”¨äºæ³¨å…¥")
            
            # å¦‚æœæœ‰å­é—®é¢˜ç­”æ¡ˆï¼Œæ·»åŠ åˆ°ä¸Šä¸‹æ–‡ä¸­
            if has_sub_answers:
                sub_answers_block = "\n\n### ğŸ“‹ å­é—®é¢˜åˆ†è§£ä¸å›ç­”:\n"
                for i, sub_answer in enumerate(self._last_sub_answers, 1):
                    sub_q = sub_answer.get('sub_question', '')
                    answer = sub_answer.get('answer', '')
                    sub_answers_block += f"\n**å­é—®é¢˜{i}**: {sub_q}\n**å›ç­”{i}**: {answer}\n"
                
                sub_answers_block += "\n**æ³¨æ„**: ä»¥ä¸Šæ˜¯å„å­é—®é¢˜çš„ç‹¬ç«‹å›ç­”ï¼Œè¯·ç»“åˆè¿™äº›ä¿¡æ¯å’Œä¸šåŠ¡è§„å®šç»™å‡ºå®Œæ•´ç­”æ¡ˆã€‚"
                assistant_context += sub_answers_block
                logger.info(f"[æç¤ºè¯æ„å»º] å·²å°† {len(self._last_sub_answers)} ä¸ªå­é—®é¢˜ç­”æ¡ˆæ³¨å…¥ä¸Šä¸‹æ–‡")
            
            # å¦‚æœæœ‰å­é—®é¢˜ç­”æ¡ˆåˆæˆï¼Œæ·»åŠ åˆ°ä¸Šä¸‹æ–‡ä¸­
            if self._last_synthesized_answer:
                synthesis_block = (
                    f"\n\n###  å­é—®é¢˜ç»¼åˆåˆ†æ:\n"
                    f"> {self._last_synthesized_answer}\n\n"
                    f"**æ³¨æ„**: ä»¥ä¸Šæ˜¯å¯¹å¤šä¸ªå­é—®é¢˜ç­”æ¡ˆçš„ç»¼åˆæ•´ç†ï¼Œè¯·ç»“åˆå…·ä½“ä¸šåŠ¡è§„å®šç»™å‡ºæœ€ç»ˆå›ç­”ã€‚"
                )
                assistant_context += synthesis_block
                logger.info(f"[æç¤ºè¯æ„å»º] å·²å°†åˆæˆç­”æ¡ˆæ³¨å…¥ä¸Šä¸‹æ–‡ | é•¿åº¦: {len(self._last_synthesized_answer)}")

            # æ ¹æ®æ€è€ƒæ¨¡å¼é€‰æ‹©ä¸åŒçš„ system å’Œ user prompt
            if enable_thinking:
                system_prompt = get_knowledge_system_rag_advanced()
                user_template = get_knowledge_user_rag_advanced()
            else:
                system_prompt = get_knowledge_system_rag_simple()
                user_template = get_knowledge_user_rag_simple()

            # user_template æ˜¯åˆ—è¡¨ï¼Œéœ€è¦ join åå† format
            user_prompt_str = "\n".join(user_template) if isinstance(user_template, list) else user_template
            # å¦‚æœå…³é—­æ€è€ƒæ¨¡å¼ï¼Œè‡ªåŠ¨åœ¨é—®é¢˜åè¿½åŠ  /no_think æŒ‡ä»¤ï¼ˆé˜¿é‡Œäº‘æ–‡æ¡£å»ºè®®ï¼‰
            actual_question = f"{question}/no_think" if not enable_thinking else question
            if not enable_thinking:
                logger.info(f"âœ“ å·²åœ¨é—®é¢˜åè¿½åŠ  /no_think æŒ‡ä»¤: '{actual_question}'")
            
            # å°†å‚è€ƒèµ„æ–™ç›´æ¥æ³¨å…¥åˆ° user_prompt ä¸­ï¼Œè€Œä¸æ˜¯ä½œä¸ºå•ç‹¬çš„ assistant_context
            user_prompt = user_prompt_str.format(context=assistant_context, question=actual_question)
            # æ¸…ç©º assistant_contextï¼Œå› ä¸ºå·²ç»åˆå¹¶åˆ° user_prompt ä¸­
            assistant_context_for_llm = None
            logger.info("[æç¤ºè¯æ„å»º] å·²å°†å‚è€ƒèµ„æ–™åˆå¹¶åˆ°ç”¨æˆ·é—®é¢˜ä¸­ï¼ˆäºŒæ®µå¼ï¼‰")

        else:
            # æ²¡æœ‰æ£€ç´¢åˆ°ç›¸å…³å†…å®¹
            assistant_context = None

            if enable_thinking:
                system_prompt = get_knowledge_system_no_rag_think()
                user_template = get_knowledge_user_no_rag_think()
            else:
                system_prompt = get_knowledge_system_no_rag_simple()
                user_template = get_knowledge_user_no_rag_simple()

            # user_template å¯èƒ½æ˜¯åˆ—è¡¨æˆ–å­—ç¬¦ä¸²
            user_prompt_str = "\n".join(user_template) if isinstance(user_template, list) else user_template
            # å¦‚æœå…³é—­æ€è€ƒæ¨¡å¼ï¼Œè‡ªåŠ¨åœ¨é—®é¢˜åè¿½åŠ  /no_think æŒ‡ä»¤ï¼ˆé˜¿é‡Œäº‘æ–‡æ¡£å»ºè®®ï¼‰
            actual_question = f"{question}/no_think" if not enable_thinking else question
            if not enable_thinking:
                logger.info(f"âœ“ å·²åœ¨é—®é¢˜åè¿½åŠ  /no_think æŒ‡ä»¤: '{actual_question}'")
            user_prompt = user_prompt_str.format(question=actual_question)

        # system_prompt å¯èƒ½æ˜¯åˆ—è¡¨ï¼Œéœ€è¦è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        if isinstance(system_prompt, list):
            system_prompt = "\n".join(system_prompt)

        # ç¡®å®šå®é™…ä¼ ç»™ LLM çš„ assistant_context
        # å¦‚æœä½¿ç”¨äºŒæ®µå¼ï¼ˆå‚è€ƒèµ„æ–™å·²åˆå¹¶åˆ° user_promptï¼‰ï¼Œåˆ™ä¼  None
        llm_assistant_context = assistant_context_for_llm if 'assistant_context_for_llm' in locals() else assistant_context

        # æ„å»º fallback_promptï¼ˆç”¨äºä¸æ”¯æŒ chat æ¨¡å¼çš„æƒ…å†µï¼‰
        fallback_parts = [system_prompt]
        if llm_assistant_context:
            fallback_parts.append(llm_assistant_context)
        fallback_parts.append(user_prompt)

        prompt_result = {
            "system_prompt": system_prompt,
            "user_prompt": user_prompt,
            "assistant_context": llm_assistant_context,  # å®é™…ä¼ ç»™ LLM çš„
            "assistant_context_log": assistant_context,  # ç”¨äºæ—¥å¿—è®°å½•
            "fallback_prompt": "\n\n".join(fallback_parts)
        }
        
        # è¾“å‡ºä¸Šä¸‹æ–‡åˆ°æ—¥å¿—æ–‡ä»¶
        self._log_prompt_to_file(question, prompt_result)
        
        return prompt_result

    def _log_prompt_to_file(self, question: str, prompt_parts: Dict[str, Any]):
        """
        å°†æç¤ºè¯ä¸Šä¸‹æ–‡è¾“å‡ºåˆ°æ—¥å¿—æ–‡ä»¶ï¼ˆæ¯æ¬¡é—®ç­”å•ç‹¬ä¿å­˜ï¼‰
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            prompt_parts: æç¤ºè¯å­—å…¸
        """
        try:
            # ç¡®ä¿ logs ç›®å½•å­˜åœ¨
            logs_dir = Path("logs")
            logs_dir.mkdir(exist_ok=True)
            
            # ç”Ÿæˆå”¯ä¸€çš„æ—¥å¿—æ–‡ä»¶åï¼ˆåŸºäºæ—¶é—´æˆ³ï¼‰
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            log_file = logs_dir / f"prompt_{timestamp}.txt"
            
            # æ„å»ºæ—¥å¿—å†…å®¹ï¼ˆå®Œæ•´çš„å•æ¬¡é—®ç­”ä¸Šä¸‹æ–‡ï¼‰
            log_content = []
            log_content.append("=" * 100)
            log_content.append(f"é—®ç­”æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            log_content.append("=" * 100)
            log_content.append("")
            
            # ç”¨æˆ·é—®é¢˜
            log_content.append("ã€ç”¨æˆ·é—®é¢˜ã€‘")
            log_content.append(question)
            log_content.append("")
            log_content.append("-" * 100)
            log_content.append("")
            
            # System Prompt
            log_content.append("ã€System Promptã€‘")
            log_content.append(prompt_parts.get('system_prompt', 'N/A'))
            log_content.append("")
            log_content.append("-" * 100)
            log_content.append("")
            
            # Assistant Context (æ£€ç´¢æ–‡æ¡£ + å­é—®é¢˜ç­”æ¡ˆ)
            # ä½¿ç”¨ assistant_context_log æ˜¾ç¤ºå®Œæ•´çš„å‚è€ƒèµ„æ–™ï¼ˆå³ä½¿å·²åˆå¹¶åˆ°ç”¨æˆ·é—®é¢˜ä¸­ï¼‰
            context_for_log = prompt_parts.get('assistant_context_log') or prompt_parts.get('assistant_context')
            if context_for_log:
                log_content.append("ã€å‚è€ƒèµ„æ–™ã€‘ï¼ˆä»¥ä¸‹å†…å®¹å·²æ³¨å…¥åˆ°ç”¨æˆ·é—®é¢˜ä¸­ï¼‰")
                log_content.append(context_for_log)
                log_content.append("")
                log_content.append("-" * 100)
                log_content.append("")
            else:
                log_content.append("ã€å‚è€ƒèµ„æ–™ã€‘")
                log_content.append("æ— æ£€ç´¢æ–‡æ¡£æˆ–å­é—®é¢˜ç­”æ¡ˆ")
                log_content.append("")
                log_content.append("-" * 100)
                log_content.append("")
            
            # User Prompt
            log_content.append("ã€User Promptã€‘")
            log_content.append(prompt_parts.get('user_prompt', 'N/A'))
            log_content.append("")
            log_content.append("=" * 100)
            
            # å†™å…¥æ–‡ä»¶ï¼ˆæ¯æ¬¡é—®ç­”ç‹¬ç«‹æ–‡ä»¶ï¼‰
            with open(log_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(log_content))
            
            logger.info(f"[æç¤ºè¯æ—¥å¿—] å·²ä¿å­˜åˆ° {log_file}")
            
            # åŒæ—¶è¿½åŠ åˆ°æ€»æ—¥å¿—æ–‡ä»¶ï¼ˆå¯é€‰ï¼Œä¾¿äºæŸ¥çœ‹æ‰€æœ‰è®°å½•ï¼‰
            all_logs_file = logs_dir / "prompts_logs_all.txt"
            with open(all_logs_file, 'a', encoding='utf-8') as f:
                f.write('\n'.join(log_content))
                f.write('\n\n')
            
        except Exception as e:
            logger.error(f"[æç¤ºè¯æ—¥å¿—] ä¿å­˜å¤±è´¥: {e}")

    def _call_llm(self, llm, prompt_parts, enable_thinking: bool = False):
        """
        è°ƒç”¨ LLMï¼Œæ”¯æŒæ€è€ƒå†…å®¹å’Œæ­£æ–‡å†…å®¹çš„åˆ†ç¦»

        Args:
            llm: LLM å®ä¾‹
            prompt_parts: æç¤ºè¯å­—å…¸
            enable_thinking: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼ï¼ˆç”¨äºè§£æè¾“å‡ºï¼‰

        Note:
            æ”¯æŒä¸¤ç§æ€è€ƒæ¨¡å¼ï¼š
            1. é˜¿é‡Œäº‘åŸç”Ÿ reasoning_content å­—æ®µï¼ˆæ¨èï¼‰
            2. æ–‡æœ¬æ ‡è®°æ–¹å¼ï¼ˆå…¼å®¹å…¶ä»–æ¨¡å‹ï¼‰
        """
        logger.info(f"ä½¿ç”¨å¤–éƒ¨ Prompt:\n{prompt_parts['fallback_prompt'][:200]}...")

        response_stream = self.llm_wrapper.stream(
            llm,
            prompt=prompt_parts['fallback_prompt'],
            system_prompt=prompt_parts['system_prompt'],
            user_prompt=prompt_parts['user_prompt'],
            assistant_context=prompt_parts['assistant_context'],
            use_chat_mode=Settings.USE_CHAT_MODE,
            enable_thinking=enable_thinking
        )

        # å¦‚æœå¯ç”¨æ€è€ƒæ¨¡å¼ï¼Œéœ€è¦è§£æå¹¶åˆ†ç¦»æ€è€ƒå†…å®¹å’Œæ­£æ–‡å†…å®¹
        if enable_thinking:
            buffer = ""
            in_thinking_section = False
            thinking_complete = False
            has_reasoning_content = False  # æ ‡è®°æ˜¯å¦æ£€æµ‹åˆ°åŸç”Ÿ reasoning_content
            think_output_count = 0
            content_output_count = 0
            
            # ç”¨äºç´¯ç§¯åŸç”Ÿæ ¼å¼çš„å†…å®¹
            reasoning_buffer = ""
            content_buffer = ""

            for delta in response_stream:
                # ä¼˜å…ˆæ£€æŸ¥é˜¿é‡Œäº‘åŸç”Ÿçš„ reasoning_content å­—æ®µ
                if hasattr(delta, 'reasoning_content') and delta.reasoning_content is not None:
                    has_reasoning_content = True
                    reasoning_text = delta.reasoning_content
                    if reasoning_text:
                        reasoning_buffer += reasoning_text
                        # ç´¯ç§¯åˆ°ä¸€å®šé•¿åº¦åå†å‘é€
                        if len(reasoning_buffer) >= 10:
                            think_output_count += 1
                            output = ('THINK', clean_for_sse_text(reasoning_buffer))
                            yield output
                            reasoning_buffer = ""

                # æ£€æŸ¥æ­£å¸¸å›ç­”å†…å®¹ï¼ˆæ— è®ºæ˜¯å¦æœ‰ reasoning_contentï¼Œéƒ½è¦å¤„ç†ï¼‰
                if hasattr(delta, 'content') and delta.content is not None:
                    content_text = delta.content
                    if content_text:
                        content_buffer += content_text
                        # ç´¯ç§¯åˆ°ä¸€å®šé•¿åº¦åå†å‘é€
                        if len(content_buffer) >= 10:
                            content_output_count += 1
                            output = ('CONTENT', clean_for_sse_text(content_buffer))
                            yield output
                            content_buffer = ""
                    # å¦‚æœæœ‰ reasoning_content ä¸”å·²å¤„ç†äº† contentï¼Œåˆ™è·³è¿‡åç»­çš„æ–‡æœ¬æ ‡è®°è§£æ
                    if has_reasoning_content:
                        continue

                # å¦‚æœæ²¡æœ‰ reasoning_content å­—æ®µï¼Œä½¿ç”¨æ–‡æœ¬æ ‡è®°æ–¹å¼ï¼ˆå…¼å®¹æ¨¡å¼ï¼‰
                if not has_reasoning_content:
                    # è·å–æ–‡æœ¬å†…å®¹
                    if hasattr(delta, 'delta'):
                        token = delta.delta
                    elif hasattr(delta, 'text'):
                        token = delta.text
                    elif hasattr(delta, 'content'):
                        token = delta.content
                    else:
                        token = str(delta) if delta else ''

                    if not token:
                        continue

                    buffer += token

                    # æ£€æµ‹æ€è€ƒéƒ¨åˆ†çš„å¼€å§‹å’Œç»“æŸæ ‡è®°
                    if not thinking_complete:
                        # æ£€æŸ¥æ˜¯å¦è¿›å…¥æ€è€ƒåŒºåŸŸ
                        if not in_thinking_section:
                            # æ£€æµ‹æ€è€ƒå¼€å§‹çš„å¤šç§æ ‡è®°
                            thinking_markers = [
                                'ã€å’¨è¯¢è§£æã€‘', 'ç¬¬ä¸€éƒ¨åˆ†ï¼šå’¨è¯¢è§£æ', 'ç¬¬ä¸€éƒ¨åˆ†:å’¨è¯¢è§£æ',
                                '<think>', '## æ€è€ƒè¿‡ç¨‹', '## åˆ†æè¿‡ç¨‹',
                                'å…³é”®å®ä½“', 'Key Entities', '1. å…³é”®å®ä½“'
                            ]

                            for marker in thinking_markers:
                                if marker in buffer:
                                    in_thinking_section = True
                                    logger.info(f"æ£€æµ‹åˆ°æ€è€ƒå¼€å§‹æ ‡è®°: {marker}")
                                    break

                        # æ£€æµ‹æ€è€ƒç»“æŸçš„æ ‡è®°
                        if in_thinking_section:
                            end_markers = [
                                'ã€ç»¼åˆè§£ç­”ã€‘', 'ç¬¬äºŒéƒ¨åˆ†ï¼šç»¼åˆè§£ç­”', 'ç¬¬äºŒéƒ¨åˆ†:ç»¼åˆè§£ç­”',
                                '</think>', '## æœ€ç»ˆç­”æ¡ˆ', '## å›ç­”'
                            ]

                            for marker in end_markers:
                                if marker in buffer:
                                    thinking_complete = True
                                    # è¾“å‡ºæ€è€ƒå†…å®¹ï¼ˆä¸åŒ…å«ç»“æŸæ ‡è®°ï¼‰
                                    idx = buffer.index(marker)
                                    if idx > 0:
                                        think_content = buffer[:idx]
                                        think_output_count += 1
                                        output = ('THINK', clean_for_sse_text(think_content))
                                        yield output

                                    # è·³è¿‡æ ‡è®°æœ¬èº«ï¼Œåªä¿ç•™æ ‡è®°ä¹‹åçš„å†…å®¹
                                    buffer = buffer[idx + len(marker):]
                                    break

                    # åœ¨æ€è€ƒåŒºåŸŸä¸”bufferè¶³å¤Ÿé•¿æ—¶ï¼Œæµå¼è¾“å‡ºæ€è€ƒå†…å®¹
                    if in_thinking_section and not thinking_complete and len(buffer) > 20:
                        think_output_count += 1
                        output = ('THINK', clean_for_sse_text(buffer))
                        yield output
                        buffer = ""
                    # æ€è€ƒå®Œæˆåï¼Œæµå¼è¾“å‡ºæ­£æ–‡å†…å®¹
                    elif thinking_complete and len(buffer) > 0:
                        # åªæ¸…ç†å¼€å¤´çš„æ ‡è®°ç¬¦å·ï¼ˆå†’å·ç­‰ï¼‰ï¼Œä¿ç•™æ¢è¡Œç¬¦
                        cleaned_buffer = buffer.lstrip(':ï¼š')
                        if cleaned_buffer:
                            content_output_count += 1
                            output = ('CONTENT', clean_for_sse_text(cleaned_buffer))
                            yield output
                        buffer = ""

            # è¾“å‡ºå‰©ä½™çš„buffer
            # 1. åŸç”Ÿæ ¼å¼çš„å‰©ä½™å†…å®¹
            if has_reasoning_content:
                if reasoning_buffer:
                    think_output_count += 1
                    output = ('THINK', clean_for_sse_text(reasoning_buffer))
                    yield output
                if content_buffer:
                    content_output_count += 1
                    output = ('CONTENT', clean_for_sse_text(content_buffer))
                    yield output
            # 2. æ–‡æœ¬æ ‡è®°æ¨¡å¼çš„å‰©ä½™å†…å®¹
            elif buffer:
                if in_thinking_section and not thinking_complete:
                    # å¦‚æœæ€è€ƒåŒºåŸŸæœªå®Œæˆï¼Œå‰©ä½™å†…å®¹ä½œä¸ºæ€è€ƒè¾“å‡º
                    think_output_count += 1
                    output = ('THINK', clean_for_sse_text(buffer))
                    yield output
                else:
                    # å¦åˆ™ä½œä¸ºæ­£æ–‡è¾“å‡ºï¼Œåªæ¸…ç†å¼€å¤´çš„æ ‡è®°ç¬¦å·ï¼Œä¿ç•™æ¢è¡Œç¬¦
                    cleaned_buffer = buffer.lstrip(':ï¼š')
                    if cleaned_buffer:
                        content_output_count += 1
                        output = ('CONTENT', clean_for_sse_text(cleaned_buffer))
                        yield output
        else:
            # ä¸å¯ç”¨æ€è€ƒæ¨¡å¼ï¼Œæ‰€æœ‰å†…å®¹éƒ½æ˜¯æ­£æ–‡
            buffer = ""
            for delta in response_stream:
                # è·å–æ–‡æœ¬å†…å®¹
                if hasattr(delta, 'delta'):
                    text = delta.delta
                elif hasattr(delta, 'text'):
                    text = delta.text
                elif hasattr(delta, 'content'):
                    text = delta.content
                else:
                    text = str(delta) if delta else ''

                if text:
                    buffer += text
                    # æ™ºèƒ½å‘é€ç­–ç•¥ï¼š
                    # 1. é‡åˆ°æ¢è¡Œç¬¦ç«‹å³å‘é€ï¼ˆä¿æŒæ¢è¡Œçš„åŠæ—¶æ€§ï¼‰
                    # 2. æˆ–è€… buffer è¾¾åˆ° 20 ä¸ªå­—ç¬¦å‘é€ï¼ˆå¹³è¡¡æ€§èƒ½ï¼‰
                    if '\n' in buffer or len(buffer) >= 20:
                        yield ('CONTENT', clean_for_sse_text(buffer))
                        buffer = ""
            
            # å‘é€å‰©ä½™å†…å®¹
            if buffer:
                yield ('CONTENT', clean_for_sse_text(buffer))

    def _format_sources(self, final_nodes):
        """æ ¼å¼åŒ–å‚è€ƒæ¥æº"""
        for i, node in enumerate(final_nodes):
            initial_score = node.node.metadata.get('initial_score', 0.0)
            retrieval_sources = node.node.metadata.get('retrieval_sources', [])
            vector_score = node.node.metadata.get('vector_score', 0.0)
            bm25_score = node.node.metadata.get('bm25_score', 0.0)
            vector_rank = node.node.metadata.get('vector_rank')
            bm25_rank = node.node.metadata.get('bm25_rank')
            
            source_data = {
                "id": i + 1,
                "fileName": node.node.metadata.get('file_name', 'æœªçŸ¥'),
                "initialScore": f"{initial_score:.4f}",
                "rerankedScore": f"{node.score:.4f}",
                "content": node.node.text.strip(),
                "retrievalSources": retrieval_sources,
                "vectorScore": f"{vector_score:.4f}",
                "bm25Score": f"{bm25_score:.4f}"
            }
            
            # æ·»åŠ æ’åä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if vector_rank is not None:
                source_data['vectorRank'] = vector_rank
            if bm25_rank is not None:
                source_data['bm25Rank'] = bm25_rank
            
            # æ·»åŠ åŒ¹é…çš„å…³é”®è¯ï¼ˆå¦‚æœæ˜¯å…³é”®è¯æ£€ç´¢ï¼‰
            if 'keyword' in retrieval_sources:
                matched_keywords = node.node.metadata.get('bm25_matched_keywords', [])
                if matched_keywords:
                    source_data['matchedKeywords'] = matched_keywords
            
            yield ('SOURCE', json.dumps(source_data, ensure_ascii=False))

    def _format_filtered_sources(self, filtered_results):
        """æ ¼å¼åŒ– InsertBlock è¿‡æ»¤åçš„å‚è€ƒæ¥æº"""
        for i, result in enumerate(filtered_results):
            source_data = {
                "id": i + 1,
                "fileName": result['file_name'],
                "initialScore": f"{result['initial_score']:.4f}",
                "rerankedScore": f"{result['reranked_score']:.4f}",
                "canAnswer": result['can_answer'],
                "reasoning": result['reasoning'],
                "keyPassage": result.get('key_passage', ''),
                "content": result['node'].node.text.strip()
            }
            yield f"SOURCE:{json.dumps(source_data, ensure_ascii=False)}"

    def _build_reference_log_entries(self, final_nodes, filtered_map=None):
        """æ„å»ºç”¨äºæ—¥å¿—è®°å½•çš„å‚è€ƒæ–‡çŒ®æ¡ç›®"""
        entries = []
        if not final_nodes:
            return entries

        for i, node in enumerate(final_nodes):
            file_name = node.node.metadata.get('file_name', 'æœªçŸ¥')
            initial_score = node.node.metadata.get('initial_score', 0.0)
            key = f"{file_name}_{node.score}"
            filtered_info = filtered_map.get(key) if filtered_map else None

            entries.append({
                "id": i + 1,
                "fileName": file_name,
                "initialScore": round(float(initial_score), 6),
                "rerankedScore": round(float(node.score or 0.0), 6),
                "canAnswer": (filtered_info is not None) if filtered_map else None,
                "reasoning": filtered_info.get('reasoning', '') if filtered_info else '',
                "keyPassage": filtered_info.get('key_passage', '') if filtered_info else '',
                "content": node.node.text.strip()
            })

        return entries

    def _log_reference_details(
        self,
        question: str,
        references: list,
        mode: str,
        session_id: Optional[str] = None
    ):
        """è®°å½•å‚è€ƒæ–‡çŒ®è¯¦æƒ…åˆ°æ—¥å¿—æ–‡ä»¶"""
        try:
            os.makedirs(Settings.LOG_DIR, exist_ok=True)
            log_path = os.path.join(Settings.LOG_DIR, "reference_logs.jsonl")
            payload = {
                "timestamp": datetime.utcnow().isoformat() + "Z",
                "mode": mode,
                "session_id": session_id,
                "question": question,
                "references": references
            }
            with open(log_path, "a", encoding="utf-8") as log_file:
                log_file.write(json.dumps(payload, ensure_ascii=False) + "\n")
        except Exception as e:
            logger.warning(f"è®°å½•å‚è€ƒæ–‡çŒ®æ—¥å¿—å¤±è´¥: {e}")

    def _save_log(self, question: str, response: str, client_ip: str, has_rag: bool, use_insert_block: bool = False):
        """ä¿å­˜é—®ç­”æ—¥å¿—"""
        from utils import QALogger
        qa_logger = QALogger(Settings.LOG_DIR)
        qa_logger.save_log(
            question,
            response,
            'knowledge_qa_stream',
            metadata={
                "ip": client_ip,
                "answer_type": "rag" if has_rag else "general",
                "chat_mode": Settings.USE_CHAT_MODE,
                "insert_block_mode": use_insert_block
            }
        )

    def process_conversation(
        self,
        question: str,
        session_id: str,
        enable_thinking: bool,
        rerank_top_n: int,
        llm,
        client_ip: str = "unknown",
        use_insert_block: bool = False,
        insert_block_llm_id: Optional[str] = None
    ) -> Generator[str, None, None]:
        """
        å¤„ç†æ”¯æŒå¤šè½®å¯¹è¯çš„çŸ¥è¯†é—®ç­”

        æµç¨‹ï¼š
        1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        2. InsertBlock è¿‡æ»¤ï¼ˆå¯é€‰ï¼‰
        3. è·å–å†å²å¯¹è¯
        4. ä½¿ç”¨çŸ¥è¯†é—®ç­”çš„æç¤ºè¯æ„å»º promptï¼ˆå°†å†å²å¯¹è¯æ³¨å…¥åˆ°ä¸Šä¸‹æ–‡ä¸­ï¼‰
        5. è°ƒç”¨ LLM
        6. å­˜å‚¨æœ¬è½®å¯¹è¯
        7. è¿”å›å‚è€ƒæ¥æº

        Args:
            question: é—®é¢˜å†…å®¹
            session_id: ä¼šè¯ID
            enable_thinking: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼
            rerank_top_n: é‡æ’åºåè¿”å›çš„æ–‡æ¡£æ•°é‡
            llm: LLM å®ä¾‹
            client_ip: å®¢æˆ·ç«¯ IP
            use_insert_block: æ˜¯å¦ä½¿ç”¨ InsertBlock è¿‡æ»¤æ¨¡å¼
            insert_block_llm_id: InsertBlock ä½¿ç”¨çš„ LLM ID

        Yields:
            SSE æ ¼å¼çš„å“åº”æµ
        """
        full_response = ""
        
        # æ¸…ç©ºä¸Šä¸€æ¬¡çš„åˆæˆç­”æ¡ˆï¼ˆé¿å…æ±¡æŸ“ï¼‰
        self._last_synthesized_answer = None

        try:
            logger.info(
                f"å¤„ç†å¤šè½®å¯¹è¯: ä¼šè¯ {session_id[:8]}... | '{question}' | "
                f"æ€è€ƒæ¨¡å¼: {enable_thinking} | InsertBlock: {use_insert_block}"
            )

            # è·å–å¯¹è¯ç®¡ç†å™¨
            from flask import current_app
            knowledge_service = current_app.knowledge_service
            conversation_manager = knowledge_service.conversation_manager

            if not conversation_manager:
                raise ValueError("å¯¹è¯ç®¡ç†å™¨æœªåˆå§‹åŒ–")

            # è¿”å›ä¼šè¯ID
            yield f"SESSION:{session_id}"

            # 1. è·å–æœ€è¿‘çš„å¯¹è¯å†å²ï¼ˆç”¨äºå­é—®é¢˜åˆ†è§£ï¼‰
            from config import Settings as AppSettings
            recent_turns_for_decomp = getattr(AppSettings, 'SUBQUESTION_HISTORY_COMPRESS_TURNS', 5)
            
            try:
                conversation_history_for_decomp = conversation_manager.get_recent_history(
                    session_id=session_id,
                    limit=recent_turns_for_decomp
                )
            except Exception as e:
                logger.warning(f"è·å–å¯¹è¯å†å²ç”¨äºå­é—®é¢˜åˆ†è§£å¤±è´¥: {e}")
                conversation_history_for_decomp = None
            
            # 2. æ£€ç´¢
            # å¦‚æœå‰ç«¯è®¾ç½®å‚è€ƒæ•°é‡ä¸º 0ï¼Œè·³è¿‡æ£€ç´¢
            if rerank_top_n == 0:
                logger.info("[å¯¹è¯-æ£€ç´¢è·³è¿‡] å‰ç«¯è®¾ç½®å‚è€ƒæ•°é‡ä¸º 0ï¼Œè·³è¿‡æ£€ç´¢å’Œå­é—®é¢˜åˆ†è§£")
                final_nodes = []
            else:
                yield "CONTENT:æ­£åœ¨è¿›è¡Œæ··åˆæ£€ç´¢..."
                full_response += "æ­£åœ¨è¿›è¡Œæ··åˆæ£€ç´¢...\n"

                final_nodes = self._retrieve_and_rerank(
                    question, 
                    rerank_top_n,
                    conversation_history=conversation_history_for_decomp
                )

            # 2. å¦‚æœå¯ç”¨ InsertBlock æ¨¡å¼ï¼Œè¿›è¡Œæ™ºèƒ½è¿‡æ»¤
            filtered_results = None
            filtered_map = None
            nodes_for_prompt = final_nodes

            if use_insert_block and final_nodes and self.insert_block_filter:
                yield "CONTENT:æ­£åœ¨ä½¿ç”¨ InsertBlock æ™ºèƒ½è¿‡æ»¤..."
                full_response += "æ­£åœ¨ä½¿ç”¨ InsertBlock æ™ºèƒ½è¿‡æ»¤...\n"

                filtered_results = self.insert_block_filter.filter_nodes(
                    question=question,
                    nodes=final_nodes,
                    llm_id=insert_block_llm_id
                )

                if filtered_results:
                    yield f"CONTENT:æ‰¾åˆ° {len(filtered_results)} ä¸ªå¯å›ç­”çš„èŠ‚ç‚¹"
                    full_response += f"æ‰¾åˆ° {len(filtered_results)} ä¸ªå¯å›ç­”çš„èŠ‚ç‚¹\n"
                    nodes_for_prompt = None
                    filtered_map = {}
                    for result in filtered_results:
                        key = f"{result['file_name']}_{result['reranked_score']}"
                        filtered_map[key] = result
                else:
                    yield "CONTENT:æœªæ‰¾åˆ°å¯ç›´æ¥å›ç­”çš„èŠ‚ç‚¹ï¼Œå°†ä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ"
                    full_response += "æœªæ‰¾åˆ°å¯ç›´æ¥å›ç­”çš„èŠ‚ç‚¹ï¼Œå°†ä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ\n"
                    filtered_results = None

            # 3. è·å–å†å²å¯¹è¯
            from config import Settings as AppSettings
            recent_turns = getattr(AppSettings, 'MAX_RECENT_TURNS', 6)
            relevant_turns = getattr(AppSettings, 'MAX_RELEVANT_TURNS', 3)
            max_summary_turns = getattr(AppSettings, 'MAX_SUMMARY_TURNS', 12)

            # 3.1 è·å–æœ€è¿‘çš„å¯¹è¯å†å²
            recent_history = conversation_manager.get_recent_history(
                session_id=session_id,
                limit=recent_turns
            )

            # 3.2 æ£€ç´¢ä¸å½“å‰é—®é¢˜ç›¸å…³çš„å†å²å¯¹è¯
            relevant_history = []
            if relevant_turns > 0:
                try:
                    relevant_history = conversation_manager.retrieve_relevant_history(
                        session_id=session_id,
                        current_query=question,
                        top_k=relevant_turns
                    )
                    # è¿‡æ»¤æ‰å·²ç»åœ¨æœ€è¿‘å¯¹è¯ä¸­çš„è½®æ¬¡ï¼ˆé¿å…é‡å¤ï¼‰
                    recent_turn_ids = {turn.get('turn_id') for turn in recent_history if turn.get('turn_id')}
                    relevant_history = [
                        turn for turn in relevant_history
                        if turn.get('turn_id') not in recent_turn_ids
                    ]
                    logger.info(f"æ£€ç´¢åˆ° {len(relevant_history)} æ¡ç›¸å…³å†å²å¯¹è¯ï¼ˆæ’é™¤æœ€è¿‘å¯¹è¯åï¼‰")
                except Exception as e:
                    logger.warning(f"æ£€ç´¢ç›¸å…³å†å²å¯¹è¯å¤±è´¥: {e}")
                    relevant_history = []

            # 4. æ„å»ºå†å²å¯¹è¯æ‘˜è¦ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
            # è·å–ä¼šè¯æ€»è½®æ•°
            try:
                all_history = conversation_manager.get_recent_history(
                    session_id=session_id,
                    limit=100  # å‡è®¾æœ€å¤š100è½®
                )
                total_turns = len(all_history)
            except Exception as e:
                logger.warning(f"è·å–æ€»å¯¹è¯è½®æ•°å¤±è´¥: {e}")
                total_turns = len(recent_history)
                all_history = recent_history

            history_summary = None

            # åªæœ‰å½“æ€»è½®æ•°è¶…è¿‡ MAX_SUMMARY_TURNS æ—¶æ‰ç”Ÿæˆæ‘˜è¦ï¼ˆé¿å…é¢‘ç¹æ‘˜è¦ï¼‰
            if total_turns > max_summary_turns:
                # æ’é™¤æœ€è¿‘Nè½®ï¼Œå‰©ä½™çš„ç”¨äºç”Ÿæˆæ‘˜è¦
                old_history = all_history[:-recent_turns] if len(all_history) > recent_turns else []

                if old_history and len(old_history) >= 3:  # è‡³å°‘3è½®æ‰å€¼å¾—æ‘˜è¦
                    # æ£€æŸ¥æ‘˜è¦ç¼“å­˜
                    cache_key = f"{session_id}_summary"
                    current_time = time.time()

                    if hasattr(conversation_manager, '_summary_cache'):
                        cache_entry = conversation_manager._summary_cache.get(cache_key)
                        if cache_entry:
                            cache_age = current_time - cache_entry.get('timestamp', 0)
                            summarized_count = cache_entry.get('summarized_until', 0)

                            # å¦‚æœç¼“å­˜æœ‰æ•ˆä¸”å¯¹è¯æ•°é‡æ²¡å˜åŒ–å¤ªå¤šï¼ˆå…è®¸Â±2è½®å·®å¼‚ï¼‰ï¼Œä½¿ç”¨ç¼“å­˜
                            if (cache_age < AppSettings.SUMMARY_CACHE_TTL and
                                abs(len(old_history) - summarized_count) <= 2):
                                history_summary = cache_entry.get('summary')
                                logger.info(f"ä½¿ç”¨ç¼“å­˜çš„å†å²æ‘˜è¦ (ç¼“å­˜æ—¶é•¿: {cache_age:.0f}s)")

                    # å¦‚æœæ²¡æœ‰ç¼“å­˜æˆ–ç¼“å­˜å¤±æ•ˆï¼Œç”Ÿæˆæ–°æ‘˜è¦
                    if not history_summary:
                        try:
                            history_summary = conversation_manager.summarize_old_conversations(
                                session_id=session_id,
                                conversations=old_history
                            )

                            # æ›´æ–°ç¼“å­˜
                            if history_summary and hasattr(conversation_manager, '_summary_cache'):
                                conversation_manager._summary_cache[cache_key] = {
                                    'summary': history_summary,
                                    'summarized_until': len(old_history),
                                    'timestamp': current_time
                                }
                                logger.info(f"å·²ç”Ÿæˆå¹¶ç¼“å­˜å†å²æ‘˜è¦ (è¦†ç›– {len(old_history)} è½®)")
                        except Exception as e:
                            logger.warning(f"ç”Ÿæˆå†å²æ‘˜è¦å¤±è´¥: {e}")
                            history_summary = None
                else:
                    logger.debug(f"æ—§å¯¹è¯è½®æ•°({len(old_history)})ä¸è¶³ï¼Œè·³è¿‡æ‘˜è¦ç”Ÿæˆ")
            else:
                logger.debug(f"æ€»è½®æ•°({total_turns})æœªè¾¾æ‘˜è¦é˜ˆå€¼({max_summary_turns})ï¼Œè·³è¿‡æ‘˜è¦")

            # 5. ä½¿ç”¨ä¼˜åŒ–çš„æç¤ºè¯æ„å»ºæ–¹å¼ï¼ˆæ³¨å…¥å†å²å¯¹è¯ï¼‰
            prompt_parts = self._build_prompt_with_history(
                question,
                enable_thinking,
                nodes_for_prompt,
                filtered_results=filtered_results,
                recent_history=recent_history,
                relevant_history=relevant_history,
                history_summary=history_summary
            )

            # 6. è¾“å‡ºçŠ¶æ€
            status_msg = (
                "å·²æ‰¾åˆ°ç›¸å…³èµ„æ–™ï¼Œæ­£åœ¨ç”Ÿæˆå›ç­”..."
                if final_nodes
                else "æœªæ‰¾åˆ°é«˜ç›¸å…³æ€§èµ„æ–™ï¼ŒåŸºäºé€šç”¨çŸ¥è¯†å’Œå¯¹è¯å†å²å›ç­”..."
            )
            yield f"CONTENT:{status_msg}"
            full_response += status_msg + "\n"

            # 7. è°ƒç”¨ LLM
            assistant_response = ""
            for result in self._call_llm(llm, prompt_parts, enable_thinking=enable_thinking):
                # result æ˜¯å…ƒç»„ (prefix_type, content)
                prefix_type, chunk = result
                if prefix_type == 'THINK':
                    yield f"THINK:{chunk}"
                    # æ€è€ƒå†…å®¹ä¸è®¡å…¥ assistant_response
                elif prefix_type == 'CONTENT':
                    yield f"CONTENT:{chunk}"
                    full_response += chunk
                    assistant_response += chunk

            # 8. å­˜å‚¨æœ¬è½®å¯¹è¯åˆ°å‘é‡åº“
            context_doc_names = []
            if final_nodes:
                context_doc_names = [
                    node.node.metadata.get('file_name', 'æœªçŸ¥')
                    for node in final_nodes
                ]

            # è·å–ä¸Šä¸€è½®å¯¹è¯çš„ turn_id ä½œä¸º parent_turn_id
            parent_turn_id = None
            try:
                if recent_history:
                    parent_turn_id = recent_history[-1].get('turn_id')
            except Exception as e:
                logger.warning(f"è·å–çˆ¶å¯¹è¯IDå¤±è´¥: {e}")

            # ç”Ÿæˆå½“å‰è½®æ¬¡çš„ turn_id
            import uuid
            current_turn_id = str(uuid.uuid4())

            # å­˜å‚¨å¯¹è¯ï¼ˆåŒ…å«å®Œæ•´çš„åŠ©æ‰‹å›ç­”ï¼Œå…¶ä¸­å·²ç»åŒ…å«äº†å®ä½“å’ŒåŠ¨ä½œåˆ†æï¼‰
            conversation_manager.add_conversation_turn(
                session_id=session_id,
                user_query=question,
                assistant_response=assistant_response,
                context_docs=context_doc_names,
                turn_id=current_turn_id,
                parent_turn_id=parent_turn_id
            )

            if use_insert_block and filtered_results:
                yield "CONTENT:\n\n**å‚è€ƒæ¥æºï¼ˆå…¨éƒ¨æ£€ç´¢ç»“æœï¼‰:**"
                full_response += "\n\nå‚è€ƒæ¥æºï¼ˆå…¨éƒ¨æ£€ç´¢ç»“æœï¼‰:"

                for i, node in enumerate(final_nodes):
                    file_name = node.node.metadata.get('file_name', 'æœªçŸ¥')
                    initial_score = node.node.metadata.get('initial_score', 0.0)
                    key = f"{file_name}_{node.score}"

                    filtered_info = filtered_map.get(key)

                    # æå–æ£€ç´¢å…ƒæ•°æ®
                    retrieval_sources = node.node.metadata.get('retrieval_sources', [])
                    vector_score = node.node.metadata.get('vector_score', 0.0)
                    bm25_score = node.node.metadata.get('bm25_score', 0.0)
                    vector_rank = node.node.metadata.get('vector_rank')
                    bm25_rank = node.node.metadata.get('bm25_rank')

                    source_data = {
                        "id": i + 1,
                        "fileName": file_name,
                        "initialScore": f"{initial_score:.4f}",
                        "rerankedScore": f"{node.score:.4f}",
                        "content": node.node.text.strip(),
                        # æ£€ç´¢å…ƒæ•°æ®
                        "retrievalSources": retrieval_sources,
                        "vectorScore": f"{vector_score:.4f}",
                        "bm25Score": f"{bm25_score:.4f}",
                        # InsertBlock ç‰¹æœ‰å­—æ®µ
                        "canAnswer": filtered_info is not None,
                        "reasoning": filtered_info.get('reasoning', '') if filtered_info else '',
                        "keyPassage": filtered_info.get('key_passage', '') if filtered_info else ''
                    }
                    
                    # æ·»åŠ æ’åä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    if vector_rank is not None:
                        source_data['vectorRank'] = vector_rank
                    if bm25_rank is not None:
                        source_data['bm25Rank'] = bm25_rank
                    
                    # æ·»åŠ åŒ¹é…çš„å…³é”®è¯ï¼ˆå¦‚æœæ˜¯å…³é”®è¯æ£€ç´¢ï¼‰
                    if 'keyword' in retrieval_sources:
                        matched_keywords = node.node.metadata.get('bm25_matched_keywords', [])
                        if matched_keywords:
                            source_data['matchedKeywords'] = matched_keywords

                    yield f"SOURCE:{json.dumps(source_data, ensure_ascii=False)}"

                    full_response += (
                        f"\n[{source_data['id']}] æ–‡ä»¶: {source_data['fileName']}, "
                        f"é‡æ’åˆ†: {source_data['rerankedScore']}, "
                        f"å¯å›ç­”: {source_data['canAnswer']}"
                    )

            elif final_nodes:
                yield "CONTENT:\n\n**å‚è€ƒæ¥æº:**"
                full_response += "\n\nå‚è€ƒæ¥æº:"

                for source_msg in self._format_sources(final_nodes):
                    # _format_sources è¿”å›å…ƒç»„ ('SOURCE', json_data)
                    prefix_type, json_data = source_msg
                    if prefix_type == 'SOURCE':
                        formatted_msg = f"SOURCE:{json_data}"
                        yield formatted_msg
                        data = json.loads(json_data)
                        full_response += (
                            f"\n[{data['id']}] æ–‡ä»¶: {data['fileName']}, "
                            f"é‡æ’åˆ†: {data['rerankedScore']}"
                        )

            reference_entries = self._build_reference_log_entries(final_nodes, filtered_map)

            self._log_reference_details(
                question=question,
                references=reference_entries,
                mode="conversation",
                session_id=session_id
            )

            yield "DONE:"

            # 10. ä¿å­˜æ—¥å¿—
            self._save_log(
                question,
                full_response,
                client_ip,
                bool(final_nodes),
                use_insert_block=use_insert_block
            )

        except Exception as e:
            error_msg = f"å¤„ç†é”™è¯¯: {str(e)}"
            logger.error(f"å¤šè½®å¯¹è¯å¤„ç†å‡ºé”™: {e}", exc_info=True)
            yield f"ERROR:{error_msg}"

    def _build_prompt_with_history(
        self,
        question: str,
        enable_thinking: bool,
        final_nodes,
        filtered_results=None,
        recent_history=None,
        relevant_history=None,
        history_summary=None
    ):
        """
        æ„é€ å¸¦å†å²å¯¹è¯çš„æç¤ºè¯ï¼ˆä½¿ç”¨çŸ¥è¯†é—®ç­”çš„æç¤ºè¯æ ¼å¼ï¼‰

        Args:
            question: å½“å‰é—®é¢˜
            enable_thinking: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼
            final_nodes: æ£€ç´¢åˆ°çš„èŠ‚ç‚¹
            filtered_results: InsertBlock è¿‡æ»¤ç»“æœ
            recent_history: æœ€è¿‘çš„å¯¹è¯å†å²
            relevant_history: ç›¸å…³çš„å†å²å¯¹è¯
            history_summary: å†å²å¯¹è¯æ‘˜è¦
        """
        # æ„å»ºçŸ¥è¯†åº“ä¸Šä¸‹æ–‡ï¼ˆä¸çŸ¥è¯†é—®ç­”ç›¸åŒçš„é€»è¾‘ï¼‰
        knowledge_context = None
        if filtered_results:
            # ä½¿ç”¨ InsertBlock è¿‡æ»¤ç»“æœ
            context_blocks = []
            block_index = 1
            
            for result in filtered_results:
                file_name = result['file_name']
                key_passage = result.get('key_passage', '')
                full_content = result['node'].node.text.strip()
                can_answer = result.get('can_answer', False)
                
                # ä¸¥æ ¼è¿‡æ»¤ï¼šåªæœ‰ can_answer=True ä¸” key_passage ä¸ä¸ºç©ºæ‰æ³¨å…¥ä¸Šä¸‹æ–‡
                if not can_answer:
                    logger.warning(f"[å¯¹è¯-ç²¾å‡†æ£€ç´¢è¿‡æ»¤] è·³è¿‡ä¸å¯å›ç­”çš„èŠ‚ç‚¹: {file_name}")
                    continue
                
                if not key_passage or key_passage.strip() == "":
                    logger.warning(f"[å¯¹è¯-ç²¾å‡†æ£€ç´¢è¿‡æ»¤] è·³è¿‡æ— å…³é”®æ®µè½çš„èŠ‚ç‚¹: {file_name} | can_answer={can_answer}")
                    continue
                
                block = f"### ä¸šåŠ¡è§„å®š {block_index} - {file_name}:\n> {full_content}"
                context_blocks.append(block)
                block_index += 1
                logger.info(f"[å¯¹è¯-ç²¾å‡†æ£€ç´¢é€šè¿‡] èŠ‚ç‚¹å·²æ³¨å…¥ä¸Šä¸‹æ–‡: {file_name} | å…³é”®æ®µè½é•¿åº¦: {len(key_passage)}")
                
            knowledge_context = "\n\n".join(context_blocks) if context_blocks else None

        elif final_nodes:
            # ä½¿ç”¨æ™®é€šæ£€ç´¢ç»“æœ
            context_blocks = []
            for i, node in enumerate(final_nodes):
                file_name = node.node.metadata.get('file_name', 'æœªçŸ¥æ–‡ä»¶')
                content = node.node.get_content().strip()
                block = f"### ä¸šåŠ¡è§„å®š {i + 1} - {file_name}:\n> {content}"
                context_blocks.append(block)
            knowledge_context = "\n\n".join(context_blocks)

        has_rag = bool(knowledge_context)
        
        # å¦‚æœæœ‰å­é—®é¢˜ç­”æ¡ˆåˆæˆï¼Œæ·»åŠ åˆ°çŸ¥è¯†åº“ä¸Šä¸‹æ–‡ä¸­
        if has_rag and self._last_synthesized_answer:
            synthesis_block = (
                f"\n\n### ğŸ¯ å­é—®é¢˜ç»¼åˆåˆ†æ:\n"
                f"> {self._last_synthesized_answer}\n\n"
                f"**æ³¨æ„**: ä»¥ä¸Šæ˜¯å¯¹å¤šä¸ªå­é—®é¢˜ç­”æ¡ˆçš„ç»¼åˆæ•´ç†ï¼Œè¯·ç»“åˆå…·ä½“ä¸šåŠ¡è§„å®šç»™å‡ºæœ€ç»ˆå›ç­”ã€‚"
            )
            knowledge_context += synthesis_block
            logger.info(f"[å¤šè½®æç¤ºè¯æ„å»º] å·²å°†åˆæˆç­”æ¡ˆæ³¨å…¥ä¸Šä¸‹æ–‡ | é•¿åº¦: {len(self._last_synthesized_answer)}")

        # æ„å»ºå†å²å¯¹è¯ä¸Šä¸‹æ–‡
        history_context = None
        if history_summary or recent_history or relevant_history:
            history_parts = []

            # æ·»åŠ æ‘˜è¦
            if history_summary:
                summary_prefix = get_conversation_summary_context_prefix()
                history_parts.append(f"{summary_prefix}{history_summary}")

            # æ·»åŠ æœ€è¿‘çš„å¯¹è¯
            if recent_history:
                recent_prefix = get_conversation_context_prefix_recent_history()
                recent_turns_text = "\n\n".join([
                    f"ç”¨æˆ·: {turn['user_query']}\nåŠ©æ‰‹: {turn['assistant_response']}"
                    for turn in recent_history
                ])
                history_parts.append(f"{recent_prefix}{recent_turns_text}")

            # æ·»åŠ ç›¸å…³å†å²å¯¹è¯
            if relevant_history:
                relevant_prefix = get_conversation_context_prefix_relevant_history()
                relevant_turns_text = "\n\n".join([
                    f"ç”¨æˆ·: {turn['user_query']}\nåŠ©æ‰‹: {turn['assistant_response']}"
                    for turn in relevant_history
                ])
                history_parts.append(f"{relevant_prefix}{relevant_turns_text}")

            history_context = "\n\n".join(history_parts)

        # ä½¿ç”¨çŸ¥è¯†é—®ç­”çš„æç¤ºè¯é€»è¾‘
        if has_rag:
            # è·å–å‰ç¼€
            assistant_prefix = get_knowledge_assistant_context_prefix()

            # ç»„åˆä¸Šä¸‹æ–‡ï¼šå†å²å¯¹è¯ + ä¸šåŠ¡è§„å®š
            context_parts = []
            if history_context:
                context_parts.append(history_context)
            context_parts.append(assistant_prefix + knowledge_context)

            assistant_context = "\n\n---\n\n".join(context_parts)

            # æ ¹æ®æ€è€ƒæ¨¡å¼é€‰æ‹©ä¸åŒçš„ system å’Œ user prompt
            if enable_thinking:
                system_prompt = get_knowledge_system_rag_advanced()
                user_template = get_knowledge_user_rag_advanced()
            else:
                system_prompt = get_knowledge_system_rag_simple()
                user_template = get_knowledge_user_rag_simple()

            # user_template æ˜¯åˆ—è¡¨ï¼Œéœ€è¦ join åå† format
            user_prompt_str = "\n".join(user_template) if isinstance(user_template, list) else user_template
            # å¦‚æœå…³é—­æ€è€ƒæ¨¡å¼ï¼Œè‡ªåŠ¨åœ¨é—®é¢˜åè¿½åŠ  /no_think æŒ‡ä»¤ï¼ˆé˜¿é‡Œäº‘æ–‡æ¡£å»ºè®®ï¼‰
            actual_question = f"{question}/no_think" if not enable_thinking else question
            user_prompt = user_prompt_str.format(question=actual_question)

        else:
            # æ²¡æœ‰æ£€ç´¢åˆ°ç›¸å…³å†…å®¹ï¼Œåªæœ‰å†å²å¯¹è¯
            assistant_context = history_context

            if enable_thinking:
                system_prompt = get_knowledge_system_no_rag_think()
                user_template = get_knowledge_user_no_rag_think()
            else:
                system_prompt = get_knowledge_system_no_rag_simple()
                user_template = get_knowledge_user_no_rag_simple()

            # user_template å¯èƒ½æ˜¯åˆ—è¡¨æˆ–å­—ç¬¦ä¸²
            user_prompt_str = "\n".join(user_template) if isinstance(user_template, list) else user_template
            # å¦‚æœå…³é—­æ€è€ƒæ¨¡å¼ï¼Œè‡ªåŠ¨åœ¨é—®é¢˜åè¿½åŠ  /no_think æŒ‡ä»¤ï¼ˆé˜¿é‡Œäº‘æ–‡æ¡£å»ºè®®ï¼‰
            actual_question = f"{question}/no_think" if not enable_thinking else question
            user_prompt = user_prompt_str.format(question=actual_question)

        # system_prompt å¯èƒ½æ˜¯åˆ—è¡¨ï¼Œéœ€è¦è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        if isinstance(system_prompt, list):
            system_prompt = "\n".join(system_prompt)

        # æ„å»º fallback_promptï¼ˆç”¨äºä¸æ”¯æŒ chat æ¨¡å¼çš„æƒ…å†µï¼‰
        fallback_parts = [system_prompt]
        if assistant_context:
            fallback_parts.append(assistant_context)
        fallback_parts.append(user_prompt)

        return {
            "system_prompt": system_prompt,
            "user_prompt": user_prompt,
            "assistant_context": assistant_context,
            "fallback_prompt": "\n\n".join(fallback_parts)
        }
    
    def _smart_retrieve_and_rerank(self, question: str, rerank_top_n: int, conversation_history: Optional[List[Dict]] = None):
        """
        æ™ºèƒ½è·¯ç”±æ£€ç´¢ï¼šå…ˆæ„å›¾åˆ†ç±»é€‰æ‹©çŸ¥è¯†åº“ï¼Œå†å¯é€‰å­é—®é¢˜åˆ†è§£
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            rerank_top_n: é‡æ’åºåè¿”å›çš„æ–‡æ¡£æ•°é‡
            conversation_history: å¯¹è¯å†å²ï¼ˆç”¨äºå­é—®é¢˜åˆ†è§£ï¼‰
            
        Returns:
            é‡æ’åºåçš„èŠ‚ç‚¹åˆ—è¡¨
        """
        # 1. æ„å›¾åˆ†ç±»ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        strategy = "general"  # é»˜è®¤ç­–ç•¥ï¼šåªç”¨é€šç”¨åº“
        
        if self.intent_classifier:
            try:
                strategy = self.intent_classifier.classify(question)
                logger.info(f"[æ™ºèƒ½è·¯ç”±] æ„å›¾åˆ†ç±»ç»“æœ: {strategy}")
            except Exception as e:
                logger.warning(f"[æ™ºèƒ½è·¯ç”±] æ„å›¾åˆ†ç±»å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥: general")
                strategy = "general"
        else:
            logger.info("[æ™ºèƒ½è·¯ç”±] æ„å›¾åˆ†ç±»å™¨æœªå¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥: general")
        
        # 2. æ ¹æ®ç­–ç•¥é€‰æ‹©æ£€ç´¢å™¨
        if strategy == "both" and self.multi_kb_retriever:
            # åŒåº“æ£€ç´¢
            logger.info("[æ™ºèƒ½è·¯ç”±] ä½¿ç”¨åŒåº“æ£€ç´¢ï¼ˆå…ç­¾åº“ + é€šç”¨åº“ï¼‰")
            selected_retriever = self.multi_kb_retriever
        elif strategy == "visa_free" and self.visa_free_retriever:
            # åªç”¨å…ç­¾åº“
            logger.info("[æ™ºèƒ½è·¯ç”±] ä½¿ç”¨å…ç­¾çŸ¥è¯†åº“")
            selected_retriever = self.visa_free_retriever
        else:
            # åªç”¨é€šç”¨åº“ï¼ˆé»˜è®¤ï¼‰
            logger.info("[æ™ºèƒ½è·¯ç”±] ä½¿ç”¨é€šç”¨çŸ¥è¯†åº“")
            selected_retriever = self.retriever
        
        # 3. å°è¯•å­é—®é¢˜åˆ†è§£ï¼ˆå¦‚æœå¯ç”¨ï¼‰ï¼Œä½¿ç”¨è·¯ç”±åçš„æ£€ç´¢å™¨
        if self.sub_question_decomposer and self.sub_question_decomposer.enabled:
            logger.info(f"[æ£€ç´¢ç­–ç•¥] å°è¯•ä½¿ç”¨å­é—®é¢˜åˆ†è§£æ£€ç´¢ï¼ˆå•è½®ï¼‰ | ç›®æ ‡åº“: {strategy}")
            try:
                nodes, metadata = self.sub_question_decomposer.retrieve_with_decomposition(
                    query=question,
                    rerank_top_n=rerank_top_n,
                    conversation_history=conversation_history,
                    retriever=selected_retriever  # ä¼ å…¥è·¯ç”±åçš„æ£€ç´¢å™¨
                )
                
                # è®°å½•åˆ†è§£å…ƒæ•°æ®
                if metadata.get('decomposed'):
                    logger.info(
                        f"[å­é—®é¢˜æ£€ç´¢] åˆ†è§£æ£€ç´¢å®Œæˆ | "
                        f"å­é—®é¢˜æ•°: {len(metadata['sub_questions'])} | "
                        f"è¿”å›èŠ‚ç‚¹æ•°: {len(nodes)} | "
                        f"ä½¿ç”¨åº“: {strategy}"
                    )
                    
                    # ä¿å­˜å­é—®é¢˜ç­”æ¡ˆï¼ˆç”¨äºæ³¨å…¥ä¸Šä¸‹æ–‡å’Œè¿”å›å‰ç«¯ï¼‰
                    if metadata.get('sub_answers') and len(metadata['sub_answers']) > 0:
                        # å­˜å‚¨å­é—®é¢˜ç­”æ¡ˆï¼Œä¾› _build_prompt ä½¿ç”¨
                        self._last_sub_answers = metadata['sub_answers']
                        logger.info(f"[å­é—®é¢˜ç­”æ¡ˆ] å·²ä¿å­˜ {len(metadata['sub_answers'])} ä¸ªå­é—®é¢˜ç­”æ¡ˆï¼Œå°†æ³¨å…¥ä¸Šä¸‹æ–‡")
                        
                        # å¯é€‰ï¼šç”Ÿæˆå­é—®é¢˜ç­”æ¡ˆåˆæˆ
                        try:
                            synthesized_answer = self.sub_question_decomposer.synthesize_answer(
                                original_query=question,
                                sub_answers=metadata['sub_answers']
                            )
                            if synthesized_answer:
                                # å°†åˆæˆç­”æ¡ˆæ·»åŠ åˆ°metadataï¼Œä¾›åç»­ä½¿ç”¨
                                metadata['synthesized_answer'] = synthesized_answer
                                # å­˜å‚¨ä¸ºå®ä¾‹å˜é‡ï¼Œä¾›_build_promptä½¿ç”¨
                                self._last_synthesized_answer = synthesized_answer
                                logger.info(f"[ç­”æ¡ˆåˆæˆ] å·²ç”Ÿæˆåˆæˆç­”æ¡ˆ | é•¿åº¦: {len(synthesized_answer)}")
                        except Exception as synth_e:
                            logger.warning(f"[ç­”æ¡ˆåˆæˆ] åˆæˆå¤±è´¥: {synth_e}")
                    
                    # è¿”å›èŠ‚ç‚¹å’Œå…ƒæ•°æ®
                    return nodes, metadata
                else:
                    logger.info("[å­é—®é¢˜æ£€ç´¢] æœªåˆ†è§£ï¼Œç»§ç»­æ ‡å‡†æ£€ç´¢æµç¨‹")
                    # æ¸…ç©ºå­é—®é¢˜ç­”æ¡ˆï¼Œé¿å…ä½¿ç”¨æ—§æ•°æ®
                    self._last_sub_answers = None
                    # ç»§ç»­æ‰§è¡Œæ ‡å‡†æ£€ç´¢
                    
            except Exception as e:
                logger.error(f"[å­é—®é¢˜æ£€ç´¢] åˆ†è§£æ£€ç´¢å¤±è´¥: {e}", exc_info=True)
                logger.info("[å­é—®é¢˜æ£€ç´¢] å›é€€åˆ°æ ‡å‡†æ£€ç´¢æµç¨‹")
                # æ¸…ç©ºå­é—®é¢˜ç­”æ¡ˆï¼Œé¿å…ä½¿ç”¨æ—§æ•°æ®
                self._last_sub_answers = None
                # ç»§ç»­æ‰§è¡Œæ ‡å‡†æ£€ç´¢
        
        # 4. æ ‡å‡†æ£€ç´¢å’Œé‡æ’åº
        return self._retrieve_and_rerank_with_retriever(
            question, 
            rerank_top_n, 
            selected_retriever
        )
    
    def _retrieve_and_rerank_with_retriever(
        self, 
        question: str, 
        rerank_top_n: int,
        retriever
    ):
        """
        ä½¿ç”¨æŒ‡å®šæ£€ç´¢å™¨è¿›è¡Œæ£€ç´¢å’Œé‡æ’åº
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            rerank_top_n: é‡æ’åºåè¿”å›çš„æ–‡æ¡£æ•°é‡
            retriever: æ£€ç´¢å™¨å®ä¾‹
            
        Returns:
            é‡æ’åºåçš„èŠ‚ç‚¹åˆ—è¡¨
        """
        # åˆ›å»º QueryBundleï¼ˆé‡æ’åºéœ€è¦ï¼‰
        query_bundle = QueryBundle(query_str=question)
        
        # åˆ¤æ–­æ˜¯å¦ä¸º MultiKBRetriever
        from core.multi_kb_retriever import MultiKBRetriever
        if isinstance(retriever, MultiKBRetriever):
            # MultiKBRetriever ä½¿ç”¨ retrieve_from_both æ–¹æ³•ï¼Œç›´æ¥ä¼ å…¥ query å­—ç¬¦ä¸²
            retrieved_nodes = retriever.retrieve_from_both(question)
        else:
            # å…¶ä»–æ£€ç´¢å™¨ä½¿ç”¨æ ‡å‡†çš„ retrieve æ–¹æ³•ï¼Œéœ€è¦ QueryBundle
            retrieved_nodes = retriever.retrieve(query_bundle)
        
        logger.info(f"æ£€ç´¢åˆ° {len(retrieved_nodes)} ä¸ªåˆæ­¥ç»“æœ")
        
        if not retrieved_nodes:
            logger.warning("æœªæ£€ç´¢åˆ°ä»»ä½•ç›¸å…³æ–‡æ¡£")
            return []
        
        # æ·»åŠ æ—¥å¿—ï¼šæ£€ç´¢åçš„åˆ†æ•°
        if retrieved_nodes:
            retrieval_scores = [n.score for n in retrieved_nodes[:5]]
            logger.info(f"æ£€ç´¢é˜¶æ®µTop5å¾—åˆ†: {[f'{s:.4f}' for s in retrieval_scores]}")
        
        # ä¿å­˜åŸå§‹èŠ‚ç‚¹çš„æ£€ç´¢å…ƒæ•°æ®ï¼ˆé‡æ’åºå¯èƒ½ä¼šä¸¢å¤±ï¼‰
        original_metadata = {}
        for node in retrieved_nodes:
            node_id = node.node.node_id
            original_metadata[node_id] = {
                'retrieval_sources': node.node.metadata.get('retrieval_sources', []),
                'vector_score': node.node.metadata.get('vector_score', 0.0),
                'bm25_score': node.node.metadata.get('bm25_score', 0.0),
                'bm25_matched_keywords': node.node.metadata.get('bm25_matched_keywords', []),
                'bm25_query_keywords': node.node.metadata.get('bm25_query_keywords', []),
                'vector_rank': node.node.metadata.get('vector_rank'),
                'bm25_rank': node.node.metadata.get('bm25_rank'),
                'initial_score': node.node.metadata.get('initial_score', node.score)
            }
        
        # é‡æ’åº
        reranked_nodes = self.reranker.postprocess_nodes(
            retrieved_nodes,
            query_bundle=query_bundle
        )
        
        logger.info(f"é‡æ’åºåä¿ç•™ {len(reranked_nodes)} ä¸ªç»“æœ")
        
        # æ¢å¤åŸå§‹èŠ‚ç‚¹çš„æ£€ç´¢å…ƒæ•°æ®
        for node in reranked_nodes:
            node_id = node.node.node_id
            if node_id in original_metadata:
                metadata = original_metadata[node_id]
                node.node.metadata.update(metadata)
        
        logger.info(f"å·²æ¢å¤ {len([n for n in reranked_nodes if n.node.metadata.get('retrieval_sources')])} ä¸ªèŠ‚ç‚¹çš„æ£€ç´¢å…ƒæ•°æ®")
        
        # æ·»åŠ æ—¥å¿—ï¼šé‡æ’åºåçš„åˆ†æ•°
        if reranked_nodes:
            rerank_scores = [n.score for n in reranked_nodes[:5]]
            logger.info(f"é‡æ’åºé˜¶æ®µTop5å¾—åˆ†: {[f'{s:.4f}' for s in rerank_scores]}")
        
        # æ–¹æ¡ˆ1+3ç»„åˆï¼šæŒ‰å¾—åˆ†æ’åºåä¸¥æ ¼æˆªæ–­åˆ°å‰ç«¯è¦æ±‚çš„æ•°é‡
        # ç¡®ä¿æŒ‰åˆ†æ•°ä»é«˜åˆ°ä½æ’åº
        reranked_nodes.sort(key=lambda x: x.score, reverse=True)
        
        # ä¸¥æ ¼æŒ‰ç…§å‰ç«¯ä¼ å…¥çš„ rerank_top_n å‚æ•°æˆªæ–­
        final_nodes = reranked_nodes[:rerank_top_n]
        
        if final_nodes:
            logger.info(
                f"æœ€ç»ˆè¿”å› {len(final_nodes)} ä¸ªæ–‡æ¡£ï¼ˆä¸¥æ ¼æŒ‰å‰ç«¯å‚æ•° top_k={rerank_top_n} æˆªæ–­ï¼‰ | "
                f"æœ€é«˜åˆ†: {final_nodes[0].score:.4f} | "
                f"æœ€ä½åˆ†: {final_nodes[-1].score:.4f}"
            )
        
        return final_nodes

    def debug_inspect_scores(
        self,
        question: str,
        *,
        retriever=None,
        match_substring: Optional[str] = None,
        match_node_id: Optional[str] = None,
        max_candidates: int = 50,
        include_full_text: bool = False,
        run_reranker: bool = True
    ) -> Dict[str, Any]:
        """
        è°ƒè¯•è¾…åŠ©ï¼šæŸ¥çœ‹æ£€ç´¢/é‡æ’åºé˜¶æ®µçš„èŠ‚ç‚¹å¾—åˆ†ã€‚
        ä»…åœ¨æ˜¾å¼è°ƒç”¨æ—¶æ‰§è¡Œï¼Œä¸å½±å“ç°æœ‰æµç¨‹ã€‚
        """
        if not question:
            raise ValueError("question ä¸èƒ½ä¸ºç©º")

        active_retriever = retriever or self.retriever
        if active_retriever is None:
            raise RuntimeError("æœªé…ç½®æ£€ç´¢å™¨ï¼Œæ— æ³•æ‰§è¡Œè°ƒè¯•")

        if run_reranker and self.reranker is None:
            raise RuntimeError("æœªé…ç½®é‡æ’å™¨ï¼Œæ— æ³•æ‰§è¡Œè°ƒè¯•")

        query_bundle = QueryBundle(query_str=question)

        def _execute_retriever() -> List[Any]:
            """å…¼å®¹å¤šçŸ¥è¯†åº“æ£€ç´¢å™¨çš„è°ƒç”¨æ–¹å¼"""
            try:
                from core.multi_kb_retriever import MultiKBRetriever
            except ImportError:
                MultiKBRetriever = None  # type: ignore

            if MultiKBRetriever and isinstance(active_retriever, MultiKBRetriever):
                return active_retriever.retrieve_from_all_three(question)

            return active_retriever.retrieve(query_bundle)

        def _serialize_nodes(nodes: List[Any], stage: str) -> List[Dict[str, Any]]:
            serialized = []
            for idx, node_score in enumerate(nodes[:max_candidates], start=1):
                node = node_score.node
                metadata = node.metadata or {}
                text = node.get_content()
                preview = text[:120].replace("\n", " ").strip()
                vector_rank = metadata.get("vector_rank")
                bm25_rank = metadata.get("bm25_rank")
                sources = metadata.get("retrieval_sources") or []
                source_label = "/".join(sources) if sources else "unknown"

                entry = {
                    "stage": stage,
                    "rank": idx,
                    "node_id": node.node_id,
                    "score": float(node_score.score or 0.0),
                    "vector_score": float(metadata.get("vector_score", 0.0)),
                    "bm25_score": float(metadata.get("bm25_score", 0.0)),
                    "vector_rank": vector_rank,
                    "bm25_rank": bm25_rank,
                    "sources": sources,
                    "source_label": source_label,
                    "file_name": metadata.get("file_name"),
                    "file_path": metadata.get("file_path"),
                    "text_preview": preview,
                    "metadata": metadata
                }

                if include_full_text:
                    entry["text"] = text

                serialized.append(entry)

            return serialized

        def _is_match(entry: Dict[str, Any]) -> bool:
            if not match_substring and not match_node_id:
                return False

            matched = True

            if match_substring:
                needle = match_substring.lower()
                haystack = [
                    (entry.get("text_preview") or "").lower(),
                    (entry.get("file_name") or "").lower(),
                ]
                if include_full_text:
                    haystack.append((entry.get("text") or "").lower())
                matched = any(needle in segment for segment in haystack)

            if matched and match_node_id:
                matched = match_node_id in (entry.get("node_id") or "")

            return matched

        retrieved_nodes = _execute_retriever() or []
        retrieval_serialized = _serialize_nodes(retrieved_nodes, stage="retrieval")

        rerank_serialized: List[Dict[str, Any]] = []
        if run_reranker and retrieved_nodes:
            reranked = self.reranker.postprocess_nodes(
                retrieved_nodes,
                query_bundle=query_bundle
            )
            reranked.sort(key=lambda x: x.score, reverse=True)
            rerank_serialized = _serialize_nodes(reranked, stage="rerank")

        matched_entries = []
        for entry in retrieval_serialized + rerank_serialized:
            if _is_match(entry):
                matched_entries.append(entry)

        return {
            "question": question,
            "retriever_type": type(active_retriever).__name__,
            "retrieval": retrieval_serialized,
            "rerank": rerank_serialized,
            "matches": matched_entries,
            "match_conditions": {
                "substring": match_substring,
                "node_id": match_node_id
            }
        }
