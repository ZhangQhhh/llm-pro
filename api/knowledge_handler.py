# -*- coding: utf-8 -*-
"""
çŸ¥è¯†é—®ç­”å¤„ç†å™¨
å¤„ç†çŸ¥è¯†åº“é—®ç­”çš„ä¸šåŠ¡é€»è¾‘
"""
import json
from typing import Generator, Dict, Any, Optional
from llama_index.core import QueryBundle
from config import Settings
from utils import logger, clean_for_sse_text
from prompts import (
    get_knowledge_assistant_context_prefix,
    get_knowledge_system_rag_simple,
    get_knowledge_system_rag_advanced,
    get_knowledge_system_no_rag_think,
    get_knowledge_system_no_rag_simple,
    get_knowledge_user_rag_simple,
    get_knowledge_user_rag_advanced,
    get_knowledge_user_no_rag_think,
    get_knowledge_user_no_rag_simple,
    get_conversation_system_rag_with_history,
    get_conversation_system_general_with_history,
    get_conversation_context_prefix_relevant_history,
    get_conversation_context_prefix_recent_history,
    get_conversation_context_prefix_regulations,
    get_conversation_user_rag_query,
    get_conversation_user_general_query,
    get_conversation_summary_system,
    get_conversation_summary_user,
    get_conversation_summary_context_prefix
)


class KnowledgeHandler:
    """çŸ¥è¯†é—®ç­”å¤„ç†å™¨"""

    def __init__(self, retriever, reranker, llm_wrapper, llm_service=None):
        self.retriever = retriever
        self.reranker = reranker
        self.llm_wrapper = llm_wrapper
        self.llm_service = llm_service
        self.insert_block_filter = None

        # å¦‚æœæä¾›äº† llm_serviceï¼Œåˆå§‹åŒ– InsertBlock è¿‡æ»¤å™¨
        if llm_service:
            from core.node_filter import InsertBlockFilter
            self.insert_block_filter = InsertBlockFilter(llm_service)
            logger.info("InsertBlock è¿‡æ»¤å™¨å·²åˆå§‹åŒ–")

    def process(
        self,
        question: str,
        enable_thinking: bool,
        rerank_top_n: int,
        llm,
        client_ip: str = "unknown",
        use_insert_block: bool = False,
        insert_block_llm_id: Optional[str] = None
    ) -> Generator[str, None, None]:
        """
        å¤„ç†çŸ¥è¯†é—®ç­”

        Args:
            question: é—®é¢˜å†…å®¹
            enable_thinking: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼
            rerank_top_n: é‡æ’åºåè¿”å›çš„æ–‡æ¡£æ•°é‡
            llm: LLM å®ä¾‹
            client_ip: å®¢æˆ·ç«¯ IP
            use_insert_block: æ˜¯å¦ä½¿ç”¨ InsertBlock è¿‡æ»¤æ¨¡å¼
            insert_block_llm_id: InsertBlock ä½¿ç”¨çš„ LLM ID

        Yields:
            SSE æ ¼å¼çš„å“åº”æµ
        """
        full_response = ""

        try:
            logger.info(
                f"å¤„ç†çŸ¥è¯†é—®ç­”: '{question}' | "
                f"æ€è€ƒæ¨¡å¼: {enable_thinking} | "
                f"å‚è€ƒæ–‡ä»¶æ•°: {rerank_top_n} | "
                f"InsertBlock: {use_insert_block}"
            )

            # 1. æ£€ç´¢
            yield "CONTENT:æ­£åœ¨è¿›è¡Œæ··åˆæ£€ç´¢..."
            full_response += "æ­£åœ¨è¿›è¡Œæ··åˆæ£€ç´¢...\n"

            final_nodes = self._retrieve_and_rerank(question, rerank_top_n)

            # 2. å¦‚æœå¯ç”¨ InsertBlock æ¨¡å¼ï¼Œè¿›è¡Œæ™ºèƒ½è¿‡æ»¤
            filtered_results = None
            nodes_for_prompt = final_nodes  # é»˜è®¤ä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ

            if use_insert_block and final_nodes and self.insert_block_filter:
                yield "CONTENT:æ­£åœ¨ä½¿ç”¨ InsertBlock æ™ºèƒ½è¿‡æ»¤..."
                full_response += "æ­£åœ¨ä½¿ç”¨ InsertBlock æ™ºèƒ½è¿‡æ»¤...\n"

                filtered_results = self.insert_block_filter.filter_nodes(
                    question=question,
                    nodes=final_nodes,
                    llm_id=insert_block_llm_id
                )

                if filtered_results:
                    yield f"CONTENT:æ‰¾åˆ° {len(filtered_results)} ä¸ªå¯å›ç­”çš„èŠ‚ç‚¹"
                    full_response += f"æ‰¾åˆ° {len(filtered_results)} ä¸ªå¯å›ç­”çš„èŠ‚ç‚¹\n"
                    # InsertBlock æˆåŠŸï¼šåªä½¿ç”¨è¿‡æ»¤åçš„èŠ‚ç‚¹
                    nodes_for_prompt = None  # ä¸å†ä¼ å…¥åŸå§‹èŠ‚ç‚¹
                else:
                    yield "CONTENT:æœªæ‰¾åˆ°å¯ç›´æ¥å›ç­”çš„èŠ‚ç‚¹ï¼Œå°†ä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ"
                    full_response += "æœªæ‰¾åˆ°å¯ç›´æ¥å›ç­”çš„èŠ‚ç‚¹ï¼Œå°†ä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ\n"
                    # InsertBlock å¤±è´¥ï¼šç»§ç»­ä½¿ç”¨åŸå§‹èŠ‚ç‚¹ï¼Œæ¸…ç©ºè¿‡æ»¤ç»“æœ
                    filtered_results = None

            # 3. æ„é€ æç¤ºè¯
            prompt_parts = self._build_prompt(
                question,
                enable_thinking,
                nodes_for_prompt,  # æ ¹æ® InsertBlock ç»“æœå†³å®šä¼ å…¥å“ªäº›èŠ‚ç‚¹
                filtered_results=filtered_results
            )

            # 4. è¾“å‡ºçŠ¶æ€
            status_msg = (
                "å·²æ‰¾åˆ°ç›¸å…³èµ„æ–™ï¼Œæ­£åœ¨ç”Ÿæˆå›ç­”..."
                if final_nodes
                else "æœªæ‰¾åˆ°é«˜ç›¸å…³æ€§èµ„æ–™ï¼ŒåŸºäºé€šç”¨çŸ¥è¯†å›ç­”..."
            )
            yield f"CONTENT:{status_msg}"
            full_response += status_msg + "\n"

            # 5. è°ƒç”¨ LLM
            for chunk in self._call_llm(llm, prompt_parts):
                yield f"CONTENT:{chunk}"
                full_response += chunk

            # 6. è¾“å‡ºå‚è€ƒæ¥æº
            if use_insert_block and filtered_results:
                # InsertBlock æ¨¡å¼ï¼šè¿”å›æ‰€æœ‰åŸå§‹èŠ‚ç‚¹ï¼Œä½†æ ‡æ³¨å“ªäº›è¢«é€‰ä¸­
                yield "CONTENT:\n\n**å‚è€ƒæ¥æºï¼ˆå…¨éƒ¨æ£€ç´¢ç»“æœï¼‰:**"
                full_response += "\n\nå‚è€ƒæ¥æºï¼ˆå…¨éƒ¨æ£€ç´¢ç»“æœï¼‰:"

                # æ„å»ºè¿‡æ»¤ç»“æœçš„æ˜ å°„ï¼ˆç”¨äºå¿«é€ŸæŸ¥æ‰¾ï¼‰
                filtered_map = {}
                for result in filtered_results:
                    # é€šè¿‡æ–‡ä»¶åå’Œå†…å®¹åŒ¹é…åŸå§‹èŠ‚ç‚¹
                    key = f"{result['file_name']}_{result['reranked_score']}"
                    filtered_map[key] = result

                # éå†æ‰€æœ‰åŸå§‹èŠ‚ç‚¹ï¼Œæ ‡æ³¨å“ªäº›è¢«é€‰ä¸­
                for i, node in enumerate(final_nodes):
                    file_name = node.node.metadata.get('file_name', 'æœªçŸ¥')
                    initial_score = node.node.metadata.get('initial_score', 0.0)
                    key = f"{file_name}_{node.score}"

                    # æ£€æŸ¥è¯¥èŠ‚ç‚¹æ˜¯å¦åœ¨è¿‡æ»¤ç»“æœä¸­
                    filtered_info = filtered_map.get(key)

                    source_data = {
                        "id": i + 1,
                        "fileName": file_name,
                        "initialScore": f"{initial_score:.4f}",
                        "rerankedScore": f"{node.score:.4f}",
                        "content": node.node.text.strip(),
                        # æ–°å¢å­—æ®µ
                        "canAnswer": filtered_info is not None,
                        "reasoning": filtered_info.get('reasoning', '') if filtered_info else '',
                        "keyPassage": filtered_info.get('key_passage', '') if filtered_info else ''
                    }

                    yield f"SOURCE:{json.dumps(source_data, ensure_ascii=False)}"

                    full_response += (
                        f"\n[{source_data['id']}] æ–‡ä»¶: {source_data['fileName']}, "
                        f"é‡æ’åˆ†: {source_data['rerankedScore']}, "
                        f"å¯å›ç­”: {source_data['canAnswer']}"
                    )

            elif final_nodes:
                # æ™®é€šæ¨¡å¼ï¼šæ˜¾ç¤ºæ‰€æœ‰æ£€ç´¢ç»“æœ
                yield "CONTENT:\n\n**å‚è€ƒæ¥æº:**"
                full_response += "\n\nå‚è€ƒæ¥æº:"

                for source_msg in self._format_sources(final_nodes):
                    yield source_msg
                    if source_msg.startswith("SOURCE:"):
                        data = json.loads(source_msg[7:])
                        full_response += (
                            f"\n[{data['id']}] æ–‡ä»¶: {data['fileName']}, "
                            f"åˆå§‹åˆ†: {data['initialScore']}, "
                            f"é‡æ’åˆ†: {data['rerankedScore']}"
                        )

            yield "DONE:"

            # 7. ä¿å­˜æ—¥å¿—
            self._save_log(
                question,
                full_response,
                client_ip,
                bool(final_nodes),
                use_insert_block=use_insert_block
            )

        except Exception as e:
            error_msg = f"å¤„ç†é”™è¯¯: {str(e)}"
            logger.error(f"çŸ¥è¯†é—®ç­”å¤„ç†å‡ºé”™: {e}", exc_info=True)
            yield f"ERROR:{error_msg}"

    def _retrieve_and_rerank(self, question: str, rerank_top_n: int):
        """æ£€ç´¢å’Œé‡æ’åº"""
        # åˆå§‹æ£€ç´¢
        retrieved_nodes = self.retriever.retrieve(question)

        # å–å‰ N ä¸ªé€å…¥é‡æ’
        reranker_input_top_n = Settings.RERANKER_INPUT_TOP_N
        reranker_input = retrieved_nodes[:reranker_input_top_n]

        logger.info(
            f"åˆæ£€ç´¢æ‰¾åˆ° {len(retrieved_nodes)} ä¸ªèŠ‚ç‚¹, "
            f"é€‰å–å‰ {len(reranker_input)} ä¸ªé€å…¥é‡æ’"
        )

        # é‡æ’åº
        if reranker_input:
            reranked_nodes = self.reranker.postprocess_nodes(
                reranker_input,
                query_bundle=QueryBundle(question)
            )
        else:
            reranked_nodes = []

        # é˜ˆå€¼è¿‡æ»¤
        threshold = Settings.RERANK_SCORE_THRESHOLD
        final_nodes = [
            node for node in reranked_nodes
            if node.score >= threshold
        ]

        logger.info(
            f"é‡æ’åºåæœ‰ {len(reranked_nodes)} ä¸ªèŠ‚ç‚¹, "
            f"ç»è¿‡é˜ˆå€¼ {threshold} è¿‡æ»¤åå‰©ä¸‹ {len(final_nodes)} ä¸ª"
        )

        # åº”ç”¨æœ€ç»ˆæ•°é‡é™åˆ¶
        return final_nodes[:rerank_top_n]

    def _build_prompt(
        self,
        question: str,
        enable_thinking: bool,
        final_nodes,
        filtered_results=None
    ):
        """æ„é€ æç¤ºè¯"""
        # å¦‚æœæœ‰ InsertBlock è¿‡æ»¤ç»“æœï¼Œä¼˜å…ˆä½¿ç”¨
        if filtered_results:
            # åŒæ—¶ä½¿ç”¨å…³é”®æ®µè½å’Œå®Œæ•´å†…å®¹æ„å»ºä¸Šä¸‹æ–‡
            context_blocks = []
            for i, result in enumerate(filtered_results):
                file_name = result['file_name']
                key_passage = result.get('key_passage', '')
                full_content = result['node'].node.text.strip()

                # æ„å»ºåŒ…å«å…³é”®æ®µè½å’Œå®Œæ•´å†…å®¹çš„å—
                if key_passage:
                    # å¦‚æœæœ‰å…³é”®æ®µè½ï¼Œå…ˆå±•ç¤ºå…³é”®æ®µè½ï¼Œå†å±•ç¤ºå®Œæ•´å†…å®¹
                    block = (
                        f"### æ¥æº {i + 1} - {file_name}:\n"
                        # f"**ã€å…³é”®æ®µè½ã€‘**\n> {key_passage}\n\n"
                        f"**ã€å®Œæ•´å†…å®¹ã€‘**\n> {full_content}"
                    )
                else:
                    # å¦‚æœæ²¡æœ‰å…³é”®æ®µè½ï¼Œåªå±•ç¤ºå®Œæ•´å†…å®¹
                    block = f"### æ¥æº {i + 1} - {file_name}:\n> {full_content}"
                    logger.warning(f"èŠ‚ç‚¹é€šè¿‡ç­›é€‰ä½†æ²¡æœ‰å…³é”®æ®µè½: {file_name}")

                context_blocks.append(block)

            formatted_context = "\n\n".join(context_blocks) if context_blocks else None
            has_rag = bool(context_blocks)

            logger.info(
                f"ä½¿ç”¨ InsertBlock ç»“æœæ„å»ºä¸Šä¸‹æ–‡: {len(context_blocks)} ä¸ªæ®µè½ "
                f"(åŒ…å«å…³é”®æ®µè½+å®Œæ•´å†…å®¹)"
            )
        elif final_nodes:
            # æ ¼å¼åŒ–ä¸Šä¸‹æ–‡ - ç›´æ¥æ˜¾ç¤ºæ–‡ä»¶åï¼Œå¹¶ä¸ºæ¯ä¸ªæ¥æºç¼–å·
            context_blocks = []
            for i, node in enumerate(final_nodes):
                file_name = node.node.metadata.get('file_name', 'æœªçŸ¥æ–‡ä»¶')
                content = node.node.get_content().strip()
                block = f"### æ¥æº {i + 1} - {file_name}:\n> {content}"
                context_blocks.append(block)

            formatted_context = "\n\n".join(context_blocks)
            has_rag = True
        else:
            formatted_context = None
            has_rag = False

        if has_rag:
            # è·å–å‰ç¼€
            assistant_prefix = get_knowledge_assistant_context_prefix()

            # ç»„åˆ assistant_context
            assistant_context = assistant_prefix + formatted_context

            # æ ¹æ®æ€è€ƒæ¨¡å¼é€‰æ‹©ä¸åŒçš„ system å’Œ user prompt
            if enable_thinking:
                system_prompt = get_knowledge_system_rag_advanced()
                user_template = get_knowledge_user_rag_advanced()
            else:
                system_prompt = get_knowledge_system_rag_simple()
                user_template = get_knowledge_user_rag_simple()

            # user_template æ˜¯åˆ—è¡¨ï¼Œéœ€è¦ join åå† format
            user_prompt_str = "\n".join(user_template) if isinstance(user_template, list) else user_template
            user_prompt = user_prompt_str.format(question=question)

        else:
            # æ²¡æœ‰æ£€ç´¢åˆ°ç›¸å…³å†…å®¹
            assistant_context = None

            if enable_thinking:
                system_prompt = get_knowledge_system_no_rag_think()
                user_template = get_knowledge_user_no_rag_think()
            else:
                system_prompt = get_knowledge_system_no_rag_simple()
                user_template = get_knowledge_user_no_rag_simple()

            # user_template å¯èƒ½æ˜¯åˆ—è¡¨æˆ–å­—ç¬¦ä¸²
            user_prompt_str = "\n".join(user_template) if isinstance(user_template, list) else user_template
            user_prompt = user_prompt_str.format(question=question)

        # system_prompt å¯èƒ½æ˜¯åˆ—è¡¨ï¼Œéœ€è¦è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        if isinstance(system_prompt, list):
            system_prompt = "\n".join(system_prompt)

        # æ„å»º fallback_promptï¼ˆç”¨äºä¸æ”¯æŒ chat æ¨¡å¼çš„æƒ…å†µï¼‰
        fallback_parts = [system_prompt]
        if assistant_context:
            fallback_parts.append(assistant_context)
        fallback_parts.append(user_prompt)

        return {
            "system_prompt": system_prompt,
            "user_prompt": user_prompt,
            "assistant_context": assistant_context,
            "fallback_prompt": "\n\n".join(fallback_parts)
        }

    def _call_llm(self, llm, prompt_parts, enable_thinking=False):
        """
        è°ƒç”¨ LLMï¼Œæ”¯æŒæ€è€ƒå†…å®¹å’Œæ­£æ–‡å†…å®¹çš„åˆ†ç¦»

        Args:
            llm: LLM å®ä¾‹
            prompt_parts: æç¤ºè¯å­—å…¸
            enable_thinking: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼ï¼ˆç”¨äºè§£æè¾“å‡ºï¼‰
        """
        logger.info(f"ä½¿ç”¨å¤–éƒ¨ Prompt:\n{prompt_parts['fallback_prompt'][:200]}...")

        response_stream = self.llm_wrapper.stream(
            llm,
            prompt=prompt_parts['fallback_prompt'],
            system_prompt=prompt_parts['system_prompt'],
            user_prompt=prompt_parts['user_prompt'],
            assistant_context=prompt_parts['assistant_context'],
            use_chat_mode=Settings.USE_CHAT_MODE
        )

        # å¦‚æœå¯ç”¨æ€è€ƒæ¨¡å¼ï¼Œéœ€è¦è§£æå¹¶åˆ†ç¦»æ€è€ƒå†…å®¹å’Œæ­£æ–‡å†…å®¹
        if enable_thinking:
            buffer = ""
            in_thinking_section = False
            thinking_complete = False

            for delta in response_stream:
                token = getattr(delta, 'delta', None) or getattr(delta, 'text', None) or ''
                if not token:
                    continue

                buffer += token

                # æ£€æµ‹æ€è€ƒéƒ¨åˆ†çš„å¼€å§‹å’Œç»“æŸæ ‡è®°
                if not thinking_complete:
                    # æ£€æŸ¥æ˜¯å¦è¿›å…¥æ€è€ƒåŒºåŸŸ
                    if not in_thinking_section:
                        # æ£€æµ‹æ€è€ƒå¼€å§‹çš„å¤šç§æ ‡è®°
                        thinking_markers = [
                            'ã€å’¨è¯¢è§£æã€‘', 'ç¬¬ä¸€éƒ¨åˆ†ï¼šå’¨è¯¢è§£æ', 'ç¬¬ä¸€éƒ¨åˆ†:å’¨è¯¢è§£æ',
                            '<think>', '## æ€è€ƒè¿‡ç¨‹', '## åˆ†æè¿‡ç¨‹',
                            'å…³é”®å®ä½“', 'Key Entities', '1. å…³é”®å®ä½“'
                        ]

                        for marker in thinking_markers:
                            if marker in buffer:
                                in_thinking_section = True
                                logger.info(f"æ£€æµ‹åˆ°æ€è€ƒå¼€å§‹æ ‡è®°: {marker}")
                                break

                    # æ£€æµ‹æ€è€ƒç»“æŸçš„æ ‡è®°
                    if in_thinking_section:
                        end_markers = [
                            'ã€ç»¼åˆè§£ç­”ã€‘', 'ç¬¬äºŒéƒ¨åˆ†ï¼šç»¼åˆè§£ç­”', 'ç¬¬äºŒéƒ¨åˆ†:ç»¼åˆè§£ç­”',
                            '</think>', '## æœ€ç»ˆç­”æ¡ˆ', '## å›ç­”'
                        ]

                        for marker in end_markers:
                            if marker in buffer:
                                thinking_complete = True
                                logger.info(f"æ£€æµ‹åˆ°æ€è€ƒç»“æŸæ ‡è®°: {marker}")
                                # ğŸ”¥ ä¿®å¤ç‚¹ï¼šè¾“å‡ºæ€è€ƒå†…å®¹ï¼ˆä¸åŒ…å«ç»“æŸæ ‡è®°ï¼‰
                                idx = buffer.index(marker)
                                if idx > 0:
                                    yield ('THINK', clean_for_sse_text(buffer[:idx]))

                                # ğŸ”¥ å…³é”®ä¿®å¤ï¼šè·³è¿‡æ ‡è®°æœ¬èº«ï¼Œåªä¿ç•™æ ‡è®°ä¹‹åçš„å†…å®¹
                                # è€Œä¸æ˜¯ä¿ç•™"æ ‡è®°+ä¹‹åçš„å†…å®¹"
                                buffer = buffer[idx + len(marker):]
                                logger.info(f"è·³è¿‡ç»“æŸæ ‡è®° '{marker}'ï¼Œå‰©ä½™bufferé•¿åº¦: {len(buffer)}")
                                break

                    # åœ¨æ€è€ƒåŒºåŸŸä¸”bufferè¶³å¤Ÿé•¿æ—¶ï¼Œæµå¼è¾“å‡º
                    if in_thinking_section and not thinking_complete and len(buffer) > 20:
                        yield ('THINK', clean_for_sse_text(buffer))
                        buffer = ""
                else:
                    # æ€è€ƒå®Œæˆåï¼Œæ‰€æœ‰å†…å®¹éƒ½æ˜¯æ­£æ–‡
                    # ğŸ”¥ ä¿®å¤ç‚¹ï¼šç«‹å³æ£€æŸ¥bufferä¸­æ˜¯å¦è¿˜æœ‰éœ€è¦è¿‡æ»¤çš„å†…å®¹
                    # ç§»é™¤å¯èƒ½æ®‹ç•™çš„æ ‡é¢˜æ ‡è®°ï¼ˆå¦‚"ç¬¬äºŒéƒ¨åˆ†"åé¢çš„å†’å·ã€æ¢è¡Œç­‰ï¼‰
                    if buffer and len(buffer) > 20:
                        # æ¸…ç†å¼€å¤´å¯èƒ½çš„ç©ºç™½å­—ç¬¦å’Œæ ¼å¼æ ‡è®°
                        cleaned_buffer = buffer.lstrip('\n\r :ï¼š')
                        if cleaned_buffer:
                            yield ('CONTENT', clean_for_sse_text(cleaned_buffer))
                        buffer = ""
                    elif buffer:
                        # bufferè¾ƒçŸ­ï¼Œç»§ç»­ç´¯ç§¯
                        pass

            # è¾“å‡ºå‰©ä½™çš„buffer
            if buffer:
                if in_thinking_section and not thinking_complete:
                    # å¦‚æœæ€è€ƒåŒºåŸŸæœªå®Œæˆï¼Œå‰©ä½™å†…å®¹ä½œä¸ºæ€è€ƒè¾“å‡º
                    yield ('THINK', clean_for_sse_text(buffer))
                    logger.info(f"è¾“å‡ºå‰©ä½™æ€è€ƒå†…å®¹: {len(buffer)} å­—ç¬¦")
                else:
                    # å¦åˆ™ä½œä¸ºæ­£æ–‡è¾“å‡ºï¼Œä½†è¦æ¸…ç†å¼€å¤´çš„ç©ºç™½å’Œæ ‡è®°
                    cleaned_buffer = buffer.lstrip('\n\r :ï¼š')
                    if cleaned_buffer:
                        yield ('CONTENT', clean_for_sse_text(cleaned_buffer))
                        logger.info(f"è¾“å‡ºå‰©ä½™æ­£æ–‡å†…å®¹: {len(cleaned_buffer)} å­—ç¬¦")
        else:
            # ä¸å¯ç”¨æ€è€ƒæ¨¡å¼ï¼Œæ‰€æœ‰å†…å®¹éƒ½æ˜¯æ­£æ–‡
            for delta in response_stream:
                token = getattr(delta, 'delta', None) or getattr(delta, 'text', None) or ''
                if token:
                    yield ('CONTENT', clean_for_sse_text(token))

    def _format_sources(self, final_nodes):
        """æ ¼å¼åŒ–å‚è€ƒæ¥æº"""
        for i, node in enumerate(final_nodes):
            initial_score = node.node.metadata.get('initial_score', 0.0)
            source_data = {
                "id": i + 1,
                "fileName": node.node.metadata.get('file_name', 'æœªçŸ¥'),
                "initialScore": f"{initial_score:.4f}",
                "rerankedScore": f"{node.score:.4f}",
                "content": node.node.text.strip()
            }
            yield f"SOURCE:{json.dumps(source_data, ensure_ascii=False)}"

    def _format_filtered_sources(self, filtered_results):
        """æ ¼å¼åŒ– InsertBlock è¿‡æ»¤åçš„å‚è€ƒæ¥æº"""
        for i, result in enumerate(filtered_results):
            source_data = {
                "id": i + 1,
                "fileName": result['file_name'],
                "initialScore": f"{result['initial_score']:.4f}",
                "rerankedScore": f"{result['reranked_score']:.4f}",
                "canAnswer": result['can_answer'],
                "reasoning": result['reasoning'],
                "keyPassage": result.get('key_passage', ''),
                "content": result['node'].node.text.strip()
            }
            yield f"SOURCE:{json.dumps(source_data, ensure_ascii=False)}"

    def _save_log(self, question: str, response: str, client_ip: str, has_rag: bool, use_insert_block: bool = False):
        """ä¿å­˜é—®ç­”æ—¥å¿—"""
        from utils import QALogger
        qa_logger = QALogger(Settings.LOG_DIR)
        qa_logger.save_log(
            question,
            response,
            'knowledge_qa_stream',
            metadata={
                "ip": client_ip,
                "answer_type": "rag" if has_rag else "general",
                "chat_mode": Settings.USE_CHAT_MODE,
                "insert_block_mode": use_insert_block
            }
        )

    def process_conversation(
        self,
        question: str,
        session_id: str,
        enable_thinking: bool,
        rerank_top_n: int,
        llm,
        client_ip: str = "unknown",
        use_insert_block: bool = False,
        insert_block_llm_id: Optional[str] = None
    ) -> Generator[str, None, None]:
        """
        å¤„ç†æ”¯æŒå¤šè½®å¯¹è¯çš„çŸ¥è¯†é—®ç­”

        æµç¨‹ï¼š
        1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        2. InsertBlock è¿‡æ»¤ï¼ˆå¯é€‰ï¼‰
        3. è·å–å†å²å¯¹è¯
        4. ä½¿ç”¨çŸ¥è¯†é—®ç­”çš„æç¤ºè¯æ„å»º promptï¼ˆå°†å†å²å¯¹è¯æ³¨å…¥åˆ°ä¸Šä¸‹æ–‡ä¸­ï¼‰
        5. è°ƒç”¨ LLM
        6. å­˜å‚¨æœ¬è½®å¯¹è¯
        7. è¿”å›å‚è€ƒæ¥æº

        Args:
            question: é—®é¢˜å†…å®¹
            session_id: ä¼šè¯ID
            enable_thinking: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼
            rerank_top_n: é‡æ’åºåè¿”å›çš„æ–‡æ¡£æ•°é‡
            llm: LLM å®ä¾‹
            client_ip: å®¢æˆ·ç«¯ IP
            use_insert_block: æ˜¯å¦ä½¿ç”¨ InsertBlock è¿‡æ»¤æ¨¡å¼
            insert_block_llm_id: InsertBlock ä½¿ç”¨çš„ LLM ID

        Yields:
            SSE æ ¼å¼çš„å“åº”æµ
        """
        full_response = ""

        try:
            logger.info(
                f"å¤„ç†å¤šè½®å¯¹è¯: ä¼šè¯ {session_id[:8]}... | '{question}' | "
                f"æ€è€ƒæ¨¡å¼: {enable_thinking} | InsertBlock: {use_insert_block}"
            )

            # è·å–å¯¹è¯ç®¡ç†å™¨
            from flask import current_app
            knowledge_service = current_app.knowledge_service
            conversation_manager = knowledge_service.conversation_manager

            if not conversation_manager:
                raise ValueError("å¯¹è¯ç®¡ç†å™¨æœªåˆå§‹åŒ–")

            # è¿”å›ä¼šè¯ID
            yield f"SESSION:{session_id}"

            # 1. æ£€ç´¢
            yield "CONTENT:æ­£åœ¨è¿›è¡Œæ··åˆæ£€ç´¢..."
            full_response += "æ­£åœ¨è¿›è¡Œæ··åˆæ£€ç´¢...\n"

            final_nodes = self._retrieve_and_rerank(question, rerank_top_n)

            # 2. å¦‚æœå¯ç”¨ InsertBlock æ¨¡å¼ï¼Œè¿›è¡Œæ™ºèƒ½è¿‡æ»¤
            filtered_results = None
            nodes_for_prompt = final_nodes

            if use_insert_block and final_nodes and self.insert_block_filter:
                yield "CONTENT:æ­£åœ¨ä½¿ç”¨ InsertBlock æ™ºèƒ½è¿‡æ»¤..."
                full_response += "æ­£åœ¨ä½¿ç”¨ InsertBlock æ™ºèƒ½è¿‡æ»¤...\n"

                filtered_results = self.insert_block_filter.filter_nodes(
                    question=question,
                    nodes=final_nodes,
                    llm_id=insert_block_llm_id
                )

                if filtered_results:
                    yield f"CONTENT:æ‰¾åˆ° {len(filtered_results)} ä¸ªå¯å›ç­”çš„èŠ‚ç‚¹"
                    full_response += f"æ‰¾åˆ° {len(filtered_results)} ä¸ªå¯å›ç­”çš„èŠ‚ç‚¹\n"
                    nodes_for_prompt = None
                else:
                    yield "CONTENT:æœªæ‰¾åˆ°å¯ç›´æ¥å›ç­”çš„èŠ‚ç‚¹ï¼Œå°†ä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ"
                    full_response += "æœªæ‰¾åˆ°å¯ç›´æ¥å›ç­”çš„èŠ‚ç‚¹ï¼Œå°†ä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ\n"
                    filtered_results = None

            # 3. è·å–å†å²å¯¹è¯
            from config import Settings as AppSettings
            recent_turns = getattr(AppSettings, 'MAX_RECENT_TURNS', 6)
            relevant_turns = getattr(AppSettings, 'MAX_RELEVANT_TURNS', 3)
            max_summary_turns = getattr(AppSettings, 'MAX_SUMMARY_TURNS', 12)

            # 3.1 è·å–æœ€è¿‘çš„å¯¹è¯å†å²
            recent_history = conversation_manager.get_recent_history(
                session_id=session_id,
                limit=recent_turns
            )

            # 3.2 æ£€ç´¢ä¸å½“å‰é—®é¢˜ç›¸å…³çš„å†å²å¯¹è¯
            relevant_history = []
            if relevant_turns > 0:
                try:
                    relevant_history = conversation_manager.retrieve_relevant_history(
                        session_id=session_id,
                        current_query=question,
                        top_k=relevant_turns
                    )
                    # è¿‡æ»¤æ‰å·²ç»åœ¨æœ€è¿‘å¯¹è¯ä¸­çš„è½®æ¬¡ï¼ˆé¿å…é‡å¤ï¼‰
                    recent_turn_ids = {turn.get('turn_id') for turn in recent_history if turn.get('turn_id')}
                    relevant_history = [
                        turn for turn in relevant_history
                        if turn.get('turn_id') not in recent_turn_ids
                    ]
                    logger.info(f"æ£€ç´¢åˆ° {len(relevant_history)} æ¡ç›¸å…³å†å²å¯¹è¯ï¼ˆæ’é™¤æœ€è¿‘å¯¹è¯åï¼‰")
                except Exception as e:
                    logger.warning(f"æ£€ç´¢ç›¸å…³å†å²å¯¹è¯å¤±è´¥: {e}")
                    relevant_history = []

            # 4. æ„å»ºå†å²å¯¹è¯æ‘˜è¦ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
            # è·å–ä¼šè¯æ€»è½®æ•°
            try:
                all_history = conversation_manager.get_recent_history(
                    session_id=session_id,
                    limit=100  # å‡è®¾æœ€å¤š100è½®
                )
                total_turns = len(all_history)
            except Exception as e:
                logger.warning(f"è·å–æ€»å¯¹è¯è½®æ•°å¤±è´¥: {e}")
                total_turns = len(recent_history)
                all_history = recent_history

            history_summary = None

            # åªæœ‰å½“æ€»è½®æ•°è¶…è¿‡ MAX_SUMMARY_TURNS æ—¶æ‰ç”Ÿæˆæ‘˜è¦ï¼ˆé¿å…é¢‘ç¹æ‘˜è¦ï¼‰
            if total_turns > max_summary_turns:
                # æ’é™¤æœ€è¿‘Nè½®ï¼Œå‰©ä½™çš„ç”¨äºç”Ÿæˆæ‘˜è¦
                old_history = all_history[:-recent_turns] if len(all_history) > recent_turns else []

                if old_history and len(old_history) >= 3:  # è‡³å°‘3è½®æ‰å€¼å¾—æ‘˜è¦
                    # æ£€æŸ¥æ‘˜è¦ç¼“å­˜
                    cache_key = f"{session_id}_summary"
                    current_time = time.time()

                    if hasattr(conversation_manager, '_summary_cache'):
                        cache_entry = conversation_manager._summary_cache.get(cache_key)
                        if cache_entry:
                            cache_age = current_time - cache_entry.get('timestamp', 0)
                            summarized_count = cache_entry.get('summarized_until', 0)

                            # å¦‚æœç¼“å­˜æœ‰æ•ˆä¸”å¯¹è¯æ•°é‡æ²¡å˜åŒ–å¤ªå¤šï¼ˆå…è®¸Â±2è½®å·®å¼‚ï¼‰ï¼Œä½¿ç”¨ç¼“å­˜
                            if (cache_age < AppSettings.SUMMARY_CACHE_TTL and
                                abs(len(old_history) - summarized_count) <= 2):
                                history_summary = cache_entry.get('summary')
                                logger.info(f"ä½¿ç”¨ç¼“å­˜çš„å†å²æ‘˜è¦ (ç¼“å­˜æ—¶é•¿: {cache_age:.0f}s)")

                    # å¦‚æœæ²¡æœ‰ç¼“å­˜æˆ–ç¼“å­˜å¤±æ•ˆï¼Œç”Ÿæˆæ–°æ‘˜è¦
                    if not history_summary:
                        try:
                            history_summary = conversation_manager.summarize_old_conversations(
                                session_id=session_id,
                                conversations=old_history
                            )

                            # æ›´æ–°ç¼“å­˜
                            if history_summary and hasattr(conversation_manager, '_summary_cache'):
                                conversation_manager._summary_cache[cache_key] = {
                                    'summary': history_summary,
                                    'summarized_until': len(old_history),
                                    'timestamp': current_time
                                }
                                logger.info(f"å·²ç”Ÿæˆå¹¶ç¼“å­˜å†å²æ‘˜è¦ (è¦†ç›– {len(old_history)} è½®)")
                        except Exception as e:
                            logger.warning(f"ç”Ÿæˆå†å²æ‘˜è¦å¤±è´¥: {e}")
                            history_summary = None
                else:
                    logger.debug(f"æ—§å¯¹è¯è½®æ•°({len(old_history)})ä¸è¶³ï¼Œè·³è¿‡æ‘˜è¦ç”Ÿæˆ")
            else:
                logger.debug(f"æ€»è½®æ•°({total_turns})æœªè¾¾æ‘˜è¦é˜ˆå€¼({max_summary_turns})ï¼Œè·³è¿‡æ‘˜è¦")

            # 5. ä½¿ç”¨ä¼˜åŒ–çš„æç¤ºè¯æ„å»ºæ–¹å¼ï¼ˆæ³¨å…¥å†å²å¯¹è¯ï¼‰
            prompt_parts = self._build_prompt_with_history(
                question,
                enable_thinking,
                nodes_for_prompt,
                filtered_results=filtered_results,
                recent_history=recent_history,
                relevant_history=relevant_history,
                history_summary=history_summary
            )

            # 6. è¾“å‡ºçŠ¶æ€
            status_msg = (
                "å·²æ‰¾åˆ°ç›¸å…³èµ„æ–™ï¼Œæ­£åœ¨ç”Ÿæˆå›ç­”..."
                if final_nodes
                else "æœªæ‰¾åˆ°é«˜ç›¸å…³æ€§èµ„æ–™ï¼ŒåŸºäºé€šç”¨çŸ¥è¯†å’Œå¯¹è¯å†å²å›ç­”..."
            )
            yield f"CONTENT:{status_msg}"
            full_response += status_msg + "\n"

            # 7. è°ƒç”¨ LLM
            assistant_response = ""
            for result in self._call_llm(llm, prompt_parts, enable_thinking=enable_thinking):
                # result æ˜¯å…ƒç»„ (prefix_type, content)
                prefix_type, chunk = result
                if prefix_type == 'THINK':
                    yield f"THINK:{chunk}"
                    # æ€è€ƒå†…å®¹ä¸è®¡å…¥ assistant_response
                elif prefix_type == 'CONTENT':
                    yield f"CONTENT:{chunk}"
                    full_response += chunk
                    assistant_response += chunk

            # 8. å­˜å‚¨æœ¬è½®å¯¹è¯åˆ°å‘é‡åº“
            context_doc_names = []
            if final_nodes:
                context_doc_names = [
                    node.node.metadata.get('file_name', 'æœªçŸ¥')
                    for node in final_nodes
                ]

            # è·å–ä¸Šä¸€è½®å¯¹è¯çš„ turn_id ä½œä¸º parent_turn_id
            parent_turn_id = None
            try:
                if recent_history:
                    parent_turn_id = recent_history[-1].get('turn_id')
            except Exception as e:
                logger.warning(f"è·å–çˆ¶å¯¹è¯IDå¤±è´¥: {e}")

            # ç”Ÿæˆå½“å‰è½®æ¬¡çš„ turn_id
            import uuid
            current_turn_id = str(uuid.uuid4())

            # å­˜å‚¨å¯¹è¯ï¼ˆåŒ…å«å®Œæ•´çš„åŠ©æ‰‹å›ç­”ï¼Œå…¶ä¸­å·²ç»åŒ…å«äº†å®ä½“å’ŒåŠ¨ä½œåˆ†æï¼‰
            conversation_manager.add_conversation_turn(
                session_id=session_id,
                user_query=question,
                assistant_response=assistant_response,
                context_docs=context_doc_names,
                turn_id=current_turn_id,
                parent_turn_id=parent_turn_id
            )

            # 9. è¾“å‡ºå‚è€ƒæ¥æº
            if use_insert_block and filtered_results:
                yield "CONTENT:\n\n**å‚è€ƒæ¥æºï¼ˆå…¨éƒ¨æ£€ç´¢ç»“æœï¼‰:**"
                full_response += "\n\nå‚è€ƒæ¥æºï¼ˆå…¨éƒ¨æ£€ç´¢ç»“æœï¼‰:"

                filtered_map = {}
                for result in filtered_results:
                    key = f"{result['file_name']}_{result['reranked_score']}"
                    filtered_map[key] = result

                for i, node in enumerate(final_nodes):
                    file_name = node.node.metadata.get('file_name', 'æœªçŸ¥')
                    initial_score = node.node.metadata.get('initial_score', 0.0)
                    key = f"{file_name}_{node.score}"

                    filtered_info = filtered_map.get(key)

                    source_data = {
                        "id": i + 1,
                        "fileName": file_name,
                        "initialScore": f"{initial_score:.4f}",
                        "rerankedScore": f"{node.score:.4f}",
                        "content": node.node.text.strip(),
                        "canAnswer": filtered_info is not None,
                        "reasoning": filtered_info.get('reasoning', '') if filtered_info else '',
                        "keyPassage": filtered_info.get('key_passage', '') if filtered_info else ''
                    }

                    yield f"SOURCE:{json.dumps(source_data, ensure_ascii=False)}"

                    full_response += (
                        f"\n[{source_data['id']}] æ–‡ä»¶: {source_data['fileName']}, "
                        f"é‡æ’åˆ†: {source_data['rerankedScore']}, "
                        f"å¯å›ç­”: {source_data['canAnswer']}"
                    )

            elif final_nodes:
                yield "CONTENT:\n\n**å‚è€ƒæ¥æº:**"
                full_response += "\n\nå‚è€ƒæ¥æº:"

                for source_msg in self._format_sources(final_nodes):
                    yield source_msg
                    if source_msg.startswith("SOURCE:"):
                        data = json.loads(source_msg[7:])
                        full_response += (
                            f"\n[{data['id']}] æ–‡ä»¶: {data['fileName']}, "
                            f"é‡æ’åˆ†: {data['rerankedScore']}"
                        )

            yield "DONE:"

            # 10. ä¿å­˜æ—¥å¿—
            self._save_log(
                question,
                full_response,
                client_ip,
                bool(final_nodes),
                use_insert_block=use_insert_block
            )

        except Exception as e:
            error_msg = f"å¤„ç†é”™è¯¯: {str(e)}"
            logger.error(f"å¤šè½®å¯¹è¯å¤„ç†å‡ºé”™: {e}", exc_info=True)
            yield f"ERROR:{error_msg}"

    def _build_prompt_with_history(
        self,
        question: str,
        enable_thinking: bool,
        final_nodes,
        filtered_results=None,
        recent_history=None,
        relevant_history=None,
        history_summary=None
    ):
        """
        æ„é€ å¸¦å†å²å¯¹è¯çš„æç¤ºè¯ï¼ˆä½¿ç”¨çŸ¥è¯†é—®ç­”çš„æç¤ºè¯æ ¼å¼ï¼‰

        Args:
            question: å½“å‰é—®é¢˜
            enable_thinking: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼
            final_nodes: æ£€ç´¢åˆ°çš„èŠ‚ç‚¹
            filtered_results: InsertBlock è¿‡æ»¤ç»“æœ
            recent_history: æœ€è¿‘çš„å¯¹è¯å†å²
            relevant_history: ç›¸å…³çš„å†å²å¯¹è¯
            history_summary: å†å²å¯¹è¯æ‘˜è¦
        """
        # æ„å»ºçŸ¥è¯†åº“ä¸Šä¸‹æ–‡ï¼ˆä¸çŸ¥è¯†é—®ç­”ç›¸åŒçš„é€»è¾‘ï¼‰
        knowledge_context = None
        if filtered_results:
            # ä½¿ç”¨ InsertBlock è¿‡æ»¤ç»“æœ
            context_blocks = []
            for i, result in enumerate(filtered_results):
                file_name = result['file_name']
                full_content = result['node'].node.text.strip()
                block = f"### æ¥æº {i + 1} - {file_name}:\n> {full_content}"
                context_blocks.append(block)
            knowledge_context = "\n\n".join(context_blocks) if context_blocks else None

        elif final_nodes:
            # ä½¿ç”¨æ™®é€šæ£€ç´¢ç»“æœ
            context_blocks = []
            for i, node in enumerate(final_nodes):
                file_name = node.node.metadata.get('file_name', 'æœªçŸ¥æ–‡ä»¶')
                content = node.node.get_content().strip()
                block = f"### æ¥æº {i + 1} - {file_name}:\n> {content}"
                context_blocks.append(block)
            knowledge_context = "\n\n".join(context_blocks)

        has_rag = bool(knowledge_context)

        # æ„å»ºå†å²å¯¹è¯ä¸Šä¸‹æ–‡
        history_context = None
        if history_summary or recent_history or relevant_history:
            history_parts = []

            # æ·»åŠ æ‘˜è¦
            if history_summary:
                summary_prefix = get_conversation_summary_context_prefix()
                history_parts.append(f"{summary_prefix}{history_summary}")

            # æ·»åŠ æœ€è¿‘çš„å¯¹è¯
            if recent_history:
                recent_prefix = get_conversation_context_prefix_recent_history()
                recent_turns_text = "\n\n".join([
                    f"ç”¨æˆ·: {turn['user_query']}\nåŠ©æ‰‹: {turn['assistant_response']}"
                    for turn in recent_history
                ])
                history_parts.append(f"{recent_prefix}{recent_turns_text}")

            # æ·»åŠ ç›¸å…³å†å²å¯¹è¯
            if relevant_history:
                relevant_prefix = get_conversation_context_prefix_relevant_history()
                relevant_turns_text = "\n\n".join([
                    f"ç”¨æˆ·: {turn['user_query']}\nåŠ©æ‰‹: {turn['assistant_response']}"
                    for turn in relevant_history
                ])
                history_parts.append(f"{relevant_prefix}{relevant_turns_text}")

            history_context = "\n\n".join(history_parts)

        # ä½¿ç”¨çŸ¥è¯†é—®ç­”çš„æç¤ºè¯é€»è¾‘
        if has_rag:
            # è·å–å‰ç¼€
            assistant_prefix = get_knowledge_assistant_context_prefix()

            # ç»„åˆä¸Šä¸‹æ–‡ï¼šå†å²å¯¹è¯ + ä¸šåŠ¡è§„å®š
            context_parts = []
            if history_context:
                context_parts.append(history_context)
            context_parts.append(assistant_prefix + knowledge_context)

            assistant_context = "\n\n---\n\n".join(context_parts)

            # æ ¹æ®æ€è€ƒæ¨¡å¼é€‰æ‹©ä¸åŒçš„ system å’Œ user prompt
            if enable_thinking:
                system_prompt = get_knowledge_system_rag_advanced()
                user_template = get_knowledge_user_rag_advanced()
            else:
                system_prompt = get_knowledge_system_rag_simple()
                user_template = get_knowledge_user_rag_simple()

            # user_template æ˜¯åˆ—è¡¨ï¼Œéœ€è¦ join åå† format
            user_prompt_str = "\n".join(user_template) if isinstance(user_template, list) else user_template
            user_prompt = user_prompt_str.format(question=question)

        else:
            # æ²¡æœ‰æ£€ç´¢åˆ°ç›¸å…³å†…å®¹ï¼Œåªæœ‰å†å²å¯¹è¯
            assistant_context = history_context

            if enable_thinking:
                system_prompt = get_knowledge_system_no_rag_think()
                user_template = get_knowledge_user_no_rag_think()
            else:
                system_prompt = get_knowledge_system_no_rag_simple()
                user_template = get_knowledge_user_no_rag_simple()

            # user_template å¯èƒ½æ˜¯åˆ—è¡¨æˆ–å­—ç¬¦ä¸²
            user_prompt_str = "\n".join(user_template) if isinstance(user_template, list) else user_template
            user_prompt = user_prompt_str.format(question=question)

        # system_prompt å¯èƒ½æ˜¯åˆ—è¡¨ï¼Œéœ€è¦è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        if isinstance(system_prompt, list):
            system_prompt = "\n".join(system_prompt)

        # æ„å»º fallback_promptï¼ˆç”¨äºä¸æ”¯æŒ chat æ¨¡å¼çš„æƒ…å†µï¼‰
        fallback_parts = [system_prompt]
        if assistant_context:
            fallback_parts.append(assistant_context)
        fallback_parts.append(user_prompt)

        return {
            "system_prompt": system_prompt,
            "user_prompt": user_prompt,
            "assistant_context": assistant_context,
            "fallback_prompt": "\n\n".join(fallback_parts)
        }
