# -*- coding: utf-8 -*-
"""
çŸ¥è¯†é—®ç­”å¤„ç†å™¨
å¤„ç†çŸ¥è¯†åº“é—®ç­”çš„ä¸šåŠ¡é€»è¾‘
"""
import json
import os
from datetime import datetime
from typing import Generator, Dict, Any, Optional, List
from llama_index.core import QueryBundle
from config import Settings
from utils import logger, clean_for_sse_text
from pathlib import Path
from prompts import (
    get_knowledge_assistant_context_prefix,
    get_knowledge_system_rag_simple,
    get_knowledge_system_rag_advanced,
    get_knowledge_system_no_rag_think,
    get_knowledge_system_no_rag_simple,
    get_knowledge_user_rag_simple,
    get_knowledge_user_rag_advanced,
    get_knowledge_user_no_rag_think,
    get_knowledge_user_no_rag_simple,
    get_conversation_system_rag_with_history,
    get_conversation_system_general_with_history,
    get_conversation_context_prefix_relevant_history,
    get_conversation_context_prefix_recent_history,
    get_conversation_context_prefix_regulations,
    get_conversation_user_rag_query,
    get_conversation_user_general_query,
    get_conversation_summary_system,
    get_conversation_summary_user,
    get_conversation_summary_context_prefix
)
# å¯¼å…¥æ–°çš„å·¥å…·å‡½æ•°
from utils.knowledge_utils import (
    build_knowledge_prompt,
    format_sources,
    format_filtered_sources,
    build_reference_entries,
    log_prompt_to_file,
    log_reference_details,
    save_qa_log,
    parse_thinking_stream,
    parse_normal_stream
)


class KnowledgeHandler:
    """çŸ¥è¯†é—®ç­”å¤„ç†å™¨"""

    def __init__(
        self, 
        retriever, 
        reranker, 
        llm_wrapper, 
        llm_service=None,
        # å…ç­¾çŸ¥è¯†åº“ç›¸å…³ç»„ä»¶ï¼ˆå¯é€‰ï¼‰
        visa_free_retriever=None,
        # èˆªå¸çŸ¥è¯†åº“ç›¸å…³ç»„ä»¶ï¼ˆå¯é€‰ï¼‰
        airline_retriever=None,
        # å¤šåº“æ£€ç´¢å™¨å’Œæ„å›¾åˆ†ç±»å™¨
        multi_kb_retriever=None,
        intent_classifier=None,
        # å­é—®é¢˜åˆ†è§£å™¨ï¼ˆå¯é€‰ï¼‰
        sub_question_decomposer=None,
        # éšè—çŸ¥è¯†åº“æ£€ç´¢å™¨ï¼ˆå¯é€‰ï¼‰
        hidden_kb_retriever=None
    ):
        # é€šç”¨çŸ¥è¯†åº“ç»„ä»¶
        self.retriever = retriever
        self.reranker = reranker
        self.llm_wrapper = llm_wrapper
        self.llm_service = llm_service
        self.insert_block_filter = None
        
        # å…ç­¾çŸ¥è¯†åº“ç»„ä»¶
        self.visa_free_retriever = visa_free_retriever
        # èˆªå¸çŸ¥è¯†åº“ç»„ä»¶
        self.airline_retriever = airline_retriever
        # å¤šåº“æ£€ç´¢å™¨å’Œæ„å›¾åˆ†ç±»å™¨
        self.multi_kb_retriever = multi_kb_retriever
        self.intent_classifier = intent_classifier
        # å­é—®é¢˜åˆ†è§£å™¨
        self.sub_question_decomposer = sub_question_decomposer
        # éšè—çŸ¥è¯†åº“æ£€ç´¢å™¨
        self.hidden_kb_retriever = hidden_kb_retriever
        
        # å­é—®é¢˜ç­”æ¡ˆåˆæˆï¼ˆç”¨äºä¼ é€’åˆ°æç¤ºè¯ï¼‰
        self._last_synthesized_answer = None

        # å¦‚æœæä¾›äº† llm_serviceï¼Œåˆå§‹åŒ– InsertBlock è¿‡æ»¤å™¨
        if llm_service:
            from core.node_filter import InsertBlockFilter
            self.insert_block_filter = InsertBlockFilter(llm_service)
            logger.info("InsertBlock è¿‡æ»¤å™¨å·²åˆå§‹åŒ–")
        
        # æ—¥å¿—ï¼šçŸ¥è¯†åº“åŠŸèƒ½çŠ¶æ€
        enabled_features = []
        if self.multi_kb_retriever and self.intent_classifier:
            enabled_features.append("å¤šåº“æ£€ç´¢+æ„å›¾åˆ†ç±»")
        if self.visa_free_retriever:
            enabled_features.append("å…ç­¾åº“")
        if self.airline_retriever:
            enabled_features.append("èˆªå¸åº“")
        if self.sub_question_decomposer:
            enabled_features.append("å­é—®é¢˜åˆ†è§£")
        if self.hidden_kb_retriever:
            enabled_features.append("éšè—çŸ¥è¯†åº“")
        
        if enabled_features:
            logger.info(f"âœ“ çŸ¥è¯†åº“åŠŸèƒ½å·²å¯ç”¨: {', '.join(enabled_features)}")
        else:
            logger.info("âŠ˜ ä»…ä½¿ç”¨é€šç”¨çŸ¥è¯†åº“")

    def process(
        self,
        question: str,
        enable_thinking: bool,
        rerank_top_n: int,
        llm,
        client_ip: str = "unknown",
        use_insert_block: bool = False,
        insert_block_llm_id: Optional[str] = None
    ) -> Generator[str, None, None]:
        """
        å¤„ç†çŸ¥è¯†é—®ç­”

        Args:
            question: é—®é¢˜å†…å®¹
            enable_thinking: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼
            rerank_top_n: é‡æ’åºåè¿”å›çš„æ–‡æ¡£æ•°é‡
            llm: LLM å®ä¾‹
            client_ip: å®¢æˆ·ç«¯ IP
            use_insert_block: æ˜¯å¦ä½¿ç”¨ InsertBlock è¿‡æ»¤æ¨¡å¼
            insert_block_llm_id: InsertBlock ä½¿ç”¨çš„ LLM ID

        Yields:
            SSE æ ¼å¼çš„å“åº”æµ
        """
        full_response = ""
        
        # æ¸…ç©ºä¸Šä¸€æ¬¡çš„å­é—®é¢˜ç­”æ¡ˆå’Œåˆæˆç­”æ¡ˆï¼Œé˜²æ­¢ä¸²é¢˜
        self._last_sub_answers = None
        self._last_synthesized_answer = None

        try:
            logger.info(
                f"å¤„ç†çŸ¥è¯†é—®ç­”: '{question}' | "
                f"æ€è€ƒæ¨¡å¼: {enable_thinking} | "
                f"å‚è€ƒæ–‡ä»¶æ•°: {rerank_top_n} | "
                f"InsertBlock: {use_insert_block}"
            )

            # 1. æ™ºèƒ½è·¯ç”±æ£€ç´¢ï¼ˆæ ¹æ®æ„å›¾é€‰æ‹©çŸ¥è¯†åº“ï¼‰
            # å¦‚æœå‰ç«¯è®¾ç½®å‚è€ƒæ•°é‡ä¸º 0ï¼Œè·³è¿‡æ£€ç´¢
            if rerank_top_n == 0:
                logger.info("[æ£€ç´¢è·³è¿‡] å‰ç«¯è®¾ç½®å‚è€ƒæ•°é‡ä¸º 0ï¼Œè·³è¿‡æ£€ç´¢å’Œå­é—®é¢˜åˆ†è§£")
                final_nodes = []
                result = None
                hidden_nodes = []
            else:
                yield ('CONTENT', "æ­£åœ¨è¿›è¡Œæ··åˆæ£€ç´¢...\n")
                full_response += "æ­£åœ¨è¿›è¡Œæ··åˆæ£€ç´¢...\n"
                
                # è°ƒç”¨æ£€ç´¢ï¼Œè·å–èŠ‚ç‚¹å’Œå…ƒæ•°æ®
                result = self._smart_retrieve_and_rerank(question, rerank_top_n)
                
                # 1.5 éšè—çŸ¥è¯†åº“æ£€ç´¢ï¼ˆå¹¶è¡Œè¿›è¡Œï¼Œä¸å½±å“ä¸»æµç¨‹ï¼‰
                hidden_nodes = []
                if self.hidden_kb_retriever and self.hidden_kb_retriever.enabled:
                    try:
                        logger.info("[éšè—çŸ¥è¯†åº“] å¼€å§‹å¹¶è¡Œæ£€ç´¢...")
                        hidden_nodes = self.hidden_kb_retriever.retrieve(question)
                        if hidden_nodes:
                            logger.info(f"[éšè—çŸ¥è¯†åº“] æ£€ç´¢æˆåŠŸ | è¿”å› {len(hidden_nodes)} æ¡")
                        else:
                            logger.info("[éšè—çŸ¥è¯†åº“] æœªæ£€ç´¢åˆ°ç›¸å…³å†…å®¹")
                    except Exception as e:
                        logger.warning(f"[éšè—çŸ¥è¯†åº“] æ£€ç´¢å¤±è´¥ï¼Œç»§ç»­ä¸»æµç¨‹: {e}")
                        hidden_nodes = []
            
            # æ£€æŸ¥æ˜¯å¦è¿”å›äº†å…ƒæ•°æ®ï¼ˆå­é—®é¢˜åˆ†è§£ï¼‰
            if result and isinstance(result, tuple) and len(result) == 2:
                final_nodes, retrieval_metadata = result
                
                # å¦‚æœæœ‰å­é—®é¢˜ï¼Œè¾“å‡ºåˆ°å‰ç«¯
                if retrieval_metadata.get('decomposed') and retrieval_metadata.get('sub_questions'):
                    sub_questions = retrieval_metadata['sub_questions']
                    sub_answers = retrieval_metadata.get('sub_answers', [])
                    
                    # æ„å»ºå®Œæ•´çš„å­é—®é¢˜æ•°æ®
                    sub_questions_data = {
                        'sub_questions': sub_questions,
                        'count': len(sub_questions),
                        'sub_answers': sub_answers  # åŒ…å«æ¯ä¸ªå­é—®é¢˜çš„ç­”æ¡ˆæ‘˜è¦
                    }
                    
                    yield ('SUB_QUESTIONS', sub_questions_data)
                    logger.info(
                        f"[å‰ç«¯è¾“å‡º] å·²å‘é€å­é—®é¢˜åˆ°å‰ç«¯ | "
                        f"å­é—®é¢˜æ•°: {len(sub_questions)} | "
                        f"ç­”æ¡ˆæ•°: {len(sub_answers)}"
                    )
            else:
                # å…¼å®¹æ—§ç‰ˆæœ¬ï¼ˆåªè¿”å›èŠ‚ç‚¹ï¼‰
                final_nodes = result
                retrieval_metadata = None


            # 2. å¦‚æœå¯ç”¨ InsertBlock æ¨¡å¼ï¼Œè¿›è¡Œæ™ºèƒ½è¿‡æ»¤
            filtered_results = None
            filtered_map = None
            nodes_for_prompt = final_nodes  # é»˜è®¤ä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ

            if use_insert_block and final_nodes and self.insert_block_filter:
                # å‘é€å¼€å§‹æ¶ˆæ¯
                start_msg = f"æ­£åœ¨ä½¿ç”¨ç²¾å‡†æ£€ç´¢åˆ†æ {len(final_nodes)} ä¸ªæ–‡æ¡£...\næç¤ºï¼šç³»ç»Ÿæ­£åœ¨é€ä¸ªåˆ¤æ–­æ¯ä¸ªæ–‡æ¡£æ˜¯å¦èƒ½å›ç­”æ‚¨çš„é—®é¢˜ï¼Œè¯·ç¨å€™\n"
                yield ('CONTENT', start_msg)
                full_response += start_msg
                
                # ä½¿ç”¨é˜Ÿåˆ—æ”¶é›†è¿›åº¦
                import queue
                import threading
                progress_queue = queue.Queue()
                filter_done = threading.Event()
                
                # å®šä¹‰è¿›åº¦å›è°ƒå‡½æ•°ï¼ˆå°†è¿›åº¦æ”¾å…¥é˜Ÿåˆ—ï¼‰
                def progress_callback(processed, total):
                    logger.info(f"[ç²¾å‡†æ£€ç´¢è¿›åº¦] {processed}/{total} ä¸ªæ–‡æ¡£å·²åˆ†æ")
                    progress_queue.put((processed, total))
                
                # åœ¨åå°çº¿ç¨‹æ‰§è¡Œè¿‡æ»¤
                def run_filter():
                    try:
                        result = self.insert_block_filter.filter_nodes(
                            question=question,
                            nodes=final_nodes,
                            llm_id=insert_block_llm_id,
                            progress_callback=progress_callback
                        )
                        progress_queue.put(('DONE', result))
                    except Exception as e:
                        progress_queue.put(('ERROR', e))
                    finally:
                        filter_done.set()
                
                filter_thread = threading.Thread(target=run_filter, daemon=True)
                filter_thread.start()
                
                # ä¸»çº¿ç¨‹å®šæœŸæ£€æŸ¥è¿›åº¦å¹¶å‘é€
                last_progress = 0
                filtered_results = None
                
                while not filter_done.is_set():
                    try:
                        # ç­‰å¾…0.5ç§’æˆ–ç›´åˆ°æœ‰æ–°è¿›åº¦
                        item = progress_queue.get(timeout=0.5)
                        
                        if isinstance(item, tuple):
                            if item[0] == 'DONE':
                                filtered_results = item[1]
                                break
                            elif item[0] == 'ERROR':
                                logger.error(f"ç²¾å‡†æ£€ç´¢è¿‡æ»¤å¤±è´¥: {item[1]}")
                                break
                            else:
                                # è¿›åº¦æ›´æ–°
                                processed, total = item
                                # æ¯å¤„ç†5ä¸ªæ–‡æ¡£å‘é€ä¸€æ¬¡è¿›åº¦ï¼ˆé¿å…åˆ·å±ï¼‰
                                if processed - last_progress >= 5 or processed == total:
                                    progress_msg = f"ğŸ“Š è¿›åº¦: {processed}/{total} ({int(processed/total*100)}%)\n"
                                    yield ('CONTENT', progress_msg)
                                    full_response += progress_msg
                                    last_progress = processed
                    except queue.Empty:
                        # è¶…æ—¶ï¼Œç»§ç»­ç­‰å¾…
                        continue
                
                # ç­‰å¾…çº¿ç¨‹ç»“æŸ
                filter_thread.join(timeout=1)

                if filtered_results:
                    yield ('CONTENT', f"æ‰¾åˆ° {len(filtered_results)} ä¸ªå¯å›ç­”çš„èŠ‚ç‚¹")
                    full_response += f"æ‰¾åˆ° {len(filtered_results)} ä¸ªå¯å›ç­”çš„èŠ‚ç‚¹\n"
                    # InsertBlock æˆåŠŸï¼šåªä½¿ç”¨è¿‡æ»¤åçš„èŠ‚ç‚¹
                    nodes_for_prompt = None  # ä¸å†ä¼ å…¥åŸå§‹èŠ‚ç‚¹
                    filtered_map = {}
                    for result in filtered_results:
                        key = f"{result['file_name']}_{result['reranked_score']}"
                        filtered_map[key] = result
                else:
                    yield ('CONTENT', "æœªæ‰¾åˆ°å¯ç›´æ¥å›ç­”çš„èŠ‚ç‚¹ï¼Œå°†ä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ")
                    full_response += "æœªæ‰¾åˆ°å¯ç›´æ¥å›ç­”çš„èŠ‚ç‚¹ï¼Œå°†ä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ\n"
                    # InsertBlock å¤±è´¥ï¼šç»§ç»­ä½¿ç”¨åŸå§‹èŠ‚ç‚¹ï¼Œæ¸…ç©ºè¿‡æ»¤ç»“æœ
                    filtered_results = None

            # 3. æ„é€ æç¤ºè¯ï¼ˆä½¿ç”¨æ–°å·¥å…·å‡½æ•°ï¼Œæ³¨å…¥éšè—çŸ¥è¯†åº“å†…å®¹ï¼‰
            prompt_parts = build_knowledge_prompt(
                question=question,
                enable_thinking=enable_thinking,
                final_nodes=nodes_for_prompt,  # æ ¹æ® InsertBlock ç»“æœå†³å®šä¼ å…¥å“ªäº›èŠ‚ç‚¹
                filtered_results=filtered_results,
                sub_answers=getattr(self, '_last_sub_answers', None),
                synthesized_answer=getattr(self, '_last_synthesized_answer', None),
                hidden_nodes=hidden_nodes  # éšè—çŸ¥è¯†åº“èŠ‚ç‚¹ï¼ˆä¸æ˜¾ç¤ºæ¥æºï¼‰
            )

            # 4. è¾“å‡ºçŠ¶æ€
            status_msg = (
                "å·²æ‰¾åˆ°ç›¸å…³èµ„æ–™ï¼Œæ­£åœ¨ç”Ÿæˆå›ç­”..."
                if final_nodes
                else "æœªæ‰¾åˆ°é«˜ç›¸å…³æ€§èµ„æ–™ï¼ŒåŸºäºé€šç”¨çŸ¥è¯†å›ç­”..."
            )
            yield ('CONTENT', status_msg)
            full_response += status_msg + "\n"

            # 5. è°ƒç”¨ LLM
            for result in self._call_llm(llm, prompt_parts, enable_thinking=enable_thinking):
                # result æ˜¯å…ƒç»„ (prefix_type, content)
                prefix_type, chunk = result
                if prefix_type == 'THINK':
                    yield ('THINK', chunk)
                    # æ€è€ƒå†…å®¹ä¸è®¡å…¥ full_response
                elif prefix_type == 'CONTENT':
                    yield ('CONTENT', chunk)
                    full_response += chunk

            # 6. æ”¶é›†å¹¶è¾“å‡ºå…¨å±€å…³é”®å­—ï¼ˆå»é‡åé™åˆ¶æ•°é‡ï¼‰
            # 6.1 æå–é—®é¢˜ä¸­çš„å…³é”®è¯
            import jieba
            question_keywords = list(jieba.lcut(question))
            # è¿‡æ»¤æ‰å•å­—å’Œåœç”¨è¯
            question_keywords = [kw for kw in question_keywords if len(kw) > 1]
            logger.info(f"[é—®é¢˜å…³é”®è¯] ä»é—®é¢˜ä¸­æå–: {question_keywords}")
            
            # 6.2 æ”¶é›†æ–‡æ¡£åŒ¹é…çš„å…³é”®å­—
            global_keywords = []
            if final_nodes:
                logger.info(f"[å…³é”®è¯æ”¶é›†] å¼€å§‹æ”¶é›†ï¼Œå…±æœ‰ {len(final_nodes)} ä¸ªèŠ‚ç‚¹")
                for i, node in enumerate(final_nodes):
                    retrieval_sources = node.node.metadata.get('retrieval_sources', [])
                    logger.info(f"[å…³é”®è¯æ”¶é›†] èŠ‚ç‚¹ {i+1}: retrieval_sources={retrieval_sources}")
                    if 'keyword' in retrieval_sources:
                        matched_keywords = node.node.metadata.get('bm25_matched_keywords', [])
                        logger.info(f"[å…³é”®è¯æ”¶é›†] èŠ‚ç‚¹ {i+1} æœ‰å…³é”®å­—: {matched_keywords}")
                        global_keywords.extend(matched_keywords)
                    else:
                        logger.info(f"[å…³é”®è¯æ”¶é›†] èŠ‚ç‚¹ {i+1} æ²¡æœ‰ 'keyword' æ ‡è®°ï¼Œè·³è¿‡")
                logger.info(f"[å…³é”®è¯æ”¶é›†] æ”¶é›†å®Œæˆï¼Œå…±æ”¶é›†åˆ° {len(global_keywords)} ä¸ªå…³é”®å­—: {global_keywords}")
            
            # 6.3 å»é‡é—®é¢˜å…³é”®è¯å’Œæ–‡æ¡£å…³é”®è¯
            # é—®é¢˜å…³é”®è¯å»é‡
            seen_question = set()
            unique_question_keywords = []
            for kw in question_keywords:
                if kw not in seen_question:
                    seen_question.add(kw)
                    unique_question_keywords.append(kw)
            
            # æ–‡æ¡£å…³é”®è¯å»é‡ï¼ˆæ’é™¤å·²åœ¨é—®é¢˜ä¸­çš„ï¼‰
            seen_doc = set(unique_question_keywords)
            unique_doc_keywords = []
            for kw in global_keywords:
                if kw not in seen_doc:
                    seen_doc.add(kw)
                    unique_doc_keywords.append(kw)
            
            # é™åˆ¶æ•°é‡ï¼ˆä½¿ç”¨ MAX_DISPLAY_KEYWORDSï¼‰
            from config import Settings
            max_global_keywords = getattr(Settings, 'MAX_DISPLAY_KEYWORDS', 5)
            
            # åˆ†åˆ«é™åˆ¶é—®é¢˜å…³é”®è¯å’Œæ–‡æ¡£å…³é”®è¯
            final_question_keywords = unique_question_keywords[:max_global_keywords]
            remaining_slots = max_global_keywords - len(final_question_keywords)
            final_doc_keywords = unique_doc_keywords[:remaining_slots] if remaining_slots > 0 else []
            
            logger.info(f"[å…³é”®è¯é™åˆ¶] é…ç½®å€¼: MAX_DISPLAY_KEYWORDS={max_global_keywords}")
            logger.info(f"[å…³é”®è¯è¾“å‡º] é—®é¢˜å…³é”®è¯: {final_question_keywords}")
            logger.info(f"[å…³é”®è¯è¾“å‡º] æ–‡æ¡£å…³é”®è¯: {final_doc_keywords}")
            
            # è¾“å‡ºç»“æ„åŒ–å…³é”®å­—ï¼ˆåŒºåˆ†æ¥æºï¼‰
            keywords_data = {
                "question": final_question_keywords,
                "document": final_doc_keywords
            }
            if final_question_keywords or final_doc_keywords:
                yield ('KEYWORDS', json.dumps(keywords_data, ensure_ascii=False))

            # 7. è¾“å‡ºå‚è€ƒæ¥æºï¼ˆä½¿ç”¨æ–°å·¥å…·å‡½æ•°ï¼‰
            reference_entries = build_reference_entries(final_nodes, filtered_map)

            if use_insert_block and filtered_results:
                # InsertBlock æ¨¡å¼ï¼šè¿”å›æ‰€æœ‰åŸå§‹èŠ‚ç‚¹ï¼Œä½†æ ‡æ³¨å“ªäº›è¢«é€‰ä¸­
                yield ('CONTENT', "\n\n**å‚è€ƒæ¥æºï¼ˆå…¨éƒ¨æ£€ç´¢ç»“æœï¼‰:**")
                full_response += "\n\nå‚è€ƒæ¥æºï¼ˆå…¨éƒ¨æ£€ç´¢ç»“æœï¼‰:"

                # éå†æ‰€æœ‰åŸå§‹èŠ‚ç‚¹ï¼Œæ ‡æ³¨å“ªäº›è¢«é€‰ä¸­
                for i, node in enumerate(final_nodes):
                    file_name = node.node.metadata.get('file_name', 'æœªçŸ¥')
                    initial_score = node.node.metadata.get('initial_score', 0.0)
                    key = f"{file_name}_{node.score}"

                    # æ£€æŸ¥è¯¥èŠ‚ç‚¹æ˜¯å¦åœ¨è¿‡æ»¤ç»“æœä¸­
                    filtered_info = filtered_map.get(key)

                    # æå–æ£€ç´¢å…ƒæ•°æ®
                    retrieval_sources = node.node.metadata.get('retrieval_sources', [])
                    vector_score = node.node.metadata.get('vector_score', 0.0)
                    bm25_score = node.node.metadata.get('bm25_score', 0.0)
                    vector_rank = node.node.metadata.get('vector_rank')
                    bm25_rank = node.node.metadata.get('bm25_rank')
                    
                    source_data = {
                        "id": i + 1,
                        "fileName": file_name,
                        "initialScore": f"{initial_score:.4f}",
                        "rerankedScore": f"{node.score:.4f}",
                        "content": node.node.text.strip(),
                        # æ£€ç´¢å…ƒæ•°æ®
                        "retrievalSources": retrieval_sources,
                        "vectorScore": f"{vector_score:.4f}",
                        "bm25Score": f"{bm25_score:.4f}",
                        # InsertBlock ç‰¹æœ‰å­—æ®µ
                        "canAnswer": filtered_info is not None,
                        "reasoning": filtered_info.get('reasoning', '') if filtered_info else '',
                        "keyPassage": filtered_info.get('key_passage', '') if filtered_info else ''
                    }
                    
                    # æ·»åŠ æ’åä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    if vector_rank is not None:
                        source_data['vectorRank'] = vector_rank
                    if bm25_rank is not None:
                        source_data['bm25Rank'] = bm25_rank
                    
                    # æ·»åŠ åŒ¹é…çš„å…³é”®è¯ï¼ˆå¦‚æœæ˜¯å…³é”®è¯æ£€ç´¢ï¼‰
                    if 'keyword' in retrieval_sources:
                        matched_keywords = node.node.metadata.get('bm25_matched_keywords', [])
                        if matched_keywords:
                            source_data['matchedKeywords'] = matched_keywords

                    yield ('SOURCE', json.dumps(source_data, ensure_ascii=False))

                    full_response += (
                        f"\n[{source_data['id']}] æ–‡ä»¶: {source_data['fileName']}, "
                        f"é‡æ’åˆ†: {source_data['rerankedScore']}, "
                        f"å¯å›ç­”: {source_data['canAnswer']}"
                    )

            elif final_nodes:
                # æ™®é€šæ¨¡å¼ï¼šæ˜¾ç¤ºæ‰€æœ‰æ£€ç´¢ç»“æœï¼ˆä½¿ç”¨æ–°å·¥å…·å‡½æ•°ï¼‰
                yield ('CONTENT', "\n\n**å‚è€ƒæ¥æº:**")
                full_response += "\n\nå‚è€ƒæ¥æº:"

                for source_msg in format_sources(final_nodes):
                    yield source_msg
                    if isinstance(source_msg, tuple) and source_msg[0] == "SOURCE":
                        data = json.loads(source_msg[1])
                        full_response += (
                            f"\n[{data['id']}] æ–‡ä»¶: {data['fileName']}, "
                            f"åˆå§‹åˆ†: {data['initialScore']}, "
                            f"é‡æ’åˆ†: {data['rerankedScore']}"
                        )

            # ä½¿ç”¨æ–°å·¥å…·å‡½æ•°è®°å½•å‚è€ƒæ–‡çŒ®
            log_reference_details(
                question=question,
                references=reference_entries,
                mode="single"
            )

            yield ('DONE', '')

            # 7. ä¿å­˜æ—¥å¿—ï¼ˆä½¿ç”¨æ–°å·¥å…·å‡½æ•°ï¼‰
            save_qa_log(
                question=question,
                response=full_response,
                client_ip=client_ip,
                has_rag=bool(final_nodes),
                use_insert_block=use_insert_block
            )

        except Exception as e:
            error_msg = f"å¤„ç†é”™è¯¯: {str(e)}"
            logger.error(f"çŸ¥è¯†é—®ç­”å¤„ç†å‡ºé”™: {e}", exc_info=True)
            yield ('ERROR', error_msg)

    def _retrieve_and_rerank(self, question: str, rerank_top_n: int, conversation_history: Optional[List[Dict]] = None):
        """
        æ£€ç´¢å’Œé‡æ’åºï¼ˆæ”¯æŒå­é—®é¢˜åˆ†è§£ï¼‰
        
        Args:
            question: ç”¨æˆ·æŸ¥è¯¢
            rerank_top_n: é‡æ’åºè¿”å›æ•°é‡
            conversation_history: å¯¹è¯å†å²ï¼ˆç”¨äºå¤šè½®åœºæ™¯ï¼‰
            
        Returns:
            æ£€ç´¢èŠ‚ç‚¹åˆ—è¡¨
        """
        # å¦‚æœå¯ç”¨äº†å­é—®é¢˜åˆ†è§£å™¨ï¼Œå°è¯•ä½¿ç”¨åˆ†è§£æ£€ç´¢
        if self.sub_question_decomposer and self.sub_question_decomposer.enabled:
            logger.info("[æ£€ç´¢ç­–ç•¥] å°è¯•ä½¿ç”¨å­é—®é¢˜åˆ†è§£æ£€ç´¢ï¼ˆå¤šè½®ï¼‰")
            try:
                # æ³¨æ„ï¼šå¤šè½®åœºæ™¯ä½¿ç”¨é»˜è®¤retrieverï¼Œå› ä¸ºæ²¡æœ‰æ„å›¾åˆ†ç±»
                # å¦‚æœéœ€è¦æ”¯æŒå¤šè½®+æ„å›¾è·¯ç”±ï¼Œéœ€è¦åœ¨è¿™é‡Œä¹Ÿæ·»åŠ æ„å›¾åˆ†ç±»é€»è¾‘
                nodes, metadata = self.sub_question_decomposer.retrieve_with_decomposition(
                    query=question,
                    rerank_top_n=rerank_top_n,
                    conversation_history=conversation_history
                )
                
                # è®°å½•åˆ†è§£å…ƒæ•°æ®
                if metadata.get('decomposed'):
                    logger.info(
                        f"[å­é—®é¢˜æ£€ç´¢] åˆ†è§£æ£€ç´¢å®Œæˆ | "
                        f"å­é—®é¢˜æ•°: {len(metadata['sub_questions'])} | "
                        f"è¿”å›èŠ‚ç‚¹æ•°: {len(nodes)}"
                    )
                    # è®°å½•è¯¦ç»†çš„å­é—®é¢˜ä¿¡æ¯åˆ°æ—¥å¿—
                    for i, sub_result in enumerate(metadata['sub_results'], 1):
                        logger.info(
                            f"  å­é—®é¢˜{i}: {sub_result['sub_question']} | "
                            f"èŠ‚ç‚¹æ•°: {sub_result['node_count']} | "
                            f"æœ€é«˜åˆ†: {sub_result['top_score']:.4f}"
                        )
                    
                    # å¯é€‰ï¼šç”Ÿæˆå­é—®é¢˜ç­”æ¡ˆåˆæˆï¼ˆå¦‚æœæœ‰sub_answersï¼‰
                    if metadata.get('sub_answers') and len(metadata['sub_answers']) > 0:
                        try:
                            synthesized_answer = self.sub_question_decomposer.synthesize_answer(
                                original_query=question,
                                sub_answers=metadata['sub_answers']
                            )
                            if synthesized_answer:
                                # å°†åˆæˆç­”æ¡ˆæ·»åŠ åˆ°metadataï¼Œä¾›åç»­ä½¿ç”¨
                                metadata['synthesized_answer'] = synthesized_answer
                                # å­˜å‚¨ä¸ºå®ä¾‹å˜é‡ï¼Œä¾›_build_promptä½¿ç”¨
                                self._last_synthesized_answer = synthesized_answer
                                logger.info(f"[ç­”æ¡ˆåˆæˆ] å·²ç”Ÿæˆåˆæˆç­”æ¡ˆ | é•¿åº¦: {len(synthesized_answer)}")
                        except Exception as synth_e:
                            logger.warning(f"[ç­”æ¡ˆåˆæˆ] åˆæˆå¤±è´¥: {synth_e}")
                else:
                    logger.info("[å­é—®é¢˜æ£€ç´¢] æœªåˆ†è§£ï¼Œä½¿ç”¨æ ‡å‡†æ£€ç´¢")
                
                return nodes
                
            except Exception as e:
                logger.error(f"[å­é—®é¢˜æ£€ç´¢] åˆ†è§£æ£€ç´¢å¤±è´¥: {e}", exc_info=True)
                logger.info("[å­é—®é¢˜æ£€ç´¢] å›é€€åˆ°æ ‡å‡†æ£€ç´¢æµç¨‹")
                # ç»§ç»­æ‰§è¡Œæ ‡å‡†æ£€ç´¢
        
        # æ ‡å‡†æ£€ç´¢æµç¨‹
        logger.info(f"[å•çŸ¥è¯†åº“æ£€ç´¢] å¼€å§‹æ£€ç´¢é—®é¢˜: {question}")
        logger.info(f"ğŸ” [DEBUG] ä½¿ç”¨çš„æ£€ç´¢å™¨å¯¹è±¡ID: {id(self.retriever)}")
        logger.info(f"ğŸ” [DEBUG] æ£€ç´¢å™¨ç±»å‹: {type(self.retriever).__name__}")
        retrieved_nodes = self.retriever.retrieve(question)
        
        # ğŸ” DEBUG: è®°å½•åˆå§‹æ£€ç´¢å¾—åˆ†
        if retrieved_nodes:
            initial_scores = [f"{n.score:.4f}" for n in retrieved_nodes[:5]]
            logger.info(f"[DEBUG] å•çŸ¥è¯†åº“åˆå§‹æ£€ç´¢Top5å¾—åˆ†: {', '.join(initial_scores)}")

        # å–å‰ N ä¸ªé€å…¥é‡æ’
        reranker_input_top_n = Settings.RERANKER_INPUT_TOP_N
        logger.info(f"[å•çŸ¥è¯†åº“æ£€ç´¢] é…ç½®æ£€æŸ¥ - RERANKER_INPUT_TOP_N: {reranker_input_top_n}")
        
        # è¯¦ç»†æ£€æŸ¥ retrieved_nodes
        logger.info(f"[å•çŸ¥è¯†åº“æ£€ç´¢] retrieved_nodes ç±»å‹: {type(retrieved_nodes)}")
        logger.info(f"[å•çŸ¥è¯†åº“æ£€ç´¢] retrieved_nodes é•¿åº¦: {len(retrieved_nodes) if retrieved_nodes else 'None'}")
        
        if retrieved_nodes and len(retrieved_nodes) > 0:
            logger.info(f"[å•çŸ¥è¯†åº“æ£€ç´¢] ç¬¬ä¸€ä¸ªèŠ‚ç‚¹é¢„è§ˆ: {retrieved_nodes[0].node.get_content()[:100]}...")
        
        reranker_input = retrieved_nodes[:reranker_input_top_n]

        logger.info(
            f"[å•çŸ¥è¯†åº“æ£€ç´¢] åˆæ£€ç´¢æ‰¾åˆ° {len(retrieved_nodes)} ä¸ªèŠ‚ç‚¹, "
            f"é€‰å–å‰ {len(reranker_input)} ä¸ªé€å…¥é‡æ’"
        )
        
        # å¦‚æœåˆå§‹æ£€ç´¢ä¸ºç©ºï¼Œæ‰“å°è­¦å‘Š
        if len(retrieved_nodes) == 0:
            logger.warning(
                f"[å•çŸ¥è¯†åº“æ£€ç´¢] åˆå§‹æ£€ç´¢ç»“æœä¸ºç©ºï¼\n"
                f"  é—®é¢˜: {question}\n"
                f"  æ£€ç´¢å™¨çŠ¶æ€: {self.retriever is not None}\n"
                f"  å¯èƒ½åŸå› : çŸ¥è¯†åº“ä¸ºç©ºã€ç´¢å¼•æŸåã€æˆ–é—®é¢˜ä¸çŸ¥è¯†åº“å®Œå…¨ä¸ç›¸å…³"
            )

        # é‡æ’åº
        logger.info(f"[å•çŸ¥è¯†åº“æ£€ç´¢] å‡†å¤‡é‡æ’åº - reranker_input é•¿åº¦: {len(reranker_input)}")
        
        if reranker_input:
            logger.info(f"[å•çŸ¥è¯†åº“æ£€ç´¢] âœ“ è¿›å…¥é‡æ’åºåˆ†æ”¯ï¼Œå¼€å§‹è°ƒç”¨ Reranker æ¨¡å‹")
            logger.info(f"[DEBUG] Reranker å¯¹è±¡ID: {id(self.reranker)}")
            logger.info(f"[DEBUG] Reranker ç±»å‹: {type(self.reranker).__name__}")
            logger.info(f"[DEBUG] Reranker top_n: {self.reranker.top_n}")
            logger.info(f"[DEBUG] é—®é¢˜é•¿åº¦: {len(question)} å­—ç¬¦")
            logger.info(f"[DEBUG] é—®é¢˜å†…å®¹: {question[:100]}...")
            
            # ğŸ§ª ä¸´æ—¶å®éªŒï¼šé‡æ–°åˆ›å»º Reranker æ¥éªŒè¯æ˜¯å¦æ˜¯çŠ¶æ€æ±¡æŸ“é—®é¢˜
            logger.warning("ğŸ§ª [å®éªŒ] ä¸´æ—¶é‡æ–°åˆ›å»º Reranker æ¥æµ‹è¯•...")
            from llama_index.core.postprocessor import SentenceTransformerRerank
            temp_reranker = SentenceTransformerRerank(
                model=Settings.RERANKER_MODEL_PATH,
                top_n=Settings.RERANK_TOP_N,
                device=Settings.DEVICE
            )
            logger.info(f"ğŸ§ª [å®éªŒ] ä¸´æ—¶ Reranker å¯¹è±¡ID: {id(temp_reranker)}")
            
            reranked_nodes = temp_reranker.postprocess_nodes(
                reranker_input,
                query_bundle=QueryBundle(question)
            )
            logger.info("ğŸ§ª [å®éªŒ] ä½¿ç”¨ä¸´æ—¶ Reranker å®Œæˆé‡æ’åº")
            logger.info(f"[å•çŸ¥è¯†åº“æ£€ç´¢] âœ“ Reranker å¤„ç†å®Œæˆï¼Œå¾—åˆ° {len(reranked_nodes)} ä¸ªèŠ‚ç‚¹")
            # ğŸ” DEBUG: è®°å½•é‡æ’åºåå¾—åˆ†
            if reranked_nodes:
                rerank_scores = [f"{n.score:.4f}" for n in reranked_nodes[:5]]
                logger.info(f"[DEBUG] å•çŸ¥è¯†åº“é‡æ’åºåTop5å¾—åˆ†: {', '.join(rerank_scores)}")
        else:
            logger.warning(f"[å•çŸ¥è¯†åº“æ£€ç´¢] âš ï¸ reranker_input ä¸ºç©ºï¼Œè·³è¿‡é‡æ’åºï¼")
            reranked_nodes = []

        # é˜ˆå€¼è¿‡æ»¤
        threshold = Settings.RERANK_SCORE_THRESHOLD
        final_nodes = [
            node for node in reranked_nodes
            if node.score >= threshold
        ]
        
        #  DEBUG: è®°å½•è¿‡æ»¤åå¾—åˆ†
        if final_nodes:
            final_scores = [f"{n.score:.4f}" for n in final_nodes[:5]]
            logger.info(f"[DEBUG] å•çŸ¥è¯†åº“é˜ˆå€¼è¿‡æ»¤åTop5å¾—åˆ†: {', '.join(final_scores)}")

        logger.info(
            f"[å•çŸ¥è¯†åº“æ£€ç´¢] é‡æ’åºåæœ‰ {len(reranked_nodes)} ä¸ªèŠ‚ç‚¹, "
            f"ç»è¿‡é˜ˆå€¼ {threshold} è¿‡æ»¤åå‰©ä¸‹ {len(final_nodes)} ä¸ª"
        )
        
        # å¦‚æœé˜ˆå€¼è¿‡æ»¤åä¸ºç©ºï¼Œæ‰“å°è¯¦ç»†ä¿¡æ¯
        if len(reranked_nodes) > 0 and len(final_nodes) == 0:
            max_score = max(node.score for node in reranked_nodes) if reranked_nodes else 0.0
            logger.warning(
                f"[å•çŸ¥è¯†åº“æ£€ç´¢] é˜ˆå€¼è¿‡æ»¤åç»“æœä¸ºç©ºï¼\n"
                f"  é‡æ’åºèŠ‚ç‚¹æ•°: {len(reranked_nodes)}\n"
                f"  æœ€é«˜åˆ†æ•°: {max_score:.4f}\n"
                f"  é˜ˆå€¼: {threshold}\n"
                f"  å»ºè®®: é™ä½ RERANK_SCORE_THRESHOLD æˆ–æ£€æŸ¥ Reranker æ¨¡å‹"
            )

        # åº”ç”¨æœ€ç»ˆæ•°é‡é™åˆ¶
        result = final_nodes[:rerank_top_n]
        logger.info(f"[å•çŸ¥è¯†åº“æ£€ç´¢] æœ€ç»ˆè¿”å› {len(result)} ä¸ªèŠ‚ç‚¹")
        return result

    def _call_llm(self, llm, prompt_parts, enable_thinking: bool = False):
        """
        è°ƒç”¨ LLMï¼Œæ”¯æŒæ€è€ƒå†…å®¹å’Œæ­£æ–‡å†…å®¹çš„åˆ†ç¦»ï¼ˆä½¿ç”¨æ–°å·¥å…·å‡½æ•°ï¼‰

        Args:
            llm: LLM å®ä¾‹
            prompt_parts: æç¤ºè¯å­—å…¸
            enable_thinking: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼ï¼ˆç”¨äºè§£æè¾“å‡ºï¼‰

        Note:
            æ”¯æŒä¸¤ç§æ€è€ƒæ¨¡å¼ï¼š
            1. é˜¿é‡Œäº‘åŸç”Ÿ reasoning_content å­—æ®µï¼ˆæ¨èï¼‰
            2. æ–‡æœ¬æ ‡è®°æ–¹å¼ï¼ˆå…¼å®¹å…¶ä»–æ¨¡å‹ï¼‰
        """
        logger.info(f"ä½¿ç”¨å¤–éƒ¨ Prompt:\n{prompt_parts['fallback_prompt'][:200]}...")

        response_stream = self.llm_wrapper.stream(
            llm,
            prompt=prompt_parts['fallback_prompt'],
            system_prompt=prompt_parts['system_prompt'],
            user_prompt=prompt_parts['user_prompt'],
            assistant_context=prompt_parts['assistant_context'],
            use_chat_mode=Settings.USE_CHAT_MODE,
            enable_thinking=enable_thinking
        )

        # ä½¿ç”¨æ–°å·¥å…·å‡½æ•°è§£ææµå¼è¾“å‡º
        if enable_thinking:
            yield from parse_thinking_stream(response_stream)
        else:
            yield from parse_normal_stream(response_stream)

   

    def _save_log(self, question: str, response: str, client_ip: str, has_rag: bool, use_insert_block: bool = False):
        """
        ã€å·²åºŸå¼ƒã€‘ä¿å­˜é—®ç­”æ—¥å¿— - å·²è¢« save_qa_log å·¥å…·å‡½æ•°æ›¿ä»£
        ä¿ç•™æ­¤æ–¹æ³•ä½œä¸ºå¤‡ä»½ï¼Œå¾…æµ‹è¯•é€šè¿‡åå¯åˆ é™¤
        """
        from utils import QALogger
        qa_logger = QALogger(Settings.LOG_DIR)
        qa_logger.save_log(
            question,
            response,
            'knowledge_qa_stream',
            metadata={
                "ip": client_ip,
                "answer_type": "rag" if has_rag else "general",
                "chat_mode": Settings.USE_CHAT_MODE,
                "insert_block_mode": use_insert_block
            }
        )

    def process_conversation(
        self,
        question: str,
        session_id: str,
        enable_thinking: bool,
        rerank_top_n: int,
        llm,
        client_ip: str = "unknown",
        use_insert_block: bool = False,
        insert_block_llm_id: Optional[str] = None
    ) -> Generator[str, None, None]:
        """
        å¤„ç†æ”¯æŒå¤šè½®å¯¹è¯çš„çŸ¥è¯†é—®ç­”

        æµç¨‹ï¼š
        1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        2. InsertBlock è¿‡æ»¤ï¼ˆå¯é€‰ï¼‰
        3. è·å–å†å²å¯¹è¯
        4. ä½¿ç”¨çŸ¥è¯†é—®ç­”çš„æç¤ºè¯æ„å»º promptï¼ˆå°†å†å²å¯¹è¯æ³¨å…¥åˆ°ä¸Šä¸‹æ–‡ä¸­ï¼‰
        5. è°ƒç”¨ LLM
        6. å­˜å‚¨æœ¬è½®å¯¹è¯
        7. è¿”å›å‚è€ƒæ¥æº

        Args:
            question: é—®é¢˜å†…å®¹
            session_id: ä¼šè¯ID
            enable_thinking: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼
            rerank_top_n: é‡æ’åºåè¿”å›çš„æ–‡æ¡£æ•°é‡
            llm: LLM å®ä¾‹
            client_ip: å®¢æˆ·ç«¯ IP
            use_insert_block: æ˜¯å¦ä½¿ç”¨ InsertBlock è¿‡æ»¤æ¨¡å¼
            insert_block_llm_id: InsertBlock ä½¿ç”¨çš„ LLM ID

        Yields:
            SSE æ ¼å¼çš„å“åº”æµ
        """
        full_response = ""
        
        # æ¸…ç©ºä¸Šä¸€æ¬¡çš„åˆæˆç­”æ¡ˆï¼ˆé¿å…æ±¡æŸ“ï¼‰
        self._last_synthesized_answer = None

        try:
            logger.info(
                f"å¤„ç†å¤šè½®å¯¹è¯: ä¼šè¯ {session_id[:8]}... | '{question}' | "
                f"æ€è€ƒæ¨¡å¼: {enable_thinking} | InsertBlock: {use_insert_block}"
            )

            # è·å–å¯¹è¯ç®¡ç†å™¨
            from flask import current_app
            knowledge_service = current_app.knowledge_service
            conversation_manager = knowledge_service.conversation_manager

            if not conversation_manager:
                raise ValueError("å¯¹è¯ç®¡ç†å™¨æœªåˆå§‹åŒ–")

            # è¿”å›ä¼šè¯ID
            yield f"SESSION:{session_id}"

            # 1. è·å–æœ€è¿‘çš„å¯¹è¯å†å²ï¼ˆç”¨äºå­é—®é¢˜åˆ†è§£ï¼‰
            from config import Settings as AppSettings
            recent_turns_for_decomp = getattr(AppSettings, 'SUBQUESTION_HISTORY_COMPRESS_TURNS', 5)
            
            try:
                conversation_history_for_decomp = conversation_manager.get_recent_history(
                    session_id=session_id,
                    limit=recent_turns_for_decomp
                )
            except Exception as e:
                logger.warning(f"è·å–å¯¹è¯å†å²ç”¨äºå­é—®é¢˜åˆ†è§£å¤±è´¥: {e}")
                conversation_history_for_decomp = None
            
            # 2. æ£€ç´¢
            # å¦‚æœå‰ç«¯è®¾ç½®å‚è€ƒæ•°é‡ä¸º 0ï¼Œè·³è¿‡æ£€ç´¢
            if rerank_top_n == 0:
                logger.info("[å¯¹è¯-æ£€ç´¢è·³è¿‡] å‰ç«¯è®¾ç½®å‚è€ƒæ•°é‡ä¸º 0ï¼Œè·³è¿‡æ£€ç´¢å’Œå­é—®é¢˜åˆ†è§£")
                final_nodes = []
                hidden_nodes = []
            else:
                yield "CONTENT:æ­£åœ¨è¿›è¡Œæ··åˆæ£€ç´¢..."
                full_response += "æ­£åœ¨è¿›è¡Œæ··åˆæ£€ç´¢...\n"

                final_nodes = self._retrieve_and_rerank(
                    question, 
                    rerank_top_n,
                    conversation_history=conversation_history_for_decomp
                )
                
                # 2.5 éšè—çŸ¥è¯†åº“æ£€ç´¢ï¼ˆå¹¶è¡Œè¿›è¡Œï¼‰
                hidden_nodes = []
                if self.hidden_kb_retriever and self.hidden_kb_retriever.enabled:
                    try:
                        logger.info("[å¯¹è¯-éšè—çŸ¥è¯†åº“] å¼€å§‹å¹¶è¡Œæ£€ç´¢...")
                        hidden_nodes = self.hidden_kb_retriever.retrieve(question)
                        if hidden_nodes:
                            logger.info(f"[å¯¹è¯-éšè—çŸ¥è¯†åº“] æ£€ç´¢æˆåŠŸ | è¿”å› {len(hidden_nodes)} æ¡")
                    except Exception as e:
                        logger.warning(f"[å¯¹è¯-éšè—çŸ¥è¯†åº“] æ£€ç´¢å¤±è´¥: {e}")
                        hidden_nodes = []

            # 2. å¦‚æœå¯ç”¨ InsertBlock æ¨¡å¼ï¼Œè¿›è¡Œæ™ºèƒ½è¿‡æ»¤
            filtered_results = None
            filtered_map = None
            nodes_for_prompt = final_nodes

            if use_insert_block and final_nodes and self.insert_block_filter:
                start_msg = f"æ­£åœ¨ä½¿ç”¨ç²¾å‡†æ£€ç´¢åˆ†æ {len(final_nodes)} ä¸ªæ–‡æ¡£...\næç¤ºï¼šç³»ç»Ÿæ­£åœ¨é€ä¸ªåˆ¤æ–­æ¯ä¸ªæ–‡æ¡£æ˜¯å¦èƒ½å›ç­”æ‚¨çš„é—®é¢˜ï¼Œè¯·ç¨å€™"
                yield f"CONTENT:{start_msg}"
                full_response += start_msg + "\n"
                
                # ä½¿ç”¨é˜Ÿåˆ—æ”¶é›†è¿›åº¦
                import queue
                import threading
                progress_queue = queue.Queue()
                filter_done = threading.Event()
                
                # å®šä¹‰è¿›åº¦å›è°ƒå‡½æ•°ï¼ˆå°†è¿›åº¦æ”¾å…¥é˜Ÿåˆ—ï¼‰
                def progress_callback(processed, total):
                    logger.info(f"[å¯¹è¯-ç²¾å‡†æ£€ç´¢è¿›åº¦] {processed}/{total} ä¸ªæ–‡æ¡£å·²åˆ†æ")
                    progress_queue.put((processed, total))
                
                # åœ¨åå°çº¿ç¨‹æ‰§è¡Œè¿‡æ»¤
                def run_filter():
                    try:
                        result = self.insert_block_filter.filter_nodes(
                            question=question,
                            nodes=final_nodes,
                            llm_id=insert_block_llm_id,
                            progress_callback=progress_callback
                        )
                        progress_queue.put(('DONE', result))
                    except Exception as e:
                        progress_queue.put(('ERROR', e))
                    finally:
                        filter_done.set()
                
                filter_thread = threading.Thread(target=run_filter, daemon=True)
                filter_thread.start()
                
                # ä¸»çº¿ç¨‹å®šæœŸæ£€æŸ¥è¿›åº¦å¹¶å‘é€
                last_progress = 0
                filtered_results = None
                
                while not filter_done.is_set():
                    try:
                        # ç­‰å¾…0.5ç§’æˆ–ç›´åˆ°æœ‰æ–°è¿›åº¦
                        item = progress_queue.get(timeout=0.5)
                        
                        if isinstance(item, tuple):
                            if item[0] == 'DONE':
                                filtered_results = item[1]
                                break
                            elif item[0] == 'ERROR':
                                logger.error(f"å¯¹è¯-ç²¾å‡†æ£€ç´¢è¿‡æ»¤å¤±è´¥: {item[1]}")
                                break
                            else:
                                # è¿›åº¦æ›´æ–°
                                processed, total = item
                                # æ¯å¤„ç†5ä¸ªæ–‡æ¡£å‘é€ä¸€æ¬¡è¿›åº¦ï¼ˆé¿å…åˆ·å±ï¼‰
                                if processed - last_progress >= 5 or processed == total:
                                    progress_msg = f"ğŸ“Š è¿›åº¦: {processed}/{total} ({int(processed/total*100)}%)"
                                    yield f"CONTENT:{progress_msg}"
                                    full_response += progress_msg + "\n"
                                    last_progress = processed
                    except queue.Empty:
                        # è¶…æ—¶ï¼Œç»§ç»­ç­‰å¾…
                        continue
                
                # ç­‰å¾…çº¿ç¨‹ç»“æŸ
                filter_thread.join(timeout=1)

                if filtered_results:
                    yield f"CONTENT:æ‰¾åˆ° {len(filtered_results)} ä¸ªå¯å›ç­”çš„èŠ‚ç‚¹"
                    full_response += f"æ‰¾åˆ° {len(filtered_results)} ä¸ªå¯å›ç­”çš„èŠ‚ç‚¹\n"
                    nodes_for_prompt = None
                    filtered_map = {}
                    for result in filtered_results:
                        key = f"{result['file_name']}_{result['reranked_score']}"
                        filtered_map[key] = result
                else:
                    yield "CONTENT:æœªæ‰¾åˆ°å¯ç›´æ¥å›ç­”çš„èŠ‚ç‚¹ï¼Œå°†ä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ"
                    full_response += "æœªæ‰¾åˆ°å¯ç›´æ¥å›ç­”çš„èŠ‚ç‚¹ï¼Œå°†ä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ\n"
                    filtered_results = None

            # 3. è·å–å†å²å¯¹è¯
            from config import Settings as AppSettings
            recent_turns = getattr(AppSettings, 'MAX_RECENT_TURNS', 6)
            relevant_turns = getattr(AppSettings, 'MAX_RELEVANT_TURNS', 3)
            max_summary_turns = getattr(AppSettings, 'MAX_SUMMARY_TURNS', 12)

            # 3.1 è·å–æœ€è¿‘çš„å¯¹è¯å†å²
            recent_history = conversation_manager.get_recent_history(
                session_id=session_id,
                limit=recent_turns
            )

            # 3.2 æ£€ç´¢ä¸å½“å‰é—®é¢˜ç›¸å…³çš„å†å²å¯¹è¯
            relevant_history = []
            if relevant_turns > 0:
                try:
                    relevant_history = conversation_manager.retrieve_relevant_history(
                        session_id=session_id,
                        current_query=question,
                        top_k=relevant_turns
                    )
                    # è¿‡æ»¤æ‰å·²ç»åœ¨æœ€è¿‘å¯¹è¯ä¸­çš„è½®æ¬¡ï¼ˆé¿å…é‡å¤ï¼‰
                    recent_turn_ids = {turn.get('turn_id') for turn in recent_history if turn.get('turn_id')}
                    relevant_history = [
                        turn for turn in relevant_history
                        if turn.get('turn_id') not in recent_turn_ids
                    ]
                    logger.info(f"æ£€ç´¢åˆ° {len(relevant_history)} æ¡ç›¸å…³å†å²å¯¹è¯ï¼ˆæ’é™¤æœ€è¿‘å¯¹è¯åï¼‰")
                except Exception as e:
                    logger.warning(f"æ£€ç´¢ç›¸å…³å†å²å¯¹è¯å¤±è´¥: {e}")
                    relevant_history = []

            # 4. æ„å»ºå†å²å¯¹è¯æ‘˜è¦ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
            # è·å–ä¼šè¯æ€»è½®æ•°
            try:
                all_history = conversation_manager.get_recent_history(
                    session_id=session_id,
                    limit=100  # å‡è®¾æœ€å¤š100è½®
                )
                total_turns = len(all_history)
            except Exception as e:
                logger.warning(f"è·å–æ€»å¯¹è¯è½®æ•°å¤±è´¥: {e}")
                total_turns = len(recent_history)
                all_history = recent_history

            history_summary = None

            # åªæœ‰å½“æ€»è½®æ•°è¶…è¿‡ MAX_SUMMARY_TURNS æ—¶æ‰ç”Ÿæˆæ‘˜è¦ï¼ˆé¿å…é¢‘ç¹æ‘˜è¦ï¼‰
            if total_turns > max_summary_turns:
                # æ’é™¤æœ€è¿‘Nè½®ï¼Œå‰©ä½™çš„ç”¨äºç”Ÿæˆæ‘˜è¦
                old_history = all_history[:-recent_turns] if len(all_history) > recent_turns else []

                if old_history and len(old_history) >= 3:  # è‡³å°‘3è½®æ‰å€¼å¾—æ‘˜è¦
                    # æ£€æŸ¥æ‘˜è¦ç¼“å­˜
                    cache_key = f"{session_id}_summary"
                    current_time = time.time()

                    if hasattr(conversation_manager, '_summary_cache'):
                        cache_entry = conversation_manager._summary_cache.get(cache_key)
                        if cache_entry:
                            cache_age = current_time - cache_entry.get('timestamp', 0)
                            summarized_count = cache_entry.get('summarized_until', 0)

                            # å¦‚æœç¼“å­˜æœ‰æ•ˆä¸”å¯¹è¯æ•°é‡æ²¡å˜åŒ–å¤ªå¤šï¼ˆå…è®¸Â±2è½®å·®å¼‚ï¼‰ï¼Œä½¿ç”¨ç¼“å­˜
                            if (cache_age < AppSettings.SUMMARY_CACHE_TTL and
                                abs(len(old_history) - summarized_count) <= 2):
                                history_summary = cache_entry.get('summary')
                                logger.info(f"ä½¿ç”¨ç¼“å­˜çš„å†å²æ‘˜è¦ (ç¼“å­˜æ—¶é•¿: {cache_age:.0f}s)")

                    # å¦‚æœæ²¡æœ‰ç¼“å­˜æˆ–ç¼“å­˜å¤±æ•ˆï¼Œç”Ÿæˆæ–°æ‘˜è¦
                    if not history_summary:
                        try:
                            history_summary = conversation_manager.summarize_old_conversations(
                                session_id=session_id,
                                conversations=old_history
                            )

                            # æ›´æ–°ç¼“å­˜
                            if history_summary and hasattr(conversation_manager, '_summary_cache'):
                                conversation_manager._summary_cache[cache_key] = {
                                    'summary': history_summary,
                                    'summarized_until': len(old_history),
                                    'timestamp': current_time
                                }
                                logger.info(f"å·²ç”Ÿæˆå¹¶ç¼“å­˜å†å²æ‘˜è¦ (è¦†ç›– {len(old_history)} è½®)")
                        except Exception as e:
                            logger.warning(f"ç”Ÿæˆå†å²æ‘˜è¦å¤±è´¥: {e}")
                            history_summary = None
                else:
                    logger.debug(f"æ—§å¯¹è¯è½®æ•°({len(old_history)})ä¸è¶³ï¼Œè·³è¿‡æ‘˜è¦ç”Ÿæˆ")
            else:
                logger.debug(f"æ€»è½®æ•°({total_turns})æœªè¾¾æ‘˜è¦é˜ˆå€¼({max_summary_turns})ï¼Œè·³è¿‡æ‘˜è¦")

            # 5. ä½¿ç”¨ä¼˜åŒ–çš„æç¤ºè¯æ„å»ºæ–¹å¼ï¼ˆæ³¨å…¥å†å²å¯¹è¯å’Œéšè—çŸ¥è¯†åº“ï¼‰
            prompt_parts = self._build_prompt_with_history(
                question,
                enable_thinking,
                nodes_for_prompt,
                filtered_results=filtered_results,
                recent_history=recent_history,
                relevant_history=relevant_history,
                history_summary=history_summary,
                hidden_nodes=hidden_nodes
            )

            # 6. è¾“å‡ºçŠ¶æ€
            status_msg = (
                "å·²æ‰¾åˆ°ç›¸å…³èµ„æ–™ï¼Œæ­£åœ¨ç”Ÿæˆå›ç­”..."
                if final_nodes
                else "æœªæ‰¾åˆ°é«˜ç›¸å…³æ€§èµ„æ–™ï¼ŒåŸºäºé€šç”¨çŸ¥è¯†å’Œå¯¹è¯å†å²å›ç­”..."
            )
            yield f"CONTENT:{status_msg}"
            full_response += status_msg + "\n"

            # 7. è°ƒç”¨ LLM
            assistant_response = ""
            for result in self._call_llm(llm, prompt_parts, enable_thinking=enable_thinking):
                # result æ˜¯å…ƒç»„ (prefix_type, content)
                prefix_type, chunk = result
                if prefix_type == 'THINK':
                    yield f"THINK:{chunk}"
                    # æ€è€ƒå†…å®¹ä¸è®¡å…¥ assistant_response
                elif prefix_type == 'CONTENT':
                    yield f"CONTENT:{chunk}"
                    full_response += chunk
                    assistant_response += chunk

            # 8. å­˜å‚¨æœ¬è½®å¯¹è¯åˆ°å‘é‡åº“
            context_doc_names = []
            if final_nodes:
                context_doc_names = [
                    node.node.metadata.get('file_name', 'æœªçŸ¥')
                    for node in final_nodes
                ]

            # è·å–ä¸Šä¸€è½®å¯¹è¯çš„ turn_id ä½œä¸º parent_turn_id
            parent_turn_id = None
            try:
                if recent_history:
                    parent_turn_id = recent_history[-1].get('turn_id')
            except Exception as e:
                logger.warning(f"è·å–çˆ¶å¯¹è¯IDå¤±è´¥: {e}")

            # ç”Ÿæˆå½“å‰è½®æ¬¡çš„ turn_id
            import uuid
            current_turn_id = str(uuid.uuid4())

            # å­˜å‚¨å¯¹è¯ï¼ˆåŒ…å«å®Œæ•´çš„åŠ©æ‰‹å›ç­”ï¼Œå…¶ä¸­å·²ç»åŒ…å«äº†å®ä½“å’ŒåŠ¨ä½œåˆ†æï¼‰
            conversation_manager.add_conversation_turn(
                session_id=session_id,
                user_query=question,
                assistant_response=assistant_response,
                turn_id=current_turn_id,
                parent_turn_id=parent_turn_id
            )
            
            # 7. æ”¶é›†å¹¶è¾“å‡ºå…¨å±€å…³é”®å­—ï¼ˆå»é‡åé™åˆ¶æ•°é‡ï¼‰
            # 7.1 æå–é—®é¢˜ä¸­çš„å…³é”®è¯
            import jieba
            question_keywords = list(jieba.lcut(question))
            # è¿‡æ»¤æ‰å•å­—å’Œåœç”¨è¯
            question_keywords = [kw for kw in question_keywords if len(kw) > 1]
            logger.info(f"[å¯¹è¯-é—®é¢˜å…³é”®è¯] ä»é—®é¢˜ä¸­æå–: {question_keywords}")
            
            # 7.2 æ”¶é›†æ–‡æ¡£åŒ¹é…çš„å…³é”®å­—
            global_keywords = []
            if final_nodes:
                logger.info(f"[å¯¹è¯-å…³é”®è¯æ”¶é›†] å¼€å§‹æ”¶é›†ï¼Œå…±æœ‰ {len(final_nodes)} ä¸ªèŠ‚ç‚¹")
                for i, node in enumerate(final_nodes):
                    retrieval_sources = node.node.metadata.get('retrieval_sources', [])
                    logger.info(f"[å¯¹è¯-å…³é”®è¯æ”¶é›†] èŠ‚ç‚¹ {i+1}: retrieval_sources={retrieval_sources}")
                    if 'keyword' in retrieval_sources:
                        matched_keywords = node.node.metadata.get('bm25_matched_keywords', [])
                        logger.info(f"[å¯¹è¯-å…³é”®è¯æ”¶é›†] èŠ‚ç‚¹ {i+1} æœ‰å…³é”®å­—: {matched_keywords}")
                        global_keywords.extend(matched_keywords)
                    else:
                        logger.info(f"[å¯¹è¯-å…³é”®è¯æ”¶é›†] èŠ‚ç‚¹ {i+1} æ²¡æœ‰ 'keyword' æ ‡è®°ï¼Œè·³è¿‡")
                logger.info(f"[å¯¹è¯-å…³é”®è¯æ”¶é›†] æ”¶é›†å®Œæˆï¼Œå…±æ”¶é›†åˆ° {len(global_keywords)} ä¸ªå…³é”®å­—: {global_keywords}")
            
            # 7.3 å»é‡é—®é¢˜å…³é”®è¯å’Œæ–‡æ¡£å…³é”®è¯
            # é—®é¢˜å…³é”®è¯å»é‡
            seen_question = set()
            unique_question_keywords = []
            for kw in question_keywords:
                if kw not in seen_question:
                    seen_question.add(kw)
                    unique_question_keywords.append(kw)
            
            # æ–‡æ¡£å…³é”®è¯å»é‡ï¼ˆæ’é™¤å·²åœ¨é—®é¢˜ä¸­çš„ï¼‰
            seen_doc = set(unique_question_keywords)
            unique_doc_keywords = []
            for kw in global_keywords:
                if kw not in seen_doc:
                    seen_doc.add(kw)
                    unique_doc_keywords.append(kw)
            
            # é™åˆ¶æ•°é‡ï¼ˆä½¿ç”¨ MAX_DISPLAY_KEYWORDSï¼‰
            from config import Settings as AppSettings
            max_global_keywords = getattr(AppSettings, 'MAX_DISPLAY_KEYWORDS', 5)
            
            # åˆ†åˆ«é™åˆ¶é—®é¢˜å…³é”®è¯å’Œæ–‡æ¡£å…³é”®è¯
            final_question_keywords = unique_question_keywords[:max_global_keywords]
            remaining_slots = max_global_keywords - len(final_question_keywords)
            final_doc_keywords = unique_doc_keywords[:remaining_slots] if remaining_slots > 0 else []
            
            logger.info(f"[å¯¹è¯-å…³é”®è¯é™åˆ¶] é…ç½®å€¼: MAX_DISPLAY_KEYWORDS={max_global_keywords}")
            logger.info(f"[å¯¹è¯-å…³é”®è¯è¾“å‡º] é—®é¢˜å…³é”®è¯: {final_question_keywords}")
            logger.info(f"[å¯¹è¯-å…³é”®è¯è¾“å‡º] æ–‡æ¡£å…³é”®è¯: {final_doc_keywords}")
            
            # è¾“å‡ºç»“æ„åŒ–å…³é”®å­—ï¼ˆåŒºåˆ†æ¥æºï¼‰
            keywords_data = {
                "question": final_question_keywords,
                "document": final_doc_keywords
            }
            if final_question_keywords or final_doc_keywords:
                yield f"KEYWORDS:{json.dumps(keywords_data, ensure_ascii=False)}"

            # 8. è¾“å‡ºå‚è€ƒæ¥æº
            if use_insert_block and filtered_results:
                yield "CONTENT:\n\n**å‚è€ƒæ¥æºï¼ˆå…¨éƒ¨æ£€ç´¢ç»“æœï¼‰:**"
                full_response += "\n\nå‚è€ƒæ¥æºï¼ˆå…¨éƒ¨æ£€ç´¢ç»“æœï¼‰:"

                for i, node in enumerate(final_nodes):
                    file_name = node.node.metadata.get('file_name', 'æœªçŸ¥')
                    initial_score = node.node.metadata.get('initial_score', 0.0)
                    key = f"{file_name}_{node.score}"

                    filtered_info = filtered_map.get(key)

                    # æå–æ£€ç´¢å…ƒæ•°æ®
                    retrieval_sources = node.node.metadata.get('retrieval_sources', [])
                    vector_score = node.node.metadata.get('vector_score', 0.0)
                    bm25_score = node.node.metadata.get('bm25_score', 0.0)
                    vector_rank = node.node.metadata.get('vector_rank')
                    bm25_rank = node.node.metadata.get('bm25_rank')

                    source_data = {
                        "id": i + 1,
                        "fileName": file_name,
                        "initialScore": f"{initial_score:.4f}",
                        "rerankedScore": f"{node.score:.4f}",
                        "content": node.node.text.strip(),
                        # æ£€ç´¢å…ƒæ•°æ®
                        "retrievalSources": retrieval_sources,
                        "vectorScore": f"{vector_score:.4f}",
                        "bm25Score": f"{bm25_score:.4f}",
                        # InsertBlock ç‰¹æœ‰å­—æ®µ
                        "canAnswer": filtered_info is not None,
                        "reasoning": filtered_info.get('reasoning', '') if filtered_info else '',
                        "keyPassage": filtered_info.get('key_passage', '') if filtered_info else ''
                    }
                    
                    # æ·»åŠ æ’åä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    if vector_rank is not None:
                        source_data['vectorRank'] = vector_rank
                    if bm25_rank is not None:
                        source_data['bm25Rank'] = bm25_rank
                    
                    # æ·»åŠ åŒ¹é…çš„å…³é”®è¯ï¼ˆå¦‚æœæ˜¯å…³é”®è¯æ£€ç´¢ï¼‰
                    if 'keyword' in retrieval_sources:
                        matched_keywords = node.node.metadata.get('bm25_matched_keywords', [])
                        if matched_keywords:
                            source_data['matchedKeywords'] = matched_keywords

                    yield f"SOURCE:{json.dumps(source_data, ensure_ascii=False)}"

                    full_response += (
                        f"\n[{source_data['id']}] æ–‡ä»¶: {source_data['fileName']}, "
                        f"é‡æ’åˆ†: {source_data['rerankedScore']}, "
                        f"å¯å›ç­”: {source_data['canAnswer']}"
                    )

            elif final_nodes:
                yield "CONTENT:\n\n**å‚è€ƒæ¥æº:**"
                full_response += "\n\nå‚è€ƒæ¥æº:"

                for i, node in enumerate(final_nodes):
                    yield f"SOURCE:{json.dumps(node.node.metadata, ensure_ascii=False)}"
                    full_response += (
                        f"\n[{i + 1}] æ–‡ä»¶: {node.node.metadata.get('file_name', 'æœªçŸ¥')}, "
                        f"é‡æ’åˆ†: {node.score}"
                    )

            yield "DONE:"

        except Exception as e:
            error_msg = f"å¤„ç†é”™è¯¯: {str(e)}"
            logger.error(f"å¤šè½®å¯¹è¯å¤„ç†å‡ºé”™: {e}", exc_info=True)
            yield f"ERROR:{error_msg}"

    def _build_prompt_with_history(
        self,
        question: str,
        enable_thinking: bool,
        final_nodes,
        filtered_results=None,
        recent_history=None,
        relevant_history=None,
        history_summary=None,
        hidden_nodes=None
    ):
        """
        æ„é€ å¸¦å†å²å¯¹è¯çš„æç¤ºè¯ï¼ˆä½¿ç”¨çŸ¥è¯†é—®ç­”çš„æç¤ºè¯æ ¼å¼ï¼‰

        Args:
            question: å½“å‰é—®é¢˜
            enable_thinking: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼
            final_nodes: æ£€ç´¢åˆ°çš„èŠ‚ç‚¹
            filtered_results: InsertBlock è¿‡æ»¤ç»“æœ
            recent_history: æœ€è¿‘çš„å¯¹è¯å†å²
            relevant_history: ç›¸å…³çš„å†å²å¯¹è¯
            history_summary: å†å²å¯¹è¯æ‘˜è¦
            hidden_nodes: éšè—çŸ¥è¯†åº“èŠ‚ç‚¹ï¼ˆä¸æ˜¾ç¤ºæ¥æºï¼‰
        """
        # 1. æ„å»ºéšè—çŸ¥è¯†åº“ä¸Šä¸‹æ–‡ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        hidden_context = None
        if hidden_nodes:
            from utils.knowledge_utils.context_builder import build_hidden_kb_context
            hidden_context = build_hidden_kb_context(hidden_nodes)
        
        # 2. æ„å»ºçŸ¥è¯†åº“ä¸Šä¸‹æ–‡ï¼ˆä¸çŸ¥è¯†é—®ç­”ç›¸åŒçš„é€»è¾‘ï¼‰
        knowledge_context = None
        if filtered_results:
            # ä½¿ç”¨ InsertBlock è¿‡æ»¤ç»“æœ
            context_blocks = []
            block_index = 1
            
            for result in filtered_results:
                file_name = result['file_name']
                key_passage = result.get('key_passage', '')
                full_content = result['node'].node.text.strip()
                can_answer = result.get('can_answer', False)
                
                # ä¸¥æ ¼è¿‡æ»¤ï¼šåªæœ‰ can_answer=True ä¸” key_passage ä¸ä¸ºç©ºæ‰æ³¨å…¥ä¸Šä¸‹æ–‡
                if not can_answer:
                    logger.warning(f"[å¯¹è¯-ç²¾å‡†æ£€ç´¢è¿‡æ»¤] è·³è¿‡ä¸å¯å›ç­”çš„èŠ‚ç‚¹: {file_name}")
                    continue
                
                if not key_passage or key_passage.strip() == "":
                    logger.warning(f"[å¯¹è¯-ç²¾å‡†æ£€ç´¢è¿‡æ»¤] è·³è¿‡æ— å…³é”®æ®µè½çš„èŠ‚ç‚¹: {file_name} | can_answer={can_answer}")
                    continue
                
                block = f"ã€ä¸šåŠ¡è§„å®š {block_index}ã€‘æ¥æº: {file_name}\n{full_content}"
                context_blocks.append(block)
                block_index += 1
                logger.info(f"[å¯¹è¯-ç²¾å‡†æ£€ç´¢é€šè¿‡] èŠ‚ç‚¹å·²æ³¨å…¥ä¸Šä¸‹æ–‡: {file_name} | å…³é”®æ®µè½é•¿åº¦: {len(key_passage)}")
                
            knowledge_context = "\n\n".join(context_blocks) if context_blocks else None

        elif final_nodes:
            # ä½¿ç”¨æ™®é€šæ£€ç´¢ç»“æœ
            context_blocks = []
            for i, node in enumerate(final_nodes):
                file_name = node.node.metadata.get('file_name', 'æœªçŸ¥æ–‡ä»¶')
                content = node.node.get_content().strip()
                block = f"ã€ä¸šåŠ¡è§„å®š {i + 1}ã€‘æ¥æº: {file_name}\n{content}"
                context_blocks.append(block)
            knowledge_context = "\n\n".join(context_blocks)

        has_rag = bool(knowledge_context)
        
        # å¦‚æœæœ‰å­é—®é¢˜ç­”æ¡ˆåˆæˆï¼Œæ·»åŠ åˆ°çŸ¥è¯†åº“ä¸Šä¸‹æ–‡ä¸­ï¼ˆä½¿ç”¨ç®€æ´æ ¼å¼ï¼‰
        if has_rag and self._last_synthesized_answer:
            synthesis_block = (
                f"\n\nã€å­é—®é¢˜ç»¼åˆåˆ†æã€‘\n"
                f"{self._last_synthesized_answer}\n\n"
                f"æ³¨æ„: ä»¥ä¸Šæ˜¯å¯¹å¤šä¸ªå­é—®é¢˜ç­”æ¡ˆçš„ç»¼åˆæ•´ç†ï¼Œè¯·ç»“åˆå…·ä½“ä¸šåŠ¡è§„å®šç»™å‡ºæœ€ç»ˆå›ç­”ã€‚"
            )
            knowledge_context += synthesis_block
            logger.info(f"[å¤šè½®æç¤ºè¯æ„å»º] å·²å°†åˆæˆç­”æ¡ˆæ³¨å…¥ä¸Šä¸‹æ–‡ | é•¿åº¦: {len(self._last_synthesized_answer)}")

        # æ„å»ºå†å²å¯¹è¯ä¸Šä¸‹æ–‡
        history_context = None
        if history_summary or recent_history or relevant_history:
            history_parts = []

            # æ·»åŠ æ‘˜è¦
            if history_summary:
                summary_prefix = get_conversation_summary_context_prefix()
                history_parts.append(f"{summary_prefix}{history_summary}")

            # æ·»åŠ æœ€è¿‘çš„å¯¹è¯
            if recent_history:
                recent_prefix = get_conversation_context_prefix_recent_history()
                recent_turns_text = "\n\n".join([
                    f"ç”¨æˆ·: {turn['user_query']}\nåŠ©æ‰‹: {turn['assistant_response']}"
                    for turn in recent_history
                ])
                history_parts.append(f"{recent_prefix}{recent_turns_text}")

            # æ·»åŠ ç›¸å…³å†å²å¯¹è¯
            if relevant_history:
                relevant_prefix = get_conversation_context_prefix_relevant_history()
                relevant_turns_text = "\n\n".join([
                    f"ç”¨æˆ·: {turn['user_query']}\nåŠ©æ‰‹: {turn['assistant_response']}"
                    for turn in relevant_history
                ])
                history_parts.append(f"{relevant_prefix}{relevant_turns_text}")

            history_context = "\n\n".join(history_parts)

        # ä½¿ç”¨çŸ¥è¯†é—®ç­”çš„æç¤ºè¯é€»è¾‘
        if has_rag or hidden_context:
            # è·å–å‰ç¼€
            assistant_prefix = get_knowledge_assistant_context_prefix()

            # ç»„åˆä¸Šä¸‹æ–‡ï¼šéšè—çŸ¥è¯†åº“ + å†å²å¯¹è¯ + ä¸šåŠ¡è§„å®š
            context_parts = []
            
            # 1. éšè—çŸ¥è¯†åº“ï¼ˆæœ€ä¼˜å…ˆï¼‰
            if hidden_context:
                context_parts.append(hidden_context)
            
            # 2. å†å²å¯¹è¯
            if history_context:
                context_parts.append(history_context)
            
            # 3. ä¸šåŠ¡è§„å®š
            if knowledge_context:
                context_parts.append(assistant_prefix + knowledge_context)
            elif hidden_context and not knowledge_context:
                # åªæœ‰éšè—çŸ¥è¯†åº“ï¼Œæ²¡æœ‰æ™®é€šçŸ¥è¯†åº“
                context_parts.append(assistant_prefix + "ï¼ˆå·²æ³¨å…¥å†…éƒ¨å‚è€ƒèµ„æ–™ï¼‰")

            assistant_context = "\n\n---\n\n".join(context_parts)

            # æ ¹æ®æ€è€ƒæ¨¡å¼é€‰æ‹©ä¸åŒçš„ system å’Œ user prompt
            if enable_thinking:
                system_prompt = get_knowledge_system_rag_advanced()
                user_template = get_knowledge_user_rag_advanced()
            else:
                system_prompt = get_knowledge_system_rag_simple()
                user_template = get_knowledge_user_rag_simple()

            # user_template æ˜¯åˆ—è¡¨ï¼Œéœ€è¦ join åå† format
            user_prompt_str = "\n".join(user_template) if isinstance(user_template, list) else user_template
            # å¦‚æœå…³é—­æ€è€ƒæ¨¡å¼ï¼Œè‡ªåŠ¨åœ¨é—®é¢˜åè¿½åŠ  /no_think æŒ‡ä»¤ï¼ˆé˜¿é‡Œäº‘æ–‡æ¡£å»ºè®®ï¼‰
            actual_question = f"{question}/no_think" if not enable_thinking else question
            user_prompt = user_prompt_str.format(context=assistant_context, question=actual_question)

        else:
            # æ²¡æœ‰æ£€ç´¢åˆ°ç›¸å…³å†…å®¹ï¼Œåªæœ‰å†å²å¯¹è¯
            assistant_context = history_context

            if enable_thinking:
                system_prompt = get_knowledge_system_no_rag_think()
                user_template = get_knowledge_user_no_rag_think()
            else:
                system_prompt = get_knowledge_system_no_rag_simple()
                user_template = get_knowledge_user_no_rag_simple()

            # user_template å¯èƒ½æ˜¯åˆ—è¡¨æˆ–å­—ç¬¦ä¸²
            user_prompt_str = "\n".join(user_template) if isinstance(user_template, list) else user_template
            # å¦‚æœå…³é—­æ€è€ƒæ¨¡å¼ï¼Œè‡ªåŠ¨åœ¨é—®é¢˜åè¿½åŠ  /no_think æŒ‡ä»¤ï¼ˆé˜¿é‡Œäº‘æ–‡æ¡£å»ºè®®ï¼‰
            actual_question = f"{question}/no_think" if not enable_thinking else question
            user_prompt = user_prompt_str.format(question=actual_question)

        # system_prompt å¯èƒ½æ˜¯åˆ—è¡¨ï¼Œéœ€è¦è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        if isinstance(system_prompt, list):
            system_prompt = "\n".join(system_prompt)

        # æ„å»º fallback_promptï¼ˆç”¨äºä¸æ”¯æŒ chat æ¨¡å¼çš„æƒ…å†µï¼‰
        fallback_parts = [system_prompt]
        if assistant_context:
            fallback_parts.append(assistant_context)
        fallback_parts.append(user_prompt)

        return {
            "system_prompt": system_prompt,
            "user_prompt": user_prompt,
            "assistant_context": assistant_context,
            "fallback_prompt": "\n\n".join(fallback_parts)
        }
    
    def _smart_retrieve_and_rerank(self, question: str, rerank_top_n: int, conversation_history: Optional[List[Dict]] = None):
        """
        æ™ºèƒ½è·¯ç”±æ£€ç´¢ï¼šå…ˆæ„å›¾åˆ†ç±»é€‰æ‹©çŸ¥è¯†åº“ï¼Œå†å¯é€‰å­é—®é¢˜åˆ†è§£
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            rerank_top_n: é‡æ’åºåè¿”å›çš„æ–‡æ¡£æ•°é‡
            conversation_history: å¯¹è¯å†å²ï¼ˆç”¨äºå­é—®é¢˜åˆ†è§£ï¼‰
            
        Returns:
            é‡æ’åºåçš„èŠ‚ç‚¹åˆ—è¡¨
        """
        # 1. æ„å›¾åˆ†ç±»ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        strategy = "general"  # é»˜è®¤ç­–ç•¥ï¼šåªç”¨é€šç”¨åº“
        
        if self.intent_classifier:
            try:
                strategy = self.intent_classifier.classify(question)
                logger.info(f"[æ™ºèƒ½è·¯ç”±] æ„å›¾åˆ†ç±»ç»“æœ: {strategy}")
            except Exception as e:
                logger.warning(f"[æ™ºèƒ½è·¯ç”±] æ„å›¾åˆ†ç±»å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥: general")
                strategy = "general"
        else:
            logger.info("[æ™ºèƒ½è·¯ç”±] æ„å›¾åˆ†ç±»å™¨æœªå¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥: general")
        
        # 2. æ ¹æ®ç­–ç•¥é€‰æ‹©æ£€ç´¢å™¨
        if strategy == "both" and self.multi_kb_retriever:
            # åŒåº“æ£€ç´¢
            logger.info("[æ™ºèƒ½è·¯ç”±] ä½¿ç”¨åŒåº“æ£€ç´¢ï¼ˆå…ç­¾åº“ + é€šç”¨åº“ï¼‰")
            selected_retriever = self.multi_kb_retriever
        elif strategy == "visa_free" and self.visa_free_retriever:
            # åªç”¨å…ç­¾åº“
            logger.info("[æ™ºèƒ½è·¯ç”±] ä½¿ç”¨å…ç­¾çŸ¥è¯†åº“")
            selected_retriever = self.visa_free_retriever
        else:
            # åªç”¨é€šç”¨åº“ï¼ˆé»˜è®¤ï¼‰
            logger.info("[æ™ºèƒ½è·¯ç”±] ä½¿ç”¨é€šç”¨çŸ¥è¯†åº“")
            selected_retriever = self.retriever
        
        # 3. å°è¯•å­é—®é¢˜åˆ†è§£ï¼ˆå¦‚æœå¯ç”¨ï¼‰ï¼Œä½¿ç”¨è·¯ç”±åçš„æ£€ç´¢å™¨
        if self.sub_question_decomposer and self.sub_question_decomposer.enabled:
            logger.info(f"[æ£€ç´¢ç­–ç•¥] å°è¯•ä½¿ç”¨å­é—®é¢˜åˆ†è§£æ£€ç´¢ï¼ˆå•è½®ï¼‰ | ç›®æ ‡åº“: {strategy}")
            try:
                nodes, metadata = self.sub_question_decomposer.retrieve_with_decomposition(
                    query=question,
                    rerank_top_n=rerank_top_n,
                    conversation_history=conversation_history,
                    retriever=selected_retriever  # ä¼ å…¥è·¯ç”±åçš„æ£€ç´¢å™¨
                )
                
                # è®°å½•åˆ†è§£å…ƒæ•°æ®
                if metadata.get('decomposed'):
                    logger.info(
                        f"[å­é—®é¢˜æ£€ç´¢] åˆ†è§£æ£€ç´¢å®Œæˆ | "
                        f"å­é—®é¢˜æ•°: {len(metadata['sub_questions'])} | "
                        f"è¿”å›èŠ‚ç‚¹æ•°: {len(nodes)} | "
                        f"ä½¿ç”¨åº“: {strategy}"
                    )
                    
                    # ä¿å­˜å­é—®é¢˜ç­”æ¡ˆï¼ˆç”¨äºæ³¨å…¥ä¸Šä¸‹æ–‡å’Œè¿”å›å‰ç«¯ï¼‰
                    if metadata.get('sub_answers') and len(metadata['sub_answers']) > 0:
                        # å­˜å‚¨å­é—®é¢˜ç­”æ¡ˆï¼Œä¾› _build_prompt ä½¿ç”¨
                        self._last_sub_answers = metadata['sub_answers']
                        logger.info(f"[å­é—®é¢˜ç­”æ¡ˆ] å·²ä¿å­˜ {len(metadata['sub_answers'])} ä¸ªå­é—®é¢˜ç­”æ¡ˆï¼Œå°†æ³¨å…¥ä¸Šä¸‹æ–‡")
                        
                        # å¯é€‰ï¼šç”Ÿæˆå­é—®é¢˜ç­”æ¡ˆåˆæˆ
                        try:
                            synthesized_answer = self.sub_question_decomposer.synthesize_answer(
                                original_query=question,
                                sub_answers=metadata['sub_answers']
                            )
                            if synthesized_answer:
                                # å°†åˆæˆç­”æ¡ˆæ·»åŠ åˆ°metadataï¼Œä¾›åç»­ä½¿ç”¨
                                metadata['synthesized_answer'] = synthesized_answer
                                # å­˜å‚¨ä¸ºå®ä¾‹å˜é‡ï¼Œä¾›_build_promptä½¿ç”¨
                                self._last_synthesized_answer = synthesized_answer
                                logger.info(f"[ç­”æ¡ˆåˆæˆ] å·²ç”Ÿæˆåˆæˆç­”æ¡ˆ | é•¿åº¦: {len(synthesized_answer)}")
                        except Exception as synth_e:
                            logger.warning(f"[ç­”æ¡ˆåˆæˆ] åˆæˆå¤±è´¥: {synth_e}")
                    
                    # è¿”å›èŠ‚ç‚¹å’Œå…ƒæ•°æ®
                    return nodes, metadata
                else:
                    logger.info("[å­é—®é¢˜æ£€ç´¢] æœªåˆ†è§£ï¼Œç»§ç»­æ ‡å‡†æ£€ç´¢æµç¨‹")
                    # æ¸…ç©ºå­é—®é¢˜ç­”æ¡ˆï¼Œé¿å…ä½¿ç”¨æ—§æ•°æ®
                    self._last_sub_answers = None
                    # ç»§ç»­æ‰§è¡Œæ ‡å‡†æ£€ç´¢
                    
            except Exception as e:
                logger.error(f"[å­é—®é¢˜æ£€ç´¢] åˆ†è§£æ£€ç´¢å¤±è´¥: {e}", exc_info=True)
                logger.info("[å­é—®é¢˜æ£€ç´¢] å›é€€åˆ°æ ‡å‡†æ£€ç´¢æµç¨‹")
                # æ¸…ç©ºå­é—®é¢˜ç­”æ¡ˆï¼Œé¿å…ä½¿ç”¨æ—§æ•°æ®
                self._last_sub_answers = None
                # ç»§ç»­æ‰§è¡Œæ ‡å‡†æ£€ç´¢
        
        # 4. æ ‡å‡†æ£€ç´¢å’Œé‡æ’åº
        return self._retrieve_and_rerank_with_retriever(
            question, 
            rerank_top_n, 
            selected_retriever
        )
    
    def _retrieve_and_rerank_with_retriever(
        self, 
        question: str, 
        rerank_top_n: int,
        retriever
    ):
        """
        ä½¿ç”¨æŒ‡å®šæ£€ç´¢å™¨è¿›è¡Œæ£€ç´¢å’Œé‡æ’åº
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            rerank_top_n: é‡æ’åºåè¿”å›çš„æ–‡æ¡£æ•°é‡
            retriever: æ£€ç´¢å™¨å®ä¾‹
            
        Returns:
            é‡æ’åºåçš„èŠ‚ç‚¹åˆ—è¡¨
        """
        # åˆ›å»º QueryBundleï¼ˆé‡æ’åºéœ€è¦ï¼‰
        query_bundle = QueryBundle(query_str=question)
        
        # åˆ¤æ–­æ˜¯å¦ä¸º MultiKBRetriever
        from core.multi_kb_retriever import MultiKBRetriever
        if isinstance(retriever, MultiKBRetriever):
            # MultiKBRetriever ä½¿ç”¨ retrieve_from_both æ–¹æ³•ï¼Œç›´æ¥ä¼ å…¥ query å­—ç¬¦ä¸²
            retrieved_nodes = retriever.retrieve_from_both(question)
        else:
            # å…¶ä»–æ£€ç´¢å™¨ä½¿ç”¨æ ‡å‡†çš„ retrieve æ–¹æ³•ï¼Œéœ€è¦ QueryBundle
            retrieved_nodes = retriever.retrieve(query_bundle)
        
        logger.info(f"æ£€ç´¢åˆ° {len(retrieved_nodes)} ä¸ªåˆæ­¥ç»“æœ")
        
        if not retrieved_nodes:
            logger.warning("æœªæ£€ç´¢åˆ°ä»»ä½•ç›¸å…³æ–‡æ¡£")
            return []
        
        # æ·»åŠ æ—¥å¿—ï¼šæ£€ç´¢åçš„åˆ†æ•°
        if retrieved_nodes:
            retrieval_scores = [n.score for n in retrieved_nodes[:5]]
            logger.info(f"æ£€ç´¢é˜¶æ®µTop5å¾—åˆ†: {[f'{s:.4f}' for s in retrieval_scores]}")
        
        # ä¿å­˜åŸå§‹èŠ‚ç‚¹çš„æ£€ç´¢å…ƒæ•°æ®ï¼ˆé‡æ’åºå¯èƒ½ä¼šä¸¢å¤±ï¼‰
        original_metadata = {}
        for node in retrieved_nodes:
            node_id = node.node.node_id
            original_metadata[node_id] = {
                'retrieval_sources': node.node.metadata.get('retrieval_sources', []),
                'vector_score': node.node.metadata.get('vector_score', 0.0),
                'bm25_score': node.node.metadata.get('bm25_score', 0.0),
                'bm25_matched_keywords': node.node.metadata.get('bm25_matched_keywords', []),
                'bm25_query_keywords': node.node.metadata.get('bm25_query_keywords', []),
                'vector_rank': node.node.metadata.get('vector_rank'),
                'bm25_rank': node.node.metadata.get('bm25_rank'),
                'initial_score': node.node.metadata.get('initial_score', node.score)
            }
        
        # é‡æ’åº
        reranked_nodes = self.reranker.postprocess_nodes(
            retrieved_nodes,
            query_bundle=query_bundle
        )
        
        logger.info(f"é‡æ’åºåä¿ç•™ {len(reranked_nodes)} ä¸ªç»“æœ")
        
        # æ¢å¤åŸå§‹èŠ‚ç‚¹çš„æ£€ç´¢å…ƒæ•°æ®
        for node in reranked_nodes:
            node_id = node.node.node_id
            if node_id in original_metadata:
                metadata = original_metadata[node_id]
                node.node.metadata.update(metadata)
        
        logger.info(f"å·²æ¢å¤ {len([n for n in reranked_nodes if n.node.metadata.get('retrieval_sources')])} ä¸ªèŠ‚ç‚¹çš„æ£€ç´¢å…ƒæ•°æ®")
        
        # æ·»åŠ æ—¥å¿—ï¼šé‡æ’åºåçš„åˆ†æ•°
        if reranked_nodes:
            rerank_scores = [n.score for n in reranked_nodes[:5]]
            logger.info(f"é‡æ’åºé˜¶æ®µTop5å¾—åˆ†: {[f'{s:.4f}' for s in rerank_scores]}")
        
        # æ–¹æ¡ˆ1+3ç»„åˆï¼šæŒ‰å¾—åˆ†æ’åºåä¸¥æ ¼æˆªæ–­åˆ°å‰ç«¯è¦æ±‚çš„æ•°é‡
        # ç¡®ä¿æŒ‰åˆ†æ•°ä»é«˜åˆ°ä½æ’åº
        reranked_nodes.sort(key=lambda x: x.score, reverse=True)
        
        # ä¸¥æ ¼æŒ‰ç…§å‰ç«¯ä¼ å…¥çš„ rerank_top_n å‚æ•°æˆªæ–­
        final_nodes = reranked_nodes[:rerank_top_n]
        
        if final_nodes:
            logger.info(
                f"æœ€ç»ˆè¿”å› {len(final_nodes)} ä¸ªæ–‡æ¡£ï¼ˆä¸¥æ ¼æŒ‰å‰ç«¯å‚æ•° top_k={rerank_top_n} æˆªæ–­ï¼‰ | "
                f"æœ€é«˜åˆ†: {final_nodes[0].score:.4f} | "
                f"æœ€ä½åˆ†: {final_nodes[-1].score:.4f}"
            )
        
        return final_nodes

    def debug_inspect_scores(
        self,
        question: str,
        *,
        retriever=None,
        match_substring: Optional[str] = None,
        match_node_id: Optional[str] = None,
        max_candidates: int = 50,
        include_full_text: bool = False,
        run_reranker: bool = True
    ) -> Dict[str, Any]:
        """
        è°ƒè¯•è¾…åŠ©ï¼šæŸ¥çœ‹æ£€ç´¢/é‡æ’åºé˜¶æ®µçš„èŠ‚ç‚¹å¾—åˆ†ã€‚
        ä»…åœ¨æ˜¾å¼è°ƒç”¨æ—¶æ‰§è¡Œï¼Œä¸å½±å“ç°æœ‰æµç¨‹ã€‚
        """
        if not question:
            raise ValueError("question ä¸èƒ½ä¸ºç©º")

        active_retriever = retriever or self.retriever
        if active_retriever is None:
            raise RuntimeError("æœªé…ç½®æ£€ç´¢å™¨ï¼Œæ— æ³•æ‰§è¡Œè°ƒè¯•")

        if run_reranker and self.reranker is None:
            raise RuntimeError("æœªé…ç½®é‡æ’å™¨ï¼Œæ— æ³•æ‰§è¡Œè°ƒè¯•")

        query_bundle = QueryBundle(query_str=question)

        def _execute_retriever() -> List[Any]:
            """å…¼å®¹å¤šçŸ¥è¯†åº“æ£€ç´¢å™¨çš„è°ƒç”¨æ–¹å¼"""
            try:
                from core.multi_kb_retriever import MultiKBRetriever
            except ImportError:
                MultiKBRetriever = None  # type: ignore

            if MultiKBRetriever and isinstance(active_retriever, MultiKBRetriever):
                return active_retriever.retrieve_from_all_three(question)

            return active_retriever.retrieve(query_bundle)

        def _serialize_nodes(nodes: List[Any], stage: str) -> List[Dict[str, Any]]:
            serialized = []
            for idx, node_score in enumerate(nodes[:max_candidates], start=1):
                node = node_score.node
                metadata = node.metadata or {}
                text = node.get_content()
                preview = text[:120].replace("\n", " ").strip()
                vector_rank = metadata.get("vector_rank")
                bm25_rank = metadata.get("bm25_rank")
                sources = metadata.get("retrieval_sources") or []
                source_label = "/".join(sources) if sources else "unknown"

                entry = {
                    "stage": stage,
                    "rank": idx,
                    "node_id": node.node_id,
                    "score": float(node_score.score or 0.0),
                    "vector_score": float(metadata.get("vector_score", 0.0)),
                    "bm25_score": float(metadata.get("bm25_score", 0.0)),
                    "vector_rank": vector_rank,
                    "bm25_rank": bm25_rank,
                    "sources": sources,
                    "source_label": source_label,
                    "file_name": metadata.get("file_name"),
                    "file_path": metadata.get("file_path"),
                    "text_preview": preview,
                    "metadata": metadata
                }

                if include_full_text:
                    entry["text"] = text

                serialized.append(entry)

            return serialized

        def _is_match(entry: Dict[str, Any]) -> bool:
            if not match_substring and not match_node_id:
                return False

            matched = True

            if match_substring:
                needle = match_substring.lower()
                haystack = [
                    (entry.get("text_preview") or "").lower(),
                    (entry.get("file_name") or "").lower(),
                ]
                if include_full_text:
                    haystack.append((entry.get("text") or "").lower())
                matched = any(needle in segment for segment in haystack)

            if matched and match_node_id:
                matched = match_node_id in (entry.get("node_id") or "")

            return matched

        retrieved_nodes = _execute_retriever() or []
        retrieval_serialized = _serialize_nodes(retrieved_nodes, stage="retrieval")

        rerank_serialized: List[Dict[str, Any]] = []
        if run_reranker and retrieved_nodes:
            reranked = self.reranker.postprocess_nodes(
                retrieved_nodes,
                query_bundle=query_bundle
            )
            reranked.sort(key=lambda x: x.score, reverse=True)
            rerank_serialized = _serialize_nodes(reranked, stage="rerank")

        matched_entries = []
        for entry in retrieval_serialized + rerank_serialized:
            if _is_match(entry):
                matched_entries.append(entry)

        return {
            "question": question,
            "retriever_type": type(active_retriever).__name__,
            "retrieval": retrieval_serialized,
            "rerank": rerank_serialized,
            "matches": matched_entries,
            "match_conditions": {
                "substring": match_substring,
                "node_id": match_node_id
            }
        }
