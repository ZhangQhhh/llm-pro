# -*- coding: utf-8 -*-
"""
çŸ¥è¯†é—®ç­”å¤„ç†å™¨
å¤„ç†çŸ¥è¯†åº“é—®ç­”çš„ä¸šåŠ¡é€»è¾‘
"""
import json
import os
from datetime import datetime
from typing import Generator, Dict, Any, Optional, List
from llama_index.core import QueryBundle
from llama_index.core.schema import NodeWithScore
from config import Settings
from utils import logger, clean_for_sse_text
from pathlib import Path
from prompts import (
    get_knowledge_assistant_context_prefix,
    get_knowledge_system_rag_simple,
    get_knowledge_system_rag_advanced,
    get_knowledge_system_no_rag_think,
    get_knowledge_system_no_rag_simple,
    get_knowledge_user_rag_simple,
    get_knowledge_user_rag_advanced,
    get_knowledge_user_no_rag_think,
    get_knowledge_user_no_rag_simple,
    get_conversation_system_rag_with_history,
    get_conversation_system_general_with_history,
    get_conversation_context_prefix_relevant_history,
    get_conversation_context_prefix_recent_history,
    get_conversation_context_prefix_regulations,
    get_conversation_user_rag_query,
    get_conversation_user_general_query,
    get_conversation_summary_system,
    get_conversation_summary_user,
    get_conversation_summary_context_prefix
)
# å¯¼å…¥æ–°çš„å·¥å…·å‡½æ•°
from utils.knowledge_utils import (
    build_knowledge_prompt,
    format_sources,
    format_filtered_sources,
    build_reference_entries,
    log_prompt_to_file,
    log_reference_details,
    save_qa_log,
    parse_thinking_stream,
    parse_normal_stream
)


class KnowledgeHandler:
    """çŸ¥è¯†é—®ç­”å¤„ç†å™¨"""

    def __init__(
        self, 
        retriever, 
        reranker, 
        llm_wrapper, 
        llm_service=None,
        # å…ç­¾çŸ¥è¯†åº“ç›¸å…³ç»„ä»¶ï¼ˆå¯é€‰ï¼‰
        visa_free_retriever=None,
        # èˆªå¸çŸ¥è¯†åº“ç›¸å…³ç»„ä»¶ï¼ˆå¯é€‰ï¼‰
        airline_retriever=None,
        # å¤šåº“æ£€ç´¢å™¨å’Œæ„å›¾åˆ†ç±»å™¨
        multi_kb_retriever=None,
        intent_classifier=None,
        # å­é—®é¢˜åˆ†è§£å™¨ï¼ˆå¯é€‰ï¼‰
        sub_question_decomposer=None,
        # éšè—çŸ¥è¯†åº“æ£€ç´¢å™¨ï¼ˆå¯é€‰ï¼‰
        hidden_kb_retriever=None
    ):
        # é€šç”¨çŸ¥è¯†åº“ç»„ä»¶
        self.retriever = retriever
        self.reranker = reranker
        self.llm_wrapper = llm_wrapper
        self.llm_service = llm_service
        self.insert_block_filter = None
        
        # å…ç­¾çŸ¥è¯†åº“ç»„ä»¶
        self.visa_free_retriever = visa_free_retriever
        # èˆªå¸çŸ¥è¯†åº“ç»„ä»¶
        self.airline_retriever = airline_retriever
        # å¤šåº“æ£€ç´¢å™¨å’Œæ„å›¾åˆ†ç±»å™¨
        self.multi_kb_retriever = multi_kb_retriever
        self.intent_classifier = intent_classifier
        # å­é—®é¢˜åˆ†è§£å™¨
        self.sub_question_decomposer = sub_question_decomposer
        # éšè—çŸ¥è¯†åº“æ£€ç´¢å™¨
        self.hidden_kb_retriever = hidden_kb_retriever
        
        # å­é—®é¢˜ç­”æ¡ˆåˆæˆï¼ˆç”¨äºä¼ é€’åˆ°æç¤ºè¯ï¼‰
        self._last_synthesized_answer = None

        # å¦‚æœæä¾›äº† llm_serviceï¼Œåˆå§‹åŒ– InsertBlock è¿‡æ»¤å™¨
        if llm_service:
            from core.node_filter import InsertBlockFilter
            self.insert_block_filter = InsertBlockFilter(llm_service)
        
        # çŸ¥è¯†åº“åŠŸèƒ½çŠ¶æ€ï¼ˆä»…åœ¨è°ƒè¯•æ—¶å¯ç”¨ï¼‰
        # enabled_features = []
        # if self.multi_kb_retriever and self.intent_classifier:
        #     enabled_features.append("å¤šåº“æ£€ç´¢+æ„å›¾åˆ†ç±»")
        # if enabled_features:
        #     logger.info(f"âœ“ çŸ¥è¯†åº“åŠŸèƒ½å·²å¯ç”¨: {', '.join(enabled_features)}")

    def process(
        self,
        question: str,
        enable_thinking: bool,
        rerank_top_n: int,
        llm,
        client_ip: str = "unknown",
        use_insert_block: bool = False,
        insert_block_llm_id: Optional[str] = None
    ) -> Generator[str, None, None]:
        """
        å¤„ç†çŸ¥è¯†é—®ç­”

        Args:
            question: é—®é¢˜å†…å®¹
            enable_thinking: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼
            rerank_top_n: é‡æ’åºåè¿”å›çš„æ–‡æ¡£æ•°é‡
            llm: LLM å®ä¾‹
            client_ip: å®¢æˆ·ç«¯ IP
            use_insert_block: æ˜¯å¦ä½¿ç”¨ InsertBlock è¿‡æ»¤æ¨¡å¼
            insert_block_llm_id: InsertBlock ä½¿ç”¨çš„ LLM ID

        Yields:
            SSE æ ¼å¼çš„å“åº”æµ
        """
        full_response = ""
        
        # æ¸…ç©ºä¸Šä¸€æ¬¡çš„å­é—®é¢˜ç­”æ¡ˆå’Œåˆæˆç­”æ¡ˆï¼Œé˜²æ­¢ä¸²é¢˜
        self._last_sub_answers = None
        self._last_synthesized_answer = None

        try:
            # å¤„ç†çŸ¥è¯†é—®ç­”
            pass

            # 1. æ™ºèƒ½è·¯ç”±æ£€ç´¢ï¼ˆæ ¹æ®æ„å›¾é€‰æ‹©çŸ¥è¯†åº“ï¼‰
            # å¦‚æœå‰ç«¯è®¾ç½®å‚è€ƒæ•°é‡ä¸º 0ï¼Œè·³è¿‡æ£€ç´¢
            if rerank_top_n == 0:
                # è·³è¿‡æ£€ç´¢
                pass
                final_nodes = []
                result = None
                hidden_nodes = []
            else:
                yield ('CONTENT', "æ­£åœ¨è¿›è¡Œæ··åˆæ£€ç´¢...\n")
                full_response += "æ­£åœ¨è¿›è¡Œæ··åˆæ£€ç´¢...\n"
                
                # è°ƒç”¨æ£€ç´¢ï¼Œè·å–èŠ‚ç‚¹å’Œå…ƒæ•°æ®
                result = self._smart_retrieve_and_rerank(question, rerank_top_n)
                
                # 1.5 éšè—çŸ¥è¯†åº“æ£€ç´¢ï¼ˆå¹¶è¡Œè¿›è¡Œï¼Œä¸å½±å“ä¸»æµç¨‹ï¼‰
                hidden_nodes = []
                if self.hidden_kb_retriever and self.hidden_kb_retriever.enabled:
                    try:
                        logger.info("[hidden knowledgeåº“] å¼€å§‹å¹¶è¡Œæ£€ç´¢...")
                        hidden_nodes = self.hidden_kb_retriever.retrieve(question)
                        if hidden_nodes:
                            logger.info(f"[hidden knowledgeåº“] æ£€ç´¢æˆåŠŸ | è¿”å› {len(hidden_nodes)} æ¡")
                            
                            # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦å°†éšè—èŠ‚ç‚¹åˆå¹¶åˆ°æ˜¾ç¤ºåˆ—è¡¨
                            if Settings.HIDDEN_KB_INJECT_MODE == "visible":
                                logger.info("[hidden knowledgeåº“] visible æ¨¡å¼ï¼šå°†éšè—èŠ‚ç‚¹åˆå¹¶åˆ°å‚è€ƒæ¥æº")
                                # æ³¨æ„ï¼šä¸ä¿®æ”¹ final_nodesï¼Œåœ¨æ˜¾ç¤ºæ—¶å¤„ç†
                            else:
                                logger.info("[hidden knowledgeåº“] silent æ¨¡å¼ï¼šéšè—èŠ‚ç‚¹ä¸æ˜¾ç¤ºæ¥æº")
                        else:
                            logger.info("[hidden knowledgeåº“] æœªæ£€ç´¢åˆ°ç›¸å…³å†…å®¹")
                    except Exception as e:
                        logger.warning(f"[hidden knowledgeåº“] æ£€ç´¢å¤±è´¥ï¼Œç»§ç»­ä¸»æµç¨‹: {e}")
                        hidden_nodes = []
            
            # æ£€æŸ¥æ˜¯å¦è¿”å›äº†å…ƒæ•°æ®ï¼ˆå­é—®é¢˜åˆ†è§£ï¼‰
            if result and isinstance(result, tuple) and len(result) == 2:
                final_nodes, retrieval_metadata = result
                
                # å¦‚æœæœ‰å­é—®é¢˜ï¼Œè¾“å‡ºåˆ°å‰ç«¯
                if retrieval_metadata.get('decomposed') and retrieval_metadata.get('sub_questions'):
                    sub_questions = retrieval_metadata['sub_questions']
                    sub_answers = retrieval_metadata.get('sub_answers', [])
                    
                    # æ„å»ºå®Œæ•´çš„å­é—®é¢˜æ•°æ®
                    sub_questions_data = {
                        'sub_questions': sub_questions,
                        'count': len(sub_questions),
                        'sub_answers': sub_answers  # åŒ…å«æ¯ä¸ªå­é—®é¢˜çš„ç­”æ¡ˆæ‘˜è¦
                    }
                    
                    yield ('SUB_QUESTIONS', sub_questions_data)
                    logger.info(
                        f"[å‰ç«¯è¾“å‡º] å·²å‘é€å­é—®é¢˜åˆ°å‰ç«¯ | "
                        f"å­é—®é¢˜æ•°: {len(sub_questions)} | "
                        f"ç­”æ¡ˆæ•°: {len(sub_answers)}"
                    )
            else:
                # å…¼å®¹æ—§ç‰ˆæœ¬ï¼ˆåªè¿”å›èŠ‚ç‚¹ï¼‰
                final_nodes = result
                retrieval_metadata = None


            # 2. å¦‚æœå¯ç”¨ InsertBlock æ¨¡å¼ï¼Œè¿›è¡Œæ™ºèƒ½è¿‡æ»¤
            filtered_results = None
            filtered_map = None
            nodes_for_prompt = final_nodes  # é»˜è®¤ä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ

            if use_insert_block and final_nodes and self.insert_block_filter:
                # å‘é€å¼€å§‹æ¶ˆæ¯
                start_msg = f"æ­£åœ¨ä½¿ç”¨ç²¾å‡†æ£€ç´¢åˆ†æ {len(final_nodes)} ä¸ªæ–‡æ¡£...\næç¤ºï¼šç³»ç»Ÿæ­£åœ¨é€ä¸ªåˆ¤æ–­æ¯ä¸ªæ–‡æ¡£æ˜¯å¦èƒ½å›ç­”æ‚¨çš„é—®é¢˜ï¼Œè¯·ç¨å€™\n"
                yield ('CONTENT', start_msg)
                full_response += start_msg
                
                # ä½¿ç”¨é˜Ÿåˆ—æ”¶é›†è¿›åº¦
                import queue
                import threading
                progress_queue = queue.Queue()
                filter_done = threading.Event()
                
                # å®šä¹‰è¿›åº¦å›è°ƒå‡½æ•°ï¼ˆå°†è¿›åº¦æ”¾å…¥é˜Ÿåˆ—ï¼‰
                def progress_callback(processed, total):
                    progress_queue.put((processed, total))
                
                # åœ¨åå°çº¿ç¨‹æ‰§è¡Œè¿‡æ»¤
                def run_filter():
                    try:
                        result = self.insert_block_filter.filter_nodes(
                            question=question,
                            nodes=final_nodes,
                            llm_id=insert_block_llm_id,
                            progress_callback=progress_callback
                        )
                        progress_queue.put(('DONE', result))
                    except Exception as e:
                        progress_queue.put(('ERROR', e))
                    finally:
                        filter_done.set()
                
                filter_thread = threading.Thread(target=run_filter, daemon=True)
                filter_thread.start()
                
                # ä¸»çº¿ç¨‹å®šæœŸæ£€æŸ¥è¿›åº¦å¹¶å‘é€
                last_progress = 0
                filtered_results = None
                
                filter_error = None
                while not filter_done.is_set():
                    try:
                        # ç­‰å¾…0.5ç§’æˆ–ç›´åˆ°æœ‰æ–°è¿›åº¦
                        item = progress_queue.get(timeout=0.5)
                        
                        if isinstance(item, tuple):
                            if item[0] == 'DONE':
                                filtered_results = item[1]
                                break
                            elif item[0] == 'ERROR':
                                filter_error = item[1]
                                logger.error(f"ç²¾å‡†æ£€ç´¢è¿‡æ»¤å¤±è´¥: {filter_error}")
                                # é€šçŸ¥å‰ç«¯é”™è¯¯ä¿¡æ¯
                                error_msg = f" ç²¾å‡†æ£€ç´¢å¤±è´¥: {str(filter_error)}\n"
                                yield ('CONTENT', error_msg)
                                full_response += error_msg
                                break
                            else:
                                # è¿›åº¦æ›´æ–°
                                processed, total = item
                                # æ¯å¤„ç†5ä¸ªæ–‡æ¡£å‘é€ä¸€æ¬¡è¿›åº¦ï¼ˆé¿å…åˆ·å±ï¼‰
                                if processed - last_progress >= 5 or processed == total:
                                    progress_msg = f"ğŸ“Š è¿›åº¦: {processed}/{total} ({int(processed/total*100)}%)\n"
                                    yield ('CONTENT', progress_msg)
                                    full_response += progress_msg
                                    last_progress = processed
                    except queue.Empty:
                        # è¶…æ—¶ï¼Œç»§ç»­ç­‰å¾…
                        continue
                
                # ç­‰å¾…çº¿ç¨‹ç»“æŸï¼ˆè¶…æ—¶æ—¶é—´åº”è¶³å¤Ÿé•¿ï¼Œè¦†ç›–æ‰€æœ‰èŠ‚ç‚¹å¤„ç†ï¼‰
                # å‡è®¾å¹¶å‘å¤„ç†ï¼Œæœ€å¤šéœ€è¦ (èŠ‚ç‚¹æ•°/å¹¶å‘æ•°) * å•èŠ‚ç‚¹è¶…æ—¶æ—¶é—´
                # ç»™äºˆå……è¶³çš„æ—¶é—´ï¼š300ç§’ï¼ˆ5åˆ†é’Ÿï¼‰
                filter_thread.join(timeout=300)
                
                # æ£€æŸ¥çº¿ç¨‹æ˜¯å¦è¿˜åœ¨è¿è¡Œ
                if filter_thread.is_alive():
                    logger.warning("ç²¾å‡†æ£€ç´¢çº¿ç¨‹è¶…æ—¶æœªå®Œæˆï¼ˆ300ç§’ï¼‰ï¼Œå°†ç»§ç»­ä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ")
                    timeout_msg = " ç²¾å‡†æ£€ç´¢å¤„ç†è¶…æ—¶ï¼ˆ5åˆ†é’Ÿï¼‰ï¼Œå°†ä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ\n"
                    yield ('CONTENT', timeout_msg)
                    full_response += timeout_msg
                    filtered_results = None

                if filtered_results:
                    yield ('CONTENT', f"æ‰¾åˆ° {len(filtered_results)} ä¸ªå¯å›ç­”çš„èŠ‚ç‚¹")
                    full_response += f"æ‰¾åˆ° {len(filtered_results)} ä¸ªå¯å›ç­”çš„èŠ‚ç‚¹\n"
                    # InsertBlock æˆåŠŸï¼šåªä½¿ç”¨è¿‡æ»¤åçš„èŠ‚ç‚¹
                    nodes_for_prompt = None  # ä¸å†ä¼ å…¥åŸå§‹èŠ‚ç‚¹
                    filtered_map = {}
                    for result in filtered_results:
                        key = f"{result['file_name']}_{result['reranked_score']}"
                        filtered_map[key] = result
                else:
                    yield ('CONTENT', "æœªæ‰¾åˆ°å¯ç›´æ¥å›ç­”çš„èŠ‚ç‚¹ï¼Œå°†ä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ")
                    full_response += "æœªæ‰¾åˆ°å¯ç›´æ¥å›ç­”çš„èŠ‚ç‚¹ï¼Œå°†ä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ\n"
                    # InsertBlock å¤±è´¥ï¼šç»§ç»­ä½¿ç”¨åŸå§‹èŠ‚ç‚¹ï¼Œæ¸…ç©ºè¿‡æ»¤ç»“æœ
                    filtered_results = None

            # 3. æ„é€ æç¤ºè¯ï¼ˆä½¿ç”¨æ–°å·¥å…·å‡½æ•°ï¼Œæ³¨å…¥éšè—çŸ¥è¯†åº“å†…å®¹ï¼‰
            prompt_parts = build_knowledge_prompt(
                question=question,
                enable_thinking=enable_thinking,
                final_nodes=nodes_for_prompt,  # æ ¹æ® InsertBlock ç»“æœå†³å®šä¼ å…¥å“ªäº›èŠ‚ç‚¹
                filtered_results=filtered_results,
                sub_answers=getattr(self, '_last_sub_answers', None),
                synthesized_answer=getattr(self, '_last_synthesized_answer', None),
                hidden_nodes=hidden_nodes  # éšè—çŸ¥è¯†åº“èŠ‚ç‚¹ï¼ˆä¸æ˜¾ç¤ºæ¥æºï¼‰
            )

            # 4. è¾“å‡ºçŠ¶æ€
            status_msg = (
                "å·²æ‰¾åˆ°ç›¸å…³èµ„æ–™ï¼Œæ­£åœ¨ç”Ÿæˆå›ç­”..."
                if final_nodes
                else "æœªæ‰¾åˆ°é«˜ç›¸å…³æ€§èµ„æ–™ï¼ŒåŸºäºé€šç”¨çŸ¥è¯†å›ç­”..."
            )
            yield ('CONTENT', status_msg)
            full_response += status_msg + "\n"

            # 5. è°ƒç”¨ LLM
            for result in self._call_llm(llm, prompt_parts, enable_thinking=enable_thinking):
                # result æ˜¯å…ƒç»„ (prefix_type, content)
                prefix_type, chunk = result
                if prefix_type == 'THINK':
                    logger.debug(f"[Handler] æ”¶åˆ° THINK æ¶ˆæ¯: {len(chunk)} å­—ç¬¦ | å†…å®¹é¢„è§ˆ: {chunk[:50]}")
                    yield ('THINK', chunk)
                    logger.debug(f"[Handler] å·² yield THINK æ¶ˆæ¯")
                    # æ€è€ƒå†…å®¹ä¸è®¡å…¥ full_response
                elif prefix_type == 'CONTENT':
                    yield ('CONTENT', chunk)
                    full_response += chunk

            # 6. æ”¶é›†å¹¶è¾“å‡ºå…¨å±€å…³é”®å­—ï¼ˆå»é‡åé™åˆ¶æ•°é‡ï¼‰
            # 6.1 ä½¿ç”¨æƒé‡æ’åºæå–é—®é¢˜å…³é”®è¯
            from utils.keyword_ranker import keyword_ranker
            
            # è®¡ç®—é—®é¢˜å…³é”®è¯çš„ TF æƒé‡å¹¶æ’åº
            question_keywords_ranked = keyword_ranker.rank_question_keywords(question, top_k=100)
            
            # 6.2 æ”¶é›†æ–‡æ¡£åŒ¹é…çš„å…³é”®å­—å¹¶è®¡ç®—æƒé‡
            document_keywords_ranked = []
            if final_nodes:
                for i, node in enumerate(final_nodes):
                    retrieval_sources = node.node.metadata.get('retrieval_sources', [])
                    if 'keyword' in retrieval_sources:
                        matched_keywords = node.node.metadata.get('bm25_matched_keywords', [])
                        node_score = node.score
                        
                        # è®¡ç®—æ–‡æ¡£å…³é”®è¯æƒé‡
                        doc_kw_ranked = keyword_ranker.rank_document_keywords(
                            matched_keywords, 
                            node_score,
                            top_k=50
                        )
                        document_keywords_ranked.extend(doc_kw_ranked)
            
            # 6.3 åˆå¹¶å¹¶æŒ‰æƒé‡æ’åºå…³é”®è¯
            final_keywords = keyword_ranker.merge_and_rank_keywords(
                question_keywords_ranked,
                document_keywords_ranked,
                max_display=Settings.MAX_DISPLAY_KEYWORDS
            )
            
            # ä¸ºäº†å…¼å®¹æ—§ä»£ç ï¼Œåˆ†ç¦»é—®é¢˜å…³é”®è¯å’Œæ–‡æ¡£å…³é”®è¯
            # é—®é¢˜å…³é”®è¯ï¼šåœ¨é—®é¢˜ä¸­å‡ºç°çš„
            question_kw_set = set([kw for kw, _ in question_keywords_ranked])
            unique_question_keywords = [kw for kw in final_keywords if kw in question_kw_set]
            
            # æ–‡æ¡£å…³é”®è¯ï¼šä¸åœ¨é—®é¢˜ä¸­çš„
            unique_doc_keywords = [kw for kw in final_keywords if kw not in question_kw_set]
            
            # ä¿æŒåŸæœ‰çš„å›ºå®šåˆ†é…é€»è¾‘ï¼ˆä½†å·²ç»æŒ‰æƒé‡æ’åºï¼‰
            seen_doc = set(unique_question_keywords)
            for kw in unique_doc_keywords:
                if kw not in seen_doc:
                    seen_doc.add(kw)
                    unique_doc_keywords.append(kw)
            
            # é™åˆ¶æ•°é‡ï¼ˆä½¿ç”¨ MAX_DISPLAY_KEYWORDSï¼‰
            max_global_keywords = getattr(Settings, 'MAX_DISPLAY_KEYWORDS', 10)
            
            # å›ºå®šåˆ†é…ç­–ç•¥ï¼šé—®é¢˜å…³é”®è¯å’Œæ–‡æ¡£å…³é”®è¯å„å ä¸€åŠ
            max_question_keywords = max_global_keywords // 2  # ä¸€åŠç»™é—®é¢˜å…³é”®è¯
            max_doc_keywords = max_global_keywords - max_question_keywords  # å¦ä¸€åŠç»™æ–‡æ¡£å…³é”®è¯
            
            final_question_keywords = unique_question_keywords[:max_question_keywords]
            final_doc_keywords = unique_doc_keywords[:max_doc_keywords]
            
            # è¾“å‡ºç»“æ„åŒ–å…³é”®å­—ï¼ˆåŒºåˆ†æ¥æºï¼‰
            keywords_data = {
                "question": final_question_keywords,
                "document": final_doc_keywords
            }
            if final_question_keywords or final_doc_keywords:
                yield ('KEYWORDS', json.dumps(keywords_data, ensure_ascii=False))

            # 7. è¾“å‡ºå‚è€ƒæ¥æºï¼ˆä½¿ç”¨æ–°å·¥å…·å‡½æ•°ï¼‰
            reference_entries = build_reference_entries(final_nodes, filtered_map)

            if use_insert_block and filtered_results:
                # InsertBlock æ¨¡å¼ï¼šè¿”å›æ‰€æœ‰åŸå§‹èŠ‚ç‚¹ï¼Œä½†æ ‡æ³¨å“ªäº›è¢«é€‰ä¸­
                yield ('CONTENT', "\n\n**å‚è€ƒæ¥æºï¼ˆå…¨éƒ¨æ£€ç´¢ç»“æœï¼‰:**")
                full_response += "\n\nå‚è€ƒæ¥æºï¼ˆå…¨éƒ¨æ£€ç´¢ç»“æœï¼‰:"

                # éå†æ‰€æœ‰åŸå§‹èŠ‚ç‚¹ï¼Œæ ‡æ³¨å“ªäº›è¢«é€‰ä¸­
                for i, node in enumerate(final_nodes):
                    file_name = node.node.metadata.get('file_name', 'æœªçŸ¥')
                    initial_score = node.node.metadata.get('initial_score', 0.0)
                    key = f"{file_name}_{node.score}"

                    # æ£€æŸ¥è¯¥èŠ‚ç‚¹æ˜¯å¦åœ¨è¿‡æ»¤ç»“æœä¸­
                    filtered_info = filtered_map.get(key)

                    # æå–æ£€ç´¢å…ƒæ•°æ®
                    retrieval_sources = node.node.metadata.get('retrieval_sources', [])
                    vector_score = node.node.metadata.get('vector_score', 0.0)
                    bm25_score = node.node.metadata.get('bm25_score', 0.0)
                    vector_rank = node.node.metadata.get('vector_rank')
                    bm25_rank = node.node.metadata.get('bm25_rank')
                    
                    source_data = {
                        "id": i + 1,
                        "fileName": file_name,
                        "initialScore": f"{initial_score:.4f}",
                        "rerankedScore": f"{node.score:.4f}",
                        "content": node.node.text.strip(),
                        # æ£€ç´¢å…ƒæ•°æ®
                        "retrievalSources": retrieval_sources,
                        "vectorScore": f"{vector_score:.4f}",
                        "bm25Score": f"{bm25_score:.4f}",
                        # InsertBlock ç‰¹æœ‰å­—æ®µ
                        "canAnswer": filtered_info is not None,
                        "reasoning": filtered_info.get('reasoning', '') if filtered_info else '',
                        "keyPassage": filtered_info.get('key_passage', '') if filtered_info else ''
                    }
                    
                    # æ·»åŠ æ’åä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    if vector_rank is not None:
                        source_data['vectorRank'] = vector_rank
                    if bm25_rank is not None:
                        source_data['bm25Rank'] = bm25_rank
                    
                    # æ·»åŠ åŒ¹é…çš„å…³é”®è¯ï¼ˆBM25 æ£€ç´¢ï¼‰
                    if 'keyword' in retrieval_sources:
                        matched_keywords = node.node.metadata.get('bm25_matched_keywords', [])
                        if matched_keywords:
                            source_data['matchedKeywords'] = matched_keywords

                    yield ('SOURCE', json.dumps(source_data, ensure_ascii=False))

                    full_response += (
                        f"\n[{source_data['id']}] æ–‡ä»¶: {source_data['fileName']}, "
                        f"é‡æ’åˆ†: {source_data['rerankedScore']}, "
                        f"å¯å›ç­”: {source_data['canAnswer']}"
                    )

            elif final_nodes:
                # æ™®é€šæ¨¡å¼ï¼šæ˜¾ç¤ºæ‰€æœ‰æ£€ç´¢ç»“æœï¼ˆä½¿ç”¨æ–°å·¥å…·å‡½æ•°ï¼‰
                yield ('CONTENT', "\n\n**å‚è€ƒæ¥æº:**")
                full_response += "\n\nå‚è€ƒæ¥æº:"

                # åˆå¹¶éšè—èŠ‚ç‚¹ï¼ˆå¦‚æœæ˜¯ visible æ¨¡å¼ï¼‰
                nodes_to_display = final_nodes
                if hidden_nodes and Settings.HIDDEN_KB_INJECT_MODE == "visible":
                    logger.info(
                        f"[éšè—çŸ¥è¯†åº“] åˆå¹¶ {len(hidden_nodes)} ä¸ªéšè—èŠ‚ç‚¹åˆ°å‚è€ƒæ¥æº | "
                        f"ä¸»çŸ¥è¯†åº“: {len(final_nodes)} æ¡ | "
                        f"éšè—èŠ‚ç‚¹: {len(hidden_nodes)} æ¡ (é¢å¤–æ˜¾ç¤ºï¼Œä¸å ç”¨ rerank_top_n) | "
                        f"æ€»è®¡: {len(final_nodes) + len(hidden_nodes)} æ¡"
                    )
                    nodes_to_display = final_nodes + hidden_nodes
                
                # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦åŒ…å«éšè—èŠ‚ç‚¹
                include_hidden = (Settings.HIDDEN_KB_INJECT_MODE == "visible")
                
                for source_msg in format_sources(nodes_to_display, include_hidden=include_hidden):
                    yield source_msg
                    if isinstance(source_msg, tuple) and source_msg[0] == "SOURCE":
                        data = json.loads(source_msg[1])
                        full_response += (
                            f"\n[{data['id']}] æ–‡ä»¶: {data['fileName']}, "
                            f"åˆå§‹åˆ†: {data['initialScore']}, "
                            f"é‡æ’åˆ†: {data['rerankedScore']}"
                        )

            # ä½¿ç”¨æ–°å·¥å…·å‡½æ•°è®°å½•å‚è€ƒæ–‡çŒ®
            log_reference_details(
                question=question,
                references=reference_entries,
                mode="single"
            )

            # 8. æ ¼å¼åŒ–æ ¡éªŒå’Œä¿®å¤ï¼ˆåœ¨æœ€åä¸€æ¬¡ yield å‰ï¼‰
            from utils.response_formatter import response_formatter
            
            # æå–çº¯æ–‡æœ¬å“åº”ï¼ˆå»é™¤çŠ¶æ€æ¶ˆæ¯ï¼‰
            # full_response åŒ…å«äº†æ‰€æœ‰ CONTENT å†…å®¹ï¼Œéœ€è¦æå– LLM å®é™…å›ç­”éƒ¨åˆ†
            # ç®€å•å¤„ç†ï¼šå»é™¤æ£€ç´¢çŠ¶æ€æ¶ˆæ¯
            response_to_validate = full_response
            for status_prefix in [
                "æ­£åœ¨è¿›è¡Œæ··åˆæ£€ç´¢...",
                "å·²æ‰¾åˆ°ç›¸å…³èµ„æ–™ï¼Œæ­£åœ¨ç”Ÿæˆå›ç­”...",
                "æœªæ‰¾åˆ°é«˜ç›¸å…³æ€§èµ„æ–™ï¼ŒåŸºäºé€šç”¨çŸ¥è¯†å›ç­”...",
                "æ­£åœ¨ä½¿ç”¨ç²¾å‡†æ£€ç´¢åˆ†æ",
                "è¿›åº¦:",
                "æ‰¾åˆ°",
                "æœªæ‰¾åˆ°å¯ç›´æ¥å›ç­”çš„èŠ‚ç‚¹",
                "å‚è€ƒæ¥æº"
            ]:
                if status_prefix in response_to_validate:
                    # ç§»é™¤çŠ¶æ€æ¶ˆæ¯è¡Œ
                    lines = response_to_validate.split('\n')
                    filtered_lines = [line for line in lines if not line.strip().startswith(status_prefix)]
                    response_to_validate = '\n'.join(filtered_lines)
            
            # æ ¼å¼åŒ–æ ¡éªŒ
            validated_response = response_formatter.process_response(
                response_to_validate.strip(),
                question=question
            )
            
            # å¦‚æœæ ¼å¼è¢«ä¿®å¤ï¼Œæ›´æ–°å“åº”
            if validated_response != response_to_validate.strip():
                full_response = validated_response

            yield ('DONE', '')

            # 9. ä¿å­˜æ—¥å¿—ï¼ˆä½¿ç”¨æ–°å·¥å…·å‡½æ•°ï¼‰
            save_qa_log(
                question=question,
                response=full_response,
                client_ip=client_ip,
                has_rag=bool(final_nodes),
                use_insert_block=use_insert_block
            )

        except Exception as e:
            error_msg = f"å¤„ç†é”™è¯¯: {str(e)}"
            logger.error(f"çŸ¥è¯†é—®ç­”å¤„ç†å‡ºé”™: {e}", exc_info=True)
            yield ('ERROR', error_msg)
            # ç¡®ä¿å‘é€ DONE ä¿¡å·ï¼Œé¿å…å‰ç«¯ç­‰å¾…è¶…æ—¶
            yield ('DONE', '')

    def _retrieve_and_rerank(self, question: str, rerank_top_n: int, conversation_history: Optional[List[Dict]] = None):
        """
        æ£€ç´¢å’Œé‡æ’åºï¼ˆæ”¯æŒå­é—®é¢˜åˆ†è§£ï¼‰
        
        Args:
            question: ç”¨æˆ·æŸ¥è¯¢
            rerank_top_n: é‡æ’åºè¿”å›æ•°é‡
            conversation_history: å¯¹è¯å†å²ï¼ˆç”¨äºå¤šè½®åœºæ™¯ï¼‰
            
        Returns:
            æ£€ç´¢èŠ‚ç‚¹åˆ—è¡¨
        """
        # å¦‚æœå¯ç”¨äº†å­é—®é¢˜åˆ†è§£å™¨ï¼Œå°è¯•ä½¿ç”¨åˆ†è§£æ£€ç´¢
        if self.sub_question_decomposer and self.sub_question_decomposer.enabled:
            logger.info("[æ£€ç´¢ç­–ç•¥] å°è¯•ä½¿ç”¨å­é—®é¢˜åˆ†è§£æ£€ç´¢ï¼ˆå¤šè½®ï¼‰")
            try:
                # æ³¨æ„ï¼šå¤šè½®åœºæ™¯ä½¿ç”¨é»˜è®¤retrieverï¼Œå› ä¸ºæ²¡æœ‰æ„å›¾åˆ†ç±»
                # å¦‚æœéœ€è¦æ”¯æŒå¤šè½®+æ„å›¾è·¯ç”±ï¼Œéœ€è¦åœ¨è¿™é‡Œä¹Ÿæ·»åŠ æ„å›¾åˆ†ç±»é€»è¾‘
                nodes, metadata = self.sub_question_decomposer.retrieve_with_decomposition(
                    query=question,
                    rerank_top_n=rerank_top_n,
                    conversation_history=conversation_history
                )
                
                # è®°å½•åˆ†è§£å…ƒæ•°æ®
                if metadata.get('decomposed'):
                    logger.info(
                        f"[å­é—®é¢˜æ£€ç´¢] åˆ†è§£æ£€ç´¢å®Œæˆ | "
                        f"å­é—®é¢˜æ•°: {len(metadata['sub_questions'])} | "
                        f"è¿”å›èŠ‚ç‚¹æ•°: {len(nodes)}"
                    )
                    # è®°å½•è¯¦ç»†çš„å­é—®é¢˜ä¿¡æ¯åˆ°æ—¥å¿—
                    for i, sub_result in enumerate(metadata['sub_results'], 1):
                        logger.info(
                            f"  å­é—®é¢˜{i}: {sub_result['sub_question']} | "
                            f"èŠ‚ç‚¹æ•°: {sub_result['node_count']} | "
                            f"æœ€é«˜åˆ†: {sub_result['top_score']:.4f}"
                        )
                    
                    # å¯é€‰ï¼šç”Ÿæˆå­é—®é¢˜ç­”æ¡ˆåˆæˆï¼ˆå¦‚æœæœ‰sub_answersï¼‰
                    if metadata.get('sub_answers') and len(metadata['sub_answers']) > 0:
                        try:
                            synthesized_answer = self.sub_question_decomposer.synthesize_answer(
                                original_query=question,
                                sub_answers=metadata['sub_answers']
                            )
                            if synthesized_answer:
                                # å°†åˆæˆç­”æ¡ˆæ·»åŠ åˆ°metadataï¼Œä¾›åç»­ä½¿ç”¨
                                metadata['synthesized_answer'] = synthesized_answer
                                # å­˜å‚¨ä¸ºå®ä¾‹å˜é‡ï¼Œä¾›_build_promptä½¿ç”¨
                                self._last_synthesized_answer = synthesized_answer
                                logger.info(f"[ç­”æ¡ˆåˆæˆ] å·²ç”Ÿæˆåˆæˆç­”æ¡ˆ | é•¿åº¦: {len(synthesized_answer)}")
                        except Exception as synth_e:
                            logger.warning(f"[ç­”æ¡ˆåˆæˆ] åˆæˆå¤±è´¥: {synth_e}")
                else:
                    logger.info("[å­é—®é¢˜æ£€ç´¢] æœªåˆ†è§£ï¼Œä½¿ç”¨æ ‡å‡†æ£€ç´¢")
                
                return nodes
                
            except Exception as e:
                logger.error(f"[å­é—®é¢˜æ£€ç´¢] åˆ†è§£æ£€ç´¢å¤±è´¥: {e}", exc_info=True)
                logger.info("[å­é—®é¢˜æ£€ç´¢] å›é€€åˆ°æ ‡å‡†æ£€ç´¢æµç¨‹")
                # ç»§ç»­æ‰§è¡Œæ ‡å‡†æ£€ç´¢
        
        # æ ‡å‡†æ£€ç´¢æµç¨‹
        logger.info(f"[å•çŸ¥è¯†åº“æ£€ç´¢] å¼€å§‹æ£€ç´¢é—®é¢˜: {question}")
        logger.info(f"ğŸ” [DEBUG] ä½¿ç”¨çš„æ£€ç´¢å™¨å¯¹è±¡ID: {id(self.retriever)}")
        logger.info(f"ğŸ” [DEBUG] æ£€ç´¢å™¨ç±»å‹: {type(self.retriever).__name__}")
        retrieved_nodes = self.retriever.retrieve(question)
        
        # ğŸ” DEBUG: è®°å½•åˆå§‹æ£€ç´¢å¾—åˆ†
        if retrieved_nodes:
            initial_scores = [f"{n.score:.4f}" for n in retrieved_nodes[:5]]
            logger.info(f"[DEBUG] å•çŸ¥è¯†åº“åˆå§‹æ£€ç´¢Top5å¾—åˆ†: {', '.join(initial_scores)}")

        # å–å‰ N ä¸ªé€å…¥é‡æ’
        reranker_input_top_n = Settings.RERANKER_INPUT_TOP_N
        logger.info(f"[å•çŸ¥è¯†åº“æ£€ç´¢] é…ç½®æ£€æŸ¥ - RERANKER_INPUT_TOP_N: {reranker_input_top_n}")
        
        # è¯¦ç»†æ£€æŸ¥ retrieved_nodes
        logger.info(f"[å•çŸ¥è¯†åº“æ£€ç´¢] retrieved_nodes ç±»å‹: {type(retrieved_nodes)}")
        logger.info(f"[å•çŸ¥è¯†åº“æ£€ç´¢] retrieved_nodes é•¿åº¦: {len(retrieved_nodes) if retrieved_nodes else 'None'}")
        
        if retrieved_nodes and len(retrieved_nodes) > 0:
            logger.info(f"[å•çŸ¥è¯†åº“æ£€ç´¢] ç¬¬ä¸€ä¸ªèŠ‚ç‚¹é¢„è§ˆ: {retrieved_nodes[0].node.get_content()[:100]}...")
        
        reranker_input = retrieved_nodes[:reranker_input_top_n]

        logger.info(
            f"[å•çŸ¥è¯†åº“æ£€ç´¢] åˆæ£€ç´¢æ‰¾åˆ° {len(retrieved_nodes)} ä¸ªèŠ‚ç‚¹, "
            f"é€‰å–å‰ {len(reranker_input)} ä¸ªé€å…¥é‡æ’"
        )
        
        # å¦‚æœåˆå§‹æ£€ç´¢ä¸ºç©ºï¼Œæ‰“å°è­¦å‘Š
        if len(retrieved_nodes) == 0:
            logger.warning(
                f"[å•çŸ¥è¯†åº“æ£€ç´¢] åˆå§‹æ£€ç´¢ç»“æœä¸ºç©ºï¼\n"
                f"  é—®é¢˜: {question}\n"
                f"  æ£€ç´¢å™¨çŠ¶æ€: {self.retriever is not None}\n"
                f"  å¯èƒ½åŸå› : çŸ¥è¯†åº“ä¸ºç©ºã€ç´¢å¼•æŸåã€æˆ–é—®é¢˜ä¸çŸ¥è¯†åº“å®Œå…¨ä¸ç›¸å…³"
            )

        # é‡æ’åº
        logger.info(f"[å•çŸ¥è¯†åº“æ£€ç´¢] å‡†å¤‡é‡æ’åº - reranker_input é•¿åº¦: {len(reranker_input)}")
        
        if reranker_input:
            logger.info(f"[å•çŸ¥è¯†åº“æ£€ç´¢] âœ“ è¿›å…¥é‡æ’åºåˆ†æ”¯ï¼Œå¼€å§‹è°ƒç”¨ Reranker æ¨¡å‹")
            
            # ä½¿ç”¨å®ä¾‹çš„ Rerankerï¼ˆå·²ä¿®å¤çŠ¶æ€æ±¡æŸ“é—®é¢˜ï¼‰
            reranked_nodes = self.reranker.postprocess_nodes(
                reranker_input,
                query_bundle=QueryBundle(question)
            )
            logger.info(f"[å•çŸ¥è¯†åº“æ£€ç´¢] âœ“ Reranker å¤„ç†å®Œæˆï¼Œå¾—åˆ° {len(reranked_nodes)} ä¸ªèŠ‚ç‚¹")
            # ğŸ” DEBUG: è®°å½•é‡æ’åºåå¾—åˆ†
            if reranked_nodes:
                rerank_scores = [f"{n.score:.4f}" for n in reranked_nodes[:5]]
                logger.info(f"[DEBUG] å•çŸ¥è¯†åº“é‡æ’åºåTop5å¾—åˆ†: {', '.join(rerank_scores)}")
        else:
            logger.warning(f"[å•çŸ¥è¯†åº“æ£€ç´¢] âš ï¸ reranker_input ä¸ºç©ºï¼Œè·³è¿‡é‡æ’åºï¼")
            reranked_nodes = []

        # é˜ˆå€¼è¿‡æ»¤
        threshold = Settings.RERANK_SCORE_THRESHOLD
        final_nodes = [
            node for node in reranked_nodes
            if node.score >= threshold
        ]
        
        #  DEBUG: è®°å½•è¿‡æ»¤åå¾—åˆ†
        if final_nodes:
            final_scores = [f"{n.score:.4f}" for n in final_nodes[:5]]
            logger.info(f"[DEBUG] å•çŸ¥è¯†åº“é˜ˆå€¼è¿‡æ»¤åTop5å¾—åˆ†: {', '.join(final_scores)}")

        logger.info(
            f"[å•çŸ¥è¯†åº“æ£€ç´¢] é‡æ’åºåæœ‰ {len(reranked_nodes)} ä¸ªèŠ‚ç‚¹, "
            f"ç»è¿‡é˜ˆå€¼ {threshold} è¿‡æ»¤åå‰©ä¸‹ {len(final_nodes)} ä¸ª"
        )
        
        # å¦‚æœé˜ˆå€¼è¿‡æ»¤åä¸ºç©ºï¼Œæ‰“å°è¯¦ç»†ä¿¡æ¯
        if len(reranked_nodes) > 0 and len(final_nodes) == 0:
            max_score = max(node.score for node in reranked_nodes) if reranked_nodes else 0.0
            logger.warning(
                f"[å•çŸ¥è¯†åº“æ£€ç´¢] é˜ˆå€¼è¿‡æ»¤åç»“æœä¸ºç©ºï¼\n"
                f"  é‡æ’åºèŠ‚ç‚¹æ•°: {len(reranked_nodes)}\n"
                f"  æœ€é«˜åˆ†æ•°: {max_score:.4f}\n"
                f"  é˜ˆå€¼: {threshold}\n"
                f"  å»ºè®®: é™ä½ RERANK_SCORE_THRESHOLD æˆ–æ£€æŸ¥ Reranker æ¨¡å‹"
            )

        # åº”ç”¨æœ€ç»ˆæ•°é‡é™åˆ¶
        result = final_nodes[:rerank_top_n]
        logger.info(f"[å•çŸ¥è¯†åº“æ£€ç´¢] æœ€ç»ˆè¿”å› {len(result)} ä¸ªèŠ‚ç‚¹")
        return result

    def _call_llm(self, llm, prompt_parts, enable_thinking: bool = False):
        """
        è°ƒç”¨ LLMï¼Œæ”¯æŒæ€è€ƒå†…å®¹å’Œæ­£æ–‡å†…å®¹çš„åˆ†ç¦»ï¼ˆä½¿ç”¨æ–°å·¥å…·å‡½æ•°ï¼‰

        Args:
            llm: LLM å®ä¾‹
            prompt_parts: æç¤ºè¯å­—å…¸
            enable_thinking: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼ï¼ˆç”¨äºè§£æè¾“å‡ºï¼‰

        Note:
            æ”¯æŒä¸¤ç§æ€è€ƒæ¨¡å¼ï¼š
            1. é˜¿é‡Œäº‘åŸç”Ÿ reasoning_content å­—æ®µï¼ˆæ¨èï¼‰
            2. æ–‡æœ¬æ ‡è®°æ–¹å¼ï¼ˆå…¼å®¹å…¶ä»–æ¨¡å‹ï¼‰
        """
        logger.info(f"ä½¿ç”¨å¤–éƒ¨ Prompt:\n{prompt_parts['fallback_prompt'][:200]}...")

        response_stream = self.llm_wrapper.stream(
            llm,
            prompt=prompt_parts['fallback_prompt'],
            system_prompt=prompt_parts['system_prompt'],
            user_prompt=prompt_parts['user_prompt'],
            assistant_context=prompt_parts['assistant_context'],
            use_chat_mode=Settings.USE_CHAT_MODE,
            enable_thinking=enable_thinking
        )

        # ä½¿ç”¨æ–°å·¥å…·å‡½æ•°è§£ææµå¼è¾“å‡º
        if enable_thinking:
            yield from parse_thinking_stream(response_stream)
        else:
            yield from parse_normal_stream(response_stream)

   

    def _save_log(self, question: str, response: str, client_ip: str, has_rag: bool, use_insert_block: bool = False):
        """
        ã€å·²åºŸå¼ƒã€‘ä¿å­˜é—®ç­”æ—¥å¿— - å·²è¢« save_qa_log å·¥å…·å‡½æ•°æ›¿ä»£
        ä¿ç•™æ­¤æ–¹æ³•ä½œä¸ºå¤‡ä»½ï¼Œå¾…æµ‹è¯•é€šè¿‡åå¯åˆ é™¤
        """
        from utils import QALogger
        qa_logger = QALogger(Settings.LOG_DIR)
        qa_logger.save_log(
            question,
            response,
            'knowledge_qa_stream',
            metadata={
                "ip": client_ip,
                "answer_type": "rag" if has_rag else "general",
                "chat_mode": Settings.USE_CHAT_MODE,
                "insert_block_mode": use_insert_block
            }
        )

    def process_conversation(
        self,
        question: str,
        session_id: str,
        enable_thinking: bool,
        rerank_top_n: int,
        llm,
        client_ip: str = "unknown",
        use_insert_block: bool = False,
        insert_block_llm_id: Optional[str] = None
    ) -> Generator[str, None, None]:
        """
        å¤„ç†æ”¯æŒå¤šè½®å¯¹è¯çš„çŸ¥è¯†é—®ç­”

        æµç¨‹ï¼š
        1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        2. InsertBlock è¿‡æ»¤ï¼ˆå¯é€‰ï¼‰
        3. è·å–å†å²å¯¹è¯
        4. ä½¿ç”¨çŸ¥è¯†é—®ç­”çš„æç¤ºè¯æ„å»º promptï¼ˆå°†å†å²å¯¹è¯æ³¨å…¥åˆ°ä¸Šä¸‹æ–‡ä¸­ï¼‰
        5. è°ƒç”¨ LLM
        6. å­˜å‚¨æœ¬è½®å¯¹è¯
        7. è¿”å›å‚è€ƒæ¥æº

        Args:
            question: é—®é¢˜å†…å®¹
            session_id: ä¼šè¯ID
            enable_thinking: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼
            rerank_top_n: é‡æ’åºåè¿”å›çš„æ–‡æ¡£æ•°é‡
            llm: LLM å®ä¾‹
            client_ip: å®¢æˆ·ç«¯ IP
            use_insert_block: æ˜¯å¦ä½¿ç”¨ InsertBlock è¿‡æ»¤æ¨¡å¼
            insert_block_llm_id: InsertBlock ä½¿ç”¨çš„ LLM ID

        Yields:
            SSE æ ¼å¼çš„å“åº”æµ
        """
        full_response = ""
        
        # æ¸…ç©ºä¸Šä¸€æ¬¡çš„åˆæˆç­”æ¡ˆï¼ˆé¿å…æ±¡æŸ“ï¼‰
        self._last_synthesized_answer = None

        try:
            logger.info(
                f"å¤„ç†å¤šè½®å¯¹è¯: ä¼šè¯ {session_id[:8]}... | '{question}' | "
                f"æ€è€ƒæ¨¡å¼: {enable_thinking} | InsertBlock: {use_insert_block}"
            )

            # è·å–å¯¹è¯ç®¡ç†å™¨
            from flask import current_app
            knowledge_service = current_app.knowledge_service
            conversation_manager = knowledge_service.conversation_manager

            if not conversation_manager:
                raise ValueError("å¯¹è¯ç®¡ç†å™¨æœªåˆå§‹åŒ–")

            # è¿”å›ä¼šè¯ID
            yield f"SESSION:{session_id}"

            # 1. è·å–æœ€è¿‘çš„å¯¹è¯å†å²ï¼ˆç”¨äºå­é—®é¢˜åˆ†è§£ï¼‰
            from config import Settings as AppSettings
            recent_turns_for_decomp = getattr(AppSettings, 'SUBQUESTION_HISTORY_COMPRESS_TURNS', 5)
            
            try:
                conversation_history_for_decomp = conversation_manager.get_recent_history(
                    session_id=session_id,
                    limit=recent_turns_for_decomp
                )
            except Exception as e:
                logger.warning(f"è·å–å¯¹è¯å†å²ç”¨äºå­é—®é¢˜åˆ†è§£å¤±è´¥: {e}")
                conversation_history_for_decomp = None
            
            # 2. æ£€ç´¢
            # å¦‚æœå‰ç«¯è®¾ç½®å‚è€ƒæ•°é‡ä¸º 0ï¼Œè·³è¿‡æ£€ç´¢
            if rerank_top_n == 0:
                logger.info("[å¯¹è¯-æ£€ç´¢è·³è¿‡] å‰ç«¯è®¾ç½®å‚è€ƒæ•°é‡ä¸º 0ï¼Œè·³è¿‡æ£€ç´¢å’Œå­é—®é¢˜åˆ†è§£")
                final_nodes = []
                hidden_nodes = []
            else:
                yield "CONTENT:æ­£åœ¨è¿›è¡Œæ··åˆæ£€ç´¢..."
                full_response += "æ­£åœ¨è¿›è¡Œæ··åˆæ£€ç´¢...\n"

                final_nodes = self._retrieve_and_rerank(
                    question, 
                    rerank_top_n,
                    conversation_history=conversation_history_for_decomp
                )
                
                # 2.5 éšè—çŸ¥è¯†åº“æ£€ç´¢ï¼ˆå¹¶è¡Œè¿›è¡Œï¼‰
                hidden_nodes = []
                if self.hidden_kb_retriever and self.hidden_kb_retriever.enabled:
                    try:
                        logger.info("[å¯¹è¯-éšè—çŸ¥è¯†åº“] å¼€å§‹å¹¶è¡Œæ£€ç´¢...")
                        hidden_nodes = self.hidden_kb_retriever.retrieve(question)
                        if hidden_nodes:
                            logger.info(f"[å¯¹è¯-éšè—çŸ¥è¯†åº“] æ£€ç´¢æˆåŠŸ | è¿”å› {len(hidden_nodes)} æ¡")
                            
                            # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦å°†éšè—èŠ‚ç‚¹åˆå¹¶åˆ°æ˜¾ç¤ºåˆ—è¡¨
                            if Settings.HIDDEN_KB_INJECT_MODE == "visible":
                                logger.info("[å¯¹è¯-éšè—çŸ¥è¯†åº“] visible æ¨¡å¼ï¼šå°†éšè—èŠ‚ç‚¹åˆå¹¶åˆ°å‚è€ƒæ¥æº")
                            else:
                                logger.info("[å¯¹è¯-éšè—çŸ¥è¯†åº“] silent æ¨¡å¼ï¼šéšè—èŠ‚ç‚¹ä¸æ˜¾ç¤ºæ¥æº")
                    except Exception as e:
                        logger.warning(f"[å¯¹è¯-éšè—çŸ¥è¯†åº“] æ£€ç´¢å¤±è´¥: {e}")
                        hidden_nodes = []

            # 2. å¦‚æœå¯ç”¨ InsertBlock æ¨¡å¼ï¼Œè¿›è¡Œæ™ºèƒ½è¿‡æ»¤
            filtered_results = None
            filtered_map = None
            nodes_for_prompt = final_nodes

            if use_insert_block and final_nodes and self.insert_block_filter:
                start_msg = f"æ­£åœ¨ä½¿ç”¨ç²¾å‡†æ£€ç´¢åˆ†æ {len(final_nodes)} ä¸ªæ–‡æ¡£...\næç¤ºï¼šç³»ç»Ÿæ­£åœ¨é€ä¸ªåˆ¤æ–­æ¯ä¸ªæ–‡æ¡£æ˜¯å¦èƒ½å›ç­”æ‚¨çš„é—®é¢˜ï¼Œè¯·ç¨å€™"
                yield f"CONTENT:{start_msg}"
                full_response += start_msg + "\n"
                
                # ä½¿ç”¨é˜Ÿåˆ—æ”¶é›†è¿›åº¦
                import queue
                import threading
                progress_queue = queue.Queue()
                filter_done = threading.Event()
                
                # å®šä¹‰è¿›åº¦å›è°ƒå‡½æ•°ï¼ˆå°†è¿›åº¦æ”¾å…¥é˜Ÿåˆ—ï¼‰
                def progress_callback(processed, total):
                    logger.info(f"[å¯¹è¯-ç²¾å‡†æ£€ç´¢è¿›åº¦] {processed}/{total} ä¸ªæ–‡æ¡£å·²åˆ†æ")
                    progress_queue.put((processed, total))
                
                # åœ¨åå°çº¿ç¨‹æ‰§è¡Œè¿‡æ»¤
                def run_filter():
                    try:
                        result = self.insert_block_filter.filter_nodes(
                            question=question,
                            nodes=final_nodes,
                            llm_id=insert_block_llm_id,
                            progress_callback=progress_callback
                        )
                        progress_queue.put(('DONE', result))
                    except Exception as e:
                        progress_queue.put(('ERROR', e))
                    finally:
                        filter_done.set()
                
                filter_thread = threading.Thread(target=run_filter, daemon=True)
                filter_thread.start()
                
                # ä¸»çº¿ç¨‹å®šæœŸæ£€æŸ¥è¿›åº¦å¹¶å‘é€
                last_progress = 0
                filtered_results = None
                
                while not filter_done.is_set():
                    try:
                        # ç­‰å¾…0.5ç§’æˆ–ç›´åˆ°æœ‰æ–°è¿›åº¦
                        item = progress_queue.get(timeout=0.5)
                        
                        if isinstance(item, tuple):
                            if item[0] == 'DONE':
                                filtered_results = item[1]
                                break
                            elif item[0] == 'ERROR':
                                logger.error(f"å¯¹è¯-ç²¾å‡†æ£€ç´¢è¿‡æ»¤å¤±è´¥: {item[1]}")
                                break
                            else:
                                # è¿›åº¦æ›´æ–°
                                processed, total = item
                                # æ¯å¤„ç†5ä¸ªæ–‡æ¡£å‘é€ä¸€æ¬¡è¿›åº¦ï¼ˆé¿å…åˆ·å±ï¼‰
                                if processed - last_progress >= 5 or processed == total:
                                    progress_msg = f"ğŸ“Š è¿›åº¦: {processed}/{total} ({int(processed/total*100)}%)"
                                    yield f"CONTENT:{progress_msg}"
                                    full_response += progress_msg + "\n"
                                    last_progress = processed
                    except queue.Empty:
                        # è¶…æ—¶ï¼Œç»§ç»­ç­‰å¾…
                        continue
                
                # ç­‰å¾…çº¿ç¨‹ç»“æŸï¼ˆå¤šè½®å¯¹è¯æ¨¡å¼ï¼ŒåŒæ ·ç»™äºˆå……è¶³æ—¶é—´ï¼‰
                filter_thread.join(timeout=600)

                if filtered_results:
                    yield f"CONTENT:æ‰¾åˆ° {len(filtered_results)} ä¸ªå¯å›ç­”çš„èŠ‚ç‚¹"
                    full_response += f"æ‰¾åˆ° {len(filtered_results)} ä¸ªå¯å›ç­”çš„èŠ‚ç‚¹\n"
                    nodes_for_prompt = None
                    filtered_map = {}
                    for result in filtered_results:
                        key = f"{result['file_name']}_{result['reranked_score']}"
                        filtered_map[key] = result
                else:
                    yield "CONTENT:æœªæ‰¾åˆ°å¯ç›´æ¥å›ç­”çš„èŠ‚ç‚¹ï¼Œå°†ä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ"
                    full_response += "æœªæ‰¾åˆ°å¯ç›´æ¥å›ç­”çš„èŠ‚ç‚¹ï¼Œå°†ä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ\n"
                    filtered_results = None

            # 3. è·å–å†å²å¯¹è¯
            from config import Settings as AppSettings
            recent_turns = getattr(AppSettings, 'MAX_RECENT_TURNS', 6)
            relevant_turns = getattr(AppSettings, 'MAX_RELEVANT_TURNS', 3)
            max_summary_turns = getattr(AppSettings, 'MAX_SUMMARY_TURNS', 12)

            # 3.1 è·å–æœ€è¿‘çš„å¯¹è¯å†å²
            recent_history = conversation_manager.get_recent_history(
                session_id=session_id,
                limit=recent_turns
            )

            # 3.2 æ£€ç´¢ä¸å½“å‰é—®é¢˜ç›¸å…³çš„å†å²å¯¹è¯
            relevant_history = []
            if relevant_turns > 0:
                try:
                    relevant_history = conversation_manager.retrieve_relevant_history(
                        session_id=session_id,
                        current_query=question,
                        top_k=relevant_turns
                    )
                    # è¿‡æ»¤æ‰å·²ç»åœ¨æœ€è¿‘å¯¹è¯ä¸­çš„è½®æ¬¡ï¼ˆé¿å…é‡å¤ï¼‰
                    recent_turn_ids = {turn.get('turn_id') for turn in recent_history if turn.get('turn_id')}
                    relevant_history = [
                        turn for turn in relevant_history
                        if turn.get('turn_id') not in recent_turn_ids
                    ]
                    logger.info(f"æ£€ç´¢åˆ° {len(relevant_history)} æ¡ç›¸å…³å†å²å¯¹è¯ï¼ˆæ’é™¤æœ€è¿‘å¯¹è¯åï¼‰")
                except Exception as e:
                    logger.warning(f"æ£€ç´¢ç›¸å…³å†å²å¯¹è¯å¤±è´¥: {e}")
                    relevant_history = []

            # 4. æ„å»ºå†å²å¯¹è¯æ‘˜è¦ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
            # è·å–ä¼šè¯æ€»è½®æ•°
            try:
                all_history = conversation_manager.get_recent_history(
                    session_id=session_id,
                    limit=100  # å‡è®¾æœ€å¤š100è½®
                )
                total_turns = len(all_history)
            except Exception as e:
                logger.warning(f"è·å–æ€»å¯¹è¯è½®æ•°å¤±è´¥: {e}")
                total_turns = len(recent_history)
                all_history = recent_history

            history_summary = None

            # åªæœ‰å½“æ€»è½®æ•°è¶…è¿‡ MAX_SUMMARY_TURNS æ—¶æ‰ç”Ÿæˆæ‘˜è¦ï¼ˆé¿å…é¢‘ç¹æ‘˜è¦ï¼‰
            if total_turns > max_summary_turns:
                # æ’é™¤æœ€è¿‘Nè½®ï¼Œå‰©ä½™çš„ç”¨äºç”Ÿæˆæ‘˜è¦
                old_history = all_history[:-recent_turns] if len(all_history) > recent_turns else []

                if old_history and len(old_history) >= 3:  # è‡³å°‘3è½®æ‰å€¼å¾—æ‘˜è¦
                    # æ£€æŸ¥æ‘˜è¦ç¼“å­˜
                    cache_key = f"{session_id}_summary"
                    current_time = time.time()

                    if hasattr(conversation_manager, '_summary_cache'):
                        cache_entry = conversation_manager._summary_cache.get(cache_key)
                        if cache_entry:
                            cache_age = current_time - cache_entry.get('timestamp', 0)
                            summarized_count = cache_entry.get('summarized_until', 0)

                            # å¦‚æœç¼“å­˜æœ‰æ•ˆä¸”å¯¹è¯æ•°é‡æ²¡å˜åŒ–å¤ªå¤šï¼ˆå…è®¸Â±2è½®å·®å¼‚ï¼‰ï¼Œä½¿ç”¨ç¼“å­˜
                            if (cache_age < AppSettings.SUMMARY_CACHE_TTL and
                                abs(len(old_history) - summarized_count) <= 2):
                                history_summary = cache_entry.get('summary')
                                logger.info(f"ä½¿ç”¨ç¼“å­˜çš„å†å²æ‘˜è¦ (ç¼“å­˜æ—¶é•¿: {cache_age:.0f}s)")

                    # å¦‚æœæ²¡æœ‰ç¼“å­˜æˆ–ç¼“å­˜å¤±æ•ˆï¼Œç”Ÿæˆæ–°æ‘˜è¦
                    if not history_summary:
                        try:
                            history_summary = conversation_manager.summarize_old_conversations(
                                session_id=session_id,
                                conversations=old_history
                            )

                            # æ›´æ–°ç¼“å­˜
                            if history_summary and hasattr(conversation_manager, '_summary_cache'):
                                conversation_manager._summary_cache[cache_key] = {
                                    'summary': history_summary,
                                    'summarized_until': len(old_history),
                                    'timestamp': current_time
                                }
                                logger.info(f"å·²ç”Ÿæˆå¹¶ç¼“å­˜å†å²æ‘˜è¦ (è¦†ç›– {len(old_history)} è½®)")
                        except Exception as e:
                            logger.warning(f"ç”Ÿæˆå†å²æ‘˜è¦å¤±è´¥: {e}")
                            history_summary = None
                else:
                    logger.debug(f"æ—§å¯¹è¯è½®æ•°({len(old_history)})ä¸è¶³ï¼Œè·³è¿‡æ‘˜è¦ç”Ÿæˆ")
            else:
                logger.debug(f"æ€»è½®æ•°({total_turns})æœªè¾¾æ‘˜è¦é˜ˆå€¼({max_summary_turns})ï¼Œè·³è¿‡æ‘˜è¦")

            # 5. ä½¿ç”¨ä¼˜åŒ–çš„æç¤ºè¯æ„å»ºæ–¹å¼ï¼ˆæ³¨å…¥å†å²å¯¹è¯å’Œéšè—çŸ¥è¯†åº“ï¼‰
            prompt_parts = self._build_prompt_with_history(
                question,
                enable_thinking,
                nodes_for_prompt,
                filtered_results=filtered_results,
                recent_history=recent_history,
                relevant_history=relevant_history,
                history_summary=history_summary,
                hidden_nodes=hidden_nodes
            )

            # 6. è¾“å‡ºçŠ¶æ€
            status_msg = (
                "å·²æ‰¾åˆ°ç›¸å…³èµ„æ–™ï¼Œæ­£åœ¨ç”Ÿæˆå›ç­”..."
                if final_nodes
                else "æœªæ‰¾åˆ°é«˜ç›¸å…³æ€§èµ„æ–™ï¼ŒåŸºäºé€šç”¨çŸ¥è¯†å’Œå¯¹è¯å†å²å›ç­”..."
            )
            yield f"CONTENT:{status_msg}"
            full_response += status_msg + "\n"

            # 7. è°ƒç”¨ LLM
            assistant_response = ""
            for result in self._call_llm(llm, prompt_parts, enable_thinking=enable_thinking):
                # result æ˜¯å…ƒç»„ (prefix_type, content)
                prefix_type, chunk = result
                if prefix_type == 'THINK':
                    yield f"THINK:{chunk}"
                    # æ€è€ƒå†…å®¹ä¸è®¡å…¥ assistant_response
                elif prefix_type == 'CONTENT':
                    yield f"CONTENT:{chunk}"
                    full_response += chunk
                    assistant_response += chunk

            # 8. å­˜å‚¨æœ¬è½®å¯¹è¯åˆ°å‘é‡åº“
            context_doc_names = []
            if final_nodes:
                context_doc_names = [
                    node.node.metadata.get('file_name', 'æœªçŸ¥')
                    for node in final_nodes
                ]

            # è·å–ä¸Šä¸€è½®å¯¹è¯çš„ turn_id ä½œä¸º parent_turn_id
            parent_turn_id = None
            try:
                if recent_history:
                    parent_turn_id = recent_history[-1].get('turn_id')
            except Exception as e:
                logger.warning(f"è·å–çˆ¶å¯¹è¯IDå¤±è´¥: {e}")

            # ç”Ÿæˆå½“å‰è½®æ¬¡çš„ turn_id
            import uuid
            current_turn_id = str(uuid.uuid4())

            # å­˜å‚¨å¯¹è¯ï¼ˆåŒ…å«å®Œæ•´çš„åŠ©æ‰‹å›ç­”ï¼Œå…¶ä¸­å·²ç»åŒ…å«äº†å®ä½“å’ŒåŠ¨ä½œåˆ†æï¼‰
            conversation_manager.add_conversation_turn(
                session_id=session_id,
                user_query=question,
                assistant_response=assistant_response,
                turn_id=current_turn_id,
                parent_turn_id=parent_turn_id
            )
            
            # 7. æ”¶é›†å¹¶è¾“å‡ºå…¨å±€å…³é”®å­—ï¼ˆå»é‡åé™åˆ¶æ•°é‡ï¼‰
            # 6.1 æå–é—®é¢˜ä¸­çš„å…³é”®è¯
            import jieba
            all_keywords = list(jieba.lcut(question))
            # è¿‡æ»¤å•å­—å³å¯ï¼Œä¿ç•™æ‰€æœ‰å¤šå­—è¯
            question_keywords = [kw for kw in all_keywords if len(kw) > 1]
            
            
            # 7.2 æ”¶é›†æ–‡æ¡£åŒ¹é…çš„å…³é”®å­—
            global_keywords = []
            if final_nodes:
                for i, node in enumerate(final_nodes):
                    retrieval_sources = node.node.metadata.get('retrieval_sources', [])
                    if 'keyword' in retrieval_sources:
                        matched_keywords = node.node.metadata.get('bm25_matched_keywords', [])
                        global_keywords.extend(matched_keywords)
                    
            
            # 7.3 å»é‡é—®é¢˜å…³é”®è¯å’Œæ–‡æ¡£å…³é”®è¯
            # é—®é¢˜å…³é”®è¯å»é‡å¹¶ä¼˜å…ˆæ’åºï¼ˆä¸“ä¸šæœ¯è¯­ä¼˜å…ˆï¼‰
            seen_question = set()
            unique_question_keywords = []
            
            # ä¸“ä¸šæœ¯è¯­ä¼˜å…ˆåˆ—è¡¨ï¼ˆè¿™äº›è¯ä¼˜å…ˆæ˜¾ç¤ºï¼‰
            priority_terms = {'J2', 'J1', 'X1', 'X2', 'SLTD', 'APECå¡', 
                            'J2ç­¾è¯', 'J1ç­¾è¯', 'X1ç­¾è¯', 'X2ç­¾è¯',
                            'J2å­—ç­¾è¯', 'J1å­—ç­¾è¯', 'X1å­—ç­¾è¯', 'X2å­—ç­¾è¯'}
            
            # å…ˆæ·»åŠ ä¼˜å…ˆæœ¯è¯­
            for kw in question_keywords:
                if kw in priority_terms and kw not in seen_question:
                    seen_question.add(kw)
                    unique_question_keywords.append(kw)
            
            # å†æ·»åŠ å…¶ä»–å…³é”®è¯
            for kw in question_keywords:
                if kw not in seen_question:
                    seen_question.add(kw)
                    unique_question_keywords.append(kw)
            
            # æ–‡æ¡£å…³é”®è¯å»é‡ï¼ˆæ’é™¤å·²åœ¨é—®é¢˜ä¸­çš„ï¼‰
            seen_doc = set(unique_question_keywords)
            unique_doc_keywords = []
            for kw in global_keywords:
                if kw not in seen_doc:
                    seen_doc.add(kw)
                    unique_doc_keywords.append(kw)
            
            # é™åˆ¶æ•°é‡ï¼ˆä½¿ç”¨ MAX_DISPLAY_KEYWORDSï¼‰
            from config import Settings as AppSettings
            max_global_keywords = getattr(AppSettings, 'MAX_DISPLAY_KEYWORDS', 5)
            
            # åˆ†åˆ«é™åˆ¶é—®é¢˜å…³é”®è¯å’Œæ–‡æ¡£å…³é”®è¯
            final_question_keywords = unique_question_keywords[:max_global_keywords]
            remaining_slots = max_global_keywords - len(final_question_keywords)
            final_doc_keywords = unique_doc_keywords[:remaining_slots] if remaining_slots > 0 else []
            
            # è¾“å‡ºç»“æ„åŒ–å…³é”®å­—ï¼ˆåŒºåˆ†æ¥æºï¼‰
            keywords_data = {
                "question": final_question_keywords,
                "document": final_doc_keywords
            }
            if final_question_keywords or final_doc_keywords:
                yield f"KEYWORDS:{json.dumps(keywords_data, ensure_ascii=False)}"

            # 8. è¾“å‡ºå‚è€ƒæ¥æº
            if use_insert_block and filtered_results:
                yield "CONTENT:\n\n**å‚è€ƒæ¥æºï¼ˆå…¨éƒ¨æ£€ç´¢ç»“æœï¼‰:**"
                full_response += "\n\nå‚è€ƒæ¥æºï¼ˆå…¨éƒ¨æ£€ç´¢ç»“æœï¼‰:"

                for i, node in enumerate(final_nodes):
                    file_name = node.node.metadata.get('file_name', 'æœªçŸ¥')
                    initial_score = node.node.metadata.get('initial_score', 0.0)
                    key = f"{file_name}_{node.score}"

                    filtered_info = filtered_map.get(key)

                    # æå–æ£€ç´¢å…ƒæ•°æ®
                    retrieval_sources = node.node.metadata.get('retrieval_sources', [])
                    vector_score = node.node.metadata.get('vector_score', 0.0)
                    bm25_score = node.node.metadata.get('bm25_score', 0.0)
                    vector_rank = node.node.metadata.get('vector_rank')
                    bm25_rank = node.node.metadata.get('bm25_rank')

                    source_data = {
                        "id": i + 1,
                        "fileName": file_name,
                        "initialScore": f"{initial_score:.4f}",
                        "rerankedScore": f"{node.score:.4f}",
                        "content": node.node.text.strip(),
                        # æ£€ç´¢å…ƒæ•°æ®
                        "retrievalSources": retrieval_sources,
                        "vectorScore": f"{vector_score:.4f}",
                        "bm25Score": f"{bm25_score:.4f}",
                        # InsertBlock ç‰¹æœ‰å­—æ®µ
                        "canAnswer": filtered_info is not None,
                        "reasoning": filtered_info.get('reasoning', '') if filtered_info else '',
                        "keyPassage": filtered_info.get('key_passage', '') if filtered_info else ''
                    }
                    
                    # æ·»åŠ æ’åä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    if vector_rank is not None:
                        source_data['vectorRank'] = vector_rank
                    if bm25_rank is not None:
                        source_data['bm25Rank'] = bm25_rank
                    
                    # æ·»åŠ åŒ¹é…çš„å…³é”®è¯ï¼ˆBM25 æ£€ç´¢ï¼‰
                    if 'keyword' in retrieval_sources:
                        matched_keywords = node.node.metadata.get('bm25_matched_keywords', [])
                        if matched_keywords:
                            source_data['matchedKeywords'] = matched_keywords

                    yield f"SOURCE:{json.dumps(source_data, ensure_ascii=False)}"

                    full_response += (
                        f"\n[{source_data['id']}] æ–‡ä»¶: {source_data['fileName']}, "
                        f"é‡æ’åˆ†: {source_data['rerankedScore']}, "
                        f"å¯å›ç­”: {source_data['canAnswer']}"
                    )

            elif final_nodes:
                yield "CONTENT:\n\n**å‚è€ƒæ¥æº:**"
                full_response += "\n\nå‚è€ƒæ¥æº:"

                # åˆå¹¶éšè—èŠ‚ç‚¹ï¼ˆå¦‚æœæ˜¯ visible æ¨¡å¼ï¼‰
                nodes_to_display = final_nodes
                if hidden_nodes and Settings.HIDDEN_KB_INJECT_MODE == "visible":
                    logger.info(
                        f"[å¯¹è¯-éšè—çŸ¥è¯†åº“] åˆå¹¶ {len(hidden_nodes)} ä¸ªéšè—èŠ‚ç‚¹åˆ°å‚è€ƒæ¥æº | "
                        f"ä¸»çŸ¥è¯†åº“: {len(final_nodes)} æ¡ | "
                        f"éšè—èŠ‚ç‚¹: {len(hidden_nodes)} æ¡ (é¢å¤–æ˜¾ç¤ºï¼Œä¸å ç”¨ rerank_top_n) | "
                        f"æ€»è®¡: {len(final_nodes) + len(hidden_nodes)} æ¡"
                    )
                    nodes_to_display = final_nodes + hidden_nodes

                for i, node in enumerate(nodes_to_display):
                    # æ£€æŸ¥æ˜¯å¦ä¸ºéšè—èŠ‚ç‚¹
                    is_hidden = node.node.metadata.get('is_hidden', False)
                    
                    # æ„å»º source_data
                    source_data = {
                        "id": i + 1,
                        "fileName": node.node.metadata.get('file_name', 'æœªçŸ¥'),
                        "rerankedScore": f"{node.score:.4f}",
                        "content": node.node.text.strip()
                    }
                    
                    # å¦‚æœæ˜¯éšè—èŠ‚ç‚¹ï¼Œæ·»åŠ ç‰¹æ®Šæ ‡è®°
                    if is_hidden:
                        source_data['isHidden'] = True
                        source_data['hiddenKbName'] = node.node.metadata.get('hidden_kb_name', 'éšè—çŸ¥è¯†åº“')
                    
                    yield f"SOURCE:{json.dumps(source_data, ensure_ascii=False)}"
                    full_response += (
                        f"\n[{i + 1}] æ–‡ä»¶: {source_data['fileName']}, "
                        f"é‡æ’åˆ†: {node.score}"
                    )

            # æ ¼å¼åŒ–æ ¡éªŒå’Œä¿®å¤ï¼ˆåœ¨æœ€åä¸€æ¬¡ yield å‰ï¼‰
            from utils.response_formatter import response_formatter
            
            # æå– assistant_response è¿›è¡Œæ ¼å¼åŒ–ï¼ˆè¿™æ˜¯å®é™…çš„ LLM å›ç­”ï¼‰
            if assistant_response:
                validated_response = response_formatter.process_response(
                    assistant_response.strip(),
                    question=question
                )
                
                # å¦‚æœæ ¼å¼è¢«ä¿®å¤ï¼Œè®°å½•æ—¥å¿—
                if validated_response != assistant_response.strip():
                    logger.info("[å¯¹è¯-æ ¼å¼ä¿®å¤] å“åº”æ ¼å¼å·²è‡ªåŠ¨ä¿®å¤")

            yield "DONE:"

        except Exception as e:
            error_msg = f"å¤„ç†é”™è¯¯: {str(e)}"
            logger.error(f"å¤šè½®å¯¹è¯å¤„ç†å‡ºé”™: {e}", exc_info=True)
            yield f"ERROR:{error_msg}"
            # ç¡®ä¿å‘é€ DONE ä¿¡å·ï¼Œé¿å…å‰ç«¯ç­‰å¾…è¶…æ—¶
            yield "DONE:"

    def _build_prompt_with_history(
        self,
        question: str,
        enable_thinking: bool,
        final_nodes,
        filtered_results=None,
        recent_history=None,
        relevant_history=None,
        history_summary=None,
        hidden_nodes=None
    ):
        """
        æ„é€ å¸¦å†å²å¯¹è¯çš„æç¤ºè¯ï¼ˆä½¿ç”¨çŸ¥è¯†é—®ç­”çš„æç¤ºè¯æ ¼å¼ï¼‰

        Args:
            question: å½“å‰é—®é¢˜
            enable_thinking: æ˜¯å¦å¯ç”¨æ€è€ƒæ¨¡å¼
            final_nodes: æ£€ç´¢åˆ°çš„èŠ‚ç‚¹
            filtered_results: InsertBlock è¿‡æ»¤ç»“æœ
            recent_history: æœ€è¿‘çš„å¯¹è¯å†å²
            relevant_history: ç›¸å…³çš„å†å²å¯¹è¯
            history_summary: å†å²å¯¹è¯æ‘˜è¦
            hidden_nodes: éšè—çŸ¥è¯†åº“èŠ‚ç‚¹ï¼ˆä¸æ˜¾ç¤ºæ¥æºï¼‰
        """
        # 1. æ„å»ºéšè—çŸ¥è¯†åº“ä¸Šä¸‹æ–‡ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        hidden_context = None
        if hidden_nodes:
            from utils.knowledge_utils.context_builder import build_hidden_kb_context
            hidden_context = build_hidden_kb_context(hidden_nodes)
        
        # 2. æ„å»ºçŸ¥è¯†åº“ä¸Šä¸‹æ–‡ï¼ˆä¸çŸ¥è¯†é—®ç­”ç›¸åŒçš„é€»è¾‘ï¼‰
        knowledge_context = None
        if filtered_results:
            # ä½¿ç”¨ InsertBlock è¿‡æ»¤ç»“æœ
            context_blocks = []
            block_index = 1
            
            for result in filtered_results:
                file_name = result['file_name']
                key_passage = result.get('key_passage', '')
                full_content = result['node'].node.text.strip()
                can_answer = result.get('can_answer', False)
                
                # ä¸¥æ ¼è¿‡æ»¤ï¼šåªæœ‰ can_answer=True ä¸” key_passage ä¸ä¸ºç©ºæ‰æ³¨å…¥ä¸Šä¸‹æ–‡
                if not can_answer:
                    logger.warning(f"[å¯¹è¯-ç²¾å‡†æ£€ç´¢è¿‡æ»¤] è·³è¿‡ä¸å¯å›ç­”çš„èŠ‚ç‚¹: {file_name}")
                    continue
                
                if not key_passage or key_passage.strip() == "":
                    logger.warning(f"[å¯¹è¯-ç²¾å‡†æ£€ç´¢è¿‡æ»¤] è·³è¿‡æ— å…³é”®æ®µè½çš„èŠ‚ç‚¹: {file_name} | can_answer={can_answer}")
                    continue
                
                block = f"ã€ä¸šåŠ¡è§„å®š {block_index}ã€‘æ¥æº: {file_name}\n{full_content}"
                context_blocks.append(block)
                block_index += 1
                logger.info(f"[å¯¹è¯-ç²¾å‡†æ£€ç´¢é€šè¿‡] èŠ‚ç‚¹å·²æ³¨å…¥ä¸Šä¸‹æ–‡: {file_name} | å…³é”®æ®µè½é•¿åº¦: {len(key_passage)}")
                
            knowledge_context = "\n\n".join(context_blocks) if context_blocks else None

        elif final_nodes:
            # ä½¿ç”¨æ™®é€šæ£€ç´¢ç»“æœ
            context_blocks = []
            for i, node in enumerate(final_nodes):
                file_name = node.node.metadata.get('file_name', 'æœªçŸ¥æ–‡ä»¶')
                content = node.node.get_content().strip()
                block = f"ã€ä¸šåŠ¡è§„å®š {i + 1}ã€‘æ¥æº: {file_name}\n{content}"
                context_blocks.append(block)
            knowledge_context = "\n\n".join(context_blocks)

        has_rag = bool(knowledge_context)
        
        # å¦‚æœæœ‰å­é—®é¢˜ç­”æ¡ˆåˆæˆï¼Œæ·»åŠ åˆ°çŸ¥è¯†åº“ä¸Šä¸‹æ–‡ä¸­ï¼ˆä½¿ç”¨ç®€æ´æ ¼å¼ï¼‰
        if has_rag and self._last_synthesized_answer:
            synthesis_block = (
                f"\n\nã€å­é—®é¢˜ç»¼åˆåˆ†æã€‘\n"
                f"{self._last_synthesized_answer}\n\n"
                f"æ³¨æ„: ä»¥ä¸Šæ˜¯å¯¹å¤šä¸ªå­é—®é¢˜ç­”æ¡ˆçš„ç»¼åˆæ•´ç†ï¼Œè¯·ç»“åˆå…·ä½“ä¸šåŠ¡è§„å®šç»™å‡ºæœ€ç»ˆå›ç­”ã€‚"
            )
            knowledge_context += synthesis_block
            logger.info(f"[å¤šè½®æç¤ºè¯æ„å»º] å·²å°†åˆæˆç­”æ¡ˆæ³¨å…¥ä¸Šä¸‹æ–‡ | é•¿åº¦: {len(self._last_synthesized_answer)}")

        # æ„å»ºå†å²å¯¹è¯ä¸Šä¸‹æ–‡
        history_context = None
        if history_summary or recent_history or relevant_history:
            history_parts = []

            # æ·»åŠ æ‘˜è¦
            if history_summary:
                summary_prefix = get_conversation_summary_context_prefix()
                history_parts.append(f"{summary_prefix}{history_summary}")

            # æ·»åŠ æœ€è¿‘çš„å¯¹è¯
            if recent_history:
                recent_prefix = get_conversation_context_prefix_recent_history()
                recent_turns_text = "\n\n".join([
                    f"ç”¨æˆ·: {turn['user_query']}\nåŠ©æ‰‹: {turn['assistant_response']}"
                    for turn in recent_history
                ])
                history_parts.append(f"{recent_prefix}{recent_turns_text}")

            # æ·»åŠ ç›¸å…³å†å²å¯¹è¯
            if relevant_history:
                relevant_prefix = get_conversation_context_prefix_relevant_history()
                relevant_turns_text = "\n\n".join([
                    f"ç”¨æˆ·: {turn['user_query']}\nåŠ©æ‰‹: {turn['assistant_response']}"
                    for turn in relevant_history
                ])
                history_parts.append(f"{relevant_prefix}{relevant_turns_text}")

            history_context = "\n\n".join(history_parts)

        # ä½¿ç”¨çŸ¥è¯†é—®ç­”çš„æç¤ºè¯é€»è¾‘
        if has_rag or hidden_context:
            # è·å–å‰ç¼€
            assistant_prefix = get_knowledge_assistant_context_prefix()

            # ç»„åˆä¸Šä¸‹æ–‡ï¼šéšè—çŸ¥è¯†åº“ + å†å²å¯¹è¯ + ä¸šåŠ¡è§„å®š
            context_parts = []
            
            # 1. éšè—çŸ¥è¯†åº“ï¼ˆæœ€ä¼˜å…ˆï¼‰
            if hidden_context:
                context_parts.append(hidden_context)
            
            # 2. å†å²å¯¹è¯
            if history_context:
                context_parts.append(history_context)
            
            # 3. ä¸šåŠ¡è§„å®š
            if knowledge_context:
                context_parts.append(assistant_prefix + knowledge_context)
            elif hidden_context and not knowledge_context:
                # åªæœ‰éšè—çŸ¥è¯†åº“ï¼Œæ²¡æœ‰æ™®é€šçŸ¥è¯†åº“
                context_parts.append(assistant_prefix + "ï¼ˆå·²æ³¨å…¥å†…éƒ¨å‚è€ƒèµ„æ–™ï¼‰")

            assistant_context = "\n\n---\n\n".join(context_parts)

            # æ ¹æ®æ€è€ƒæ¨¡å¼é€‰æ‹©ä¸åŒçš„ system å’Œ user prompt
            if enable_thinking:
                system_prompt = get_knowledge_system_rag_advanced()
                user_template = get_knowledge_user_rag_advanced()
            else:
                system_prompt = get_knowledge_system_rag_simple()
                user_template = get_knowledge_user_rag_simple()

            # user_template æ˜¯åˆ—è¡¨ï¼Œéœ€è¦ join åå† format
            user_prompt_str = "\n".join(user_template) if isinstance(user_template, list) else user_template
            # å¦‚æœå…³é—­æ€è€ƒæ¨¡å¼ï¼Œè‡ªåŠ¨åœ¨é—®é¢˜åè¿½åŠ  /no_think æŒ‡ä»¤ï¼ˆé˜¿é‡Œäº‘æ–‡æ¡£å»ºè®®ï¼‰
            actual_question = f"{question}/no_think" if not enable_thinking else question
            user_prompt = user_prompt_str.format(context=assistant_context, question=actual_question)

        else:
            # æ²¡æœ‰æ£€ç´¢åˆ°ç›¸å…³å†…å®¹ï¼Œåªæœ‰å†å²å¯¹è¯
            assistant_context = history_context

            if enable_thinking:
                system_prompt = get_knowledge_system_no_rag_think()
                user_template = get_knowledge_user_no_rag_think()
            else:
                system_prompt = get_knowledge_system_no_rag_simple()
                user_template = get_knowledge_user_no_rag_simple()

            # user_template å¯èƒ½æ˜¯åˆ—è¡¨æˆ–å­—ç¬¦ä¸²
            user_prompt_str = "\n".join(user_template) if isinstance(user_template, list) else user_template
            # å¦‚æœå…³é—­æ€è€ƒæ¨¡å¼ï¼Œè‡ªåŠ¨åœ¨é—®é¢˜åè¿½åŠ  /no_think æŒ‡ä»¤ï¼ˆé˜¿é‡Œäº‘æ–‡æ¡£å»ºè®®ï¼‰
            actual_question = f"{question}/no_think" if not enable_thinking else question
            user_prompt = user_prompt_str.format(question=actual_question)

        # system_prompt å¯èƒ½æ˜¯åˆ—è¡¨ï¼Œéœ€è¦è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        if isinstance(system_prompt, list):
            system_prompt = "\n".join(system_prompt)

        # æ„å»º fallback_promptï¼ˆç”¨äºä¸æ”¯æŒ chat æ¨¡å¼çš„æƒ…å†µï¼‰
        fallback_parts = [system_prompt]
        if assistant_context:
            fallback_parts.append(assistant_context)
        fallback_parts.append(user_prompt)

        return {
            "system_prompt": system_prompt,
            "user_prompt": user_prompt,
            "assistant_context": assistant_context,
            "fallback_prompt": "\n\n".join(fallback_parts)
        }
    
    def _smart_retrieve_and_rerank(self, question: str, rerank_top_n: int, conversation_history: Optional[List[Dict]] = None):
        """
        æ™ºèƒ½è·¯ç”±æ£€ç´¢ï¼šå…ˆæ„å›¾åˆ†ç±»é€‰æ‹©çŸ¥è¯†åº“ï¼Œå†å¯é€‰å­é—®é¢˜åˆ†è§£
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            rerank_top_n: é‡æ’åºåè¿”å›çš„æ–‡æ¡£æ•°é‡
            conversation_history: å¯¹è¯å†å²ï¼ˆç”¨äºå­é—®é¢˜åˆ†è§£ï¼‰
            
        Returns:
            é‡æ’åºåçš„èŠ‚ç‚¹åˆ—è¡¨
        """
        # 1. æ„å›¾åˆ†ç±»ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        strategy = "general"  # é»˜è®¤ç­–ç•¥ï¼šåªç”¨é€šç”¨åº“
        
        if self.intent_classifier:
            try:
                strategy = self.intent_classifier.classify(question)
                logger.info(f"[æ™ºèƒ½è·¯ç”±] æ„å›¾åˆ†ç±»ç»“æœ: {strategy}")
            except Exception as e:
                logger.warning(f"[æ™ºèƒ½è·¯ç”±] æ„å›¾åˆ†ç±»å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥: general")
                strategy = "general"
        else:
            logger.info("[æ™ºèƒ½è·¯ç”±] æ„å›¾åˆ†ç±»å™¨æœªå¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥: general")
        
        # 2. æ ¹æ®ç­–ç•¥é€‰æ‹©æ£€ç´¢å™¨ï¼ˆç¡®ä¿é€šç”¨åº“å§‹ç»ˆè¢«æ£€ç´¢ï¼‰
        selected_retriever = None
        if strategy == "airline_visa_free" and self.multi_kb_retriever:
            # ä¸‰åº“æ£€ç´¢ï¼ˆèˆªå¸åº“ + å…ç­¾åº“ + é€šç”¨åº“ï¼‰
            logger.info("[æ™ºèƒ½è·¯ç”±] ç­–ç•¥: airline_visa_free â†’ ä¸‰åº“æ£€ç´¢ï¼ˆèˆªå¸åº“ + å…ç­¾åº“ + é€šç”¨åº“ï¼‰")
            selected_retriever = self.multi_kb_retriever
        elif strategy == "visa_free" and self.multi_kb_retriever:
            # åŒåº“æ£€ç´¢ï¼ˆå…ç­¾åº“ + é€šç”¨åº“ï¼‰
            logger.info("[æ™ºèƒ½è·¯ç”±] ç­–ç•¥: visa_free â†’ åŒåº“æ£€ç´¢ï¼ˆå…ç­¾åº“ + é€šç”¨åº“ï¼‰")
            selected_retriever = self.multi_kb_retriever
        elif strategy == "airline" and self.multi_kb_retriever:
            # åŒåº“æ£€ç´¢ï¼ˆèˆªå¸åº“ + é€šç”¨åº“ï¼‰
            logger.info("[æ™ºèƒ½è·¯ç”±] ç­–ç•¥: airline â†’ åŒåº“æ£€ç´¢ï¼ˆèˆªå¸åº“ + é€šç”¨åº“ï¼‰")
            selected_retriever = self.multi_kb_retriever
        else:
            # åªç”¨é€šç”¨åº“ï¼ˆé»˜è®¤ï¼‰
            logger.info("[æ™ºèƒ½è·¯ç”±] ç­–ç•¥: general â†’ ä»…é€šç”¨åº“")
            selected_retriever = self.retriever
            strategy = "general"  # ç¡®ä¿ strategy ä¸º general
        
        # 3. å°è¯•å­é—®é¢˜åˆ†è§£ï¼ˆå¦‚æœå¯ç”¨ï¼‰ï¼Œä½¿ç”¨è·¯ç”±åçš„æ£€ç´¢å™¨
        if self.sub_question_decomposer and self.sub_question_decomposer.enabled:
            logger.info(f"[æ£€ç´¢ç­–ç•¥] å°è¯•ä½¿ç”¨å­é—®é¢˜åˆ†è§£æ£€ç´¢ï¼ˆå•è½®ï¼‰ | ç›®æ ‡åº“: {strategy}")
            try:
                nodes, metadata = self.sub_question_decomposer.retrieve_with_decomposition(
                    query=question,
                    rerank_top_n=rerank_top_n,
                    conversation_history=conversation_history,
                    retriever=selected_retriever  # ä¼ å…¥è·¯ç”±åçš„æ£€ç´¢å™¨
                )
                
                # è®°å½•åˆ†è§£å…ƒæ•°æ®
                if metadata.get('decomposed'):
                    logger.info(
                        f"[å­é—®é¢˜æ£€ç´¢] åˆ†è§£æ£€ç´¢å®Œæˆ | "
                        f"å­é—®é¢˜æ•°: {len(metadata['sub_questions'])} | "
                        f"è¿”å›èŠ‚ç‚¹æ•°: {len(nodes)} | "
                        f"ä½¿ç”¨åº“: {strategy}"
                    )
                    
                    # ä¿å­˜å­é—®é¢˜ç­”æ¡ˆï¼ˆç”¨äºæ³¨å…¥ä¸Šä¸‹æ–‡å’Œè¿”å›å‰ç«¯ï¼‰
                    if metadata.get('sub_answers') and len(metadata['sub_answers']) > 0:
                        # å­˜å‚¨å­é—®é¢˜ç­”æ¡ˆï¼Œä¾› _build_prompt ä½¿ç”¨
                        self._last_sub_answers = metadata['sub_answers']
                        logger.info(f"[å­é—®é¢˜ç­”æ¡ˆ] å·²ä¿å­˜ {len(metadata['sub_answers'])} ä¸ªå­é—®é¢˜ç­”æ¡ˆï¼Œå°†æ³¨å…¥ä¸Šä¸‹æ–‡")
                        
                        # å¯é€‰ï¼šç”Ÿæˆå­é—®é¢˜ç­”æ¡ˆåˆæˆ
                        try:
                            synthesized_answer = self.sub_question_decomposer.synthesize_answer(
                                original_query=question,
                                sub_answers=metadata['sub_answers']
                            )
                            if synthesized_answer:
                                # å°†åˆæˆç­”æ¡ˆæ·»åŠ åˆ°metadataï¼Œä¾›åç»­ä½¿ç”¨
                                metadata['synthesized_answer'] = synthesized_answer
                                # å­˜å‚¨ä¸ºå®ä¾‹å˜é‡ï¼Œä¾›_build_promptä½¿ç”¨
                                self._last_synthesized_answer = synthesized_answer
                                logger.info(f"[ç­”æ¡ˆåˆæˆ] å·²ç”Ÿæˆåˆæˆç­”æ¡ˆ | é•¿åº¦: {len(synthesized_answer)}")
                        except Exception as synth_e:
                            logger.warning(f"[ç­”æ¡ˆåˆæˆ] åˆæˆå¤±è´¥: {synth_e}")
                    
                    # è¿”å›èŠ‚ç‚¹å’Œå…ƒæ•°æ®
                    return nodes, metadata
                else:
                    logger.info("[å­é—®é¢˜æ£€ç´¢] æœªåˆ†è§£ï¼Œç»§ç»­æ ‡å‡†æ£€ç´¢æµç¨‹")
                    # æ¸…ç©ºå­é—®é¢˜ç­”æ¡ˆï¼Œé¿å…ä½¿ç”¨æ—§æ•°æ®
                    self._last_sub_answers = None
                    # ç»§ç»­æ‰§è¡Œæ ‡å‡†æ£€ç´¢
                    
            except Exception as e:
                logger.error(f"[å­é—®é¢˜æ£€ç´¢] åˆ†è§£æ£€ç´¢å¤±è´¥: {e}", exc_info=True)
                logger.info("[å­é—®é¢˜æ£€ç´¢] å›é€€åˆ°æ ‡å‡†æ£€ç´¢æµç¨‹")
                # æ¸…ç©ºå­é—®é¢˜ç­”æ¡ˆï¼Œé¿å…ä½¿ç”¨æ—§æ•°æ®
                self._last_sub_answers = None
                # ç»§ç»­æ‰§è¡Œæ ‡å‡†æ£€ç´¢
        
        # 4. æ ‡å‡†æ£€ç´¢å’Œé‡æ’åºï¼ˆä¼ å…¥ strategyï¼‰
        return self._retrieve_and_rerank_with_retriever(
            question, 
            rerank_top_n, 
            selected_retriever,
            strategy=strategy  # ä¼ å…¥ç­–ç•¥
        )
    
    def _retrieve_and_rerank_with_retriever(
        self, 
        question: str, 
        rerank_top_n: int,
        retriever,
        strategy: str = "general"
    ):
        """
        ä½¿ç”¨æŒ‡å®šæ£€ç´¢å™¨è¿›è¡Œæ£€ç´¢å’Œé‡æ’åºï¼ˆæ”¯æŒ Keyword Table Fallbackï¼‰
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            rerank_top_n: é‡æ’åºåè¿”å›çš„æ–‡æ¡£æ•°é‡ï¼ˆä»…å¯¹generalç­–ç•¥ç”Ÿæ•ˆï¼‰
            retriever: æ£€ç´¢å™¨å®ä¾‹
            strategy: æ£€ç´¢ç­–ç•¥ï¼ˆairline_visa_free/visa_free/airline/generalï¼‰
            
        Returns:
            é‡æ’åºåçš„èŠ‚ç‚¹åˆ—è¡¨
            
        Note:
            - generalç­–ç•¥ï¼šä½¿ç”¨å‰ç«¯ä¼ å…¥çš„rerank_top_nå‚æ•°
            - å…¶ä»–ç­–ç•¥ï¼šä½¿ç”¨å›ºå®šé…ç½®çš„è¿”å›æ•°é‡ï¼Œå¿½ç•¥rerank_top_nå‚æ•°
        """
        # åˆ›å»º QueryBundleï¼ˆé‡æ’åºéœ€è¦ï¼‰
        query_bundle = QueryBundle(query_str=question)
        
        # BM25 + å‘é‡æ£€ç´¢èåˆç­–ç•¥
        retrieved_nodes = []
        retrieval_mode = "bm25_vector_fusion"
        
        from core.multi_kb_retriever import MultiKBRetriever
        
        # æ‰§è¡Œ BM25 + å‘é‡æ··åˆæ£€ç´¢ï¼ˆæ ¹æ®ç­–ç•¥é€‰æ‹©æ–¹æ³•ï¼‰
        logger.info(f"[èåˆç­–ç•¥] BM25 + å‘é‡æ£€ç´¢ | ç­–ç•¥: {strategy}")
        if isinstance(retriever, MultiKBRetriever):
            # æ ¹æ®ç­–ç•¥è°ƒç”¨ä¸åŒçš„æ£€ç´¢æ–¹æ³•
            if strategy == "airline_visa_free":
                # ä¸‰åº“æ£€ç´¢
                retrieved_nodes = retriever.retrieve_from_all_three(question)
            elif strategy == "visa_free":
                # å…ç­¾åº“ + é€šç”¨åº“
                retrieved_nodes = retriever.retrieve_from_both(question)
            elif strategy == "airline":
                # èˆªå¸åº“ + é€šç”¨åº“
                retrieved_nodes = retriever.retrieve_airline_only(question)
            else:
                # é»˜è®¤ï¼šæ ¹æ®å¯ç”¨æ£€ç´¢å™¨è‡ªåŠ¨é€‰æ‹©
                retrieved_nodes = retriever.retrieve(question)
        else:
            retrieved_nodes = retriever.retrieve(query_bundle)
        
        logger.info(f"[BM25+å‘é‡æ£€ç´¢] è¿”å› {len(retrieved_nodes)} ä¸ªèŠ‚ç‚¹")
        
        logger.info(f"æ£€ç´¢å®Œæˆ | æ¨¡å¼: {retrieval_mode} | ç»“æœæ•°: {len(retrieved_nodes)}")
        
        if not retrieved_nodes:
            logger.warning("æœªæ£€ç´¢åˆ°ä»»ä½•ç›¸å…³æ–‡æ¡£")
            return []
        
        # æ·»åŠ æ—¥å¿—ï¼šæ£€ç´¢åçš„åˆ†æ•°
        if retrieved_nodes:
            retrieval_scores = [n.score for n in retrieved_nodes[:5]]
            logger.info(f"æ£€ç´¢é˜¶æ®µTop5å¾—åˆ†: {[f'{s:.4f}' for s in retrieval_scores]}")
        
        # ä¿å­˜åŸå§‹èŠ‚ç‚¹çš„æ£€ç´¢å…ƒæ•°æ®ï¼ˆé‡æ’åºå¯èƒ½ä¼šä¸¢å¤±ï¼‰
        original_metadata = {}
        for node in retrieved_nodes:
            node_id = node.node.node_id
            original_metadata[node_id] = {
                'retrieval_sources': node.node.metadata.get('retrieval_sources', []),
                'vector_score': node.node.metadata.get('vector_score', 0.0),
                'bm25_score': node.node.metadata.get('bm25_score', 0.0),
                'bm25_matched_keywords': node.node.metadata.get('bm25_matched_keywords', []),
                'bm25_query_keywords': node.node.metadata.get('bm25_query_keywords', []),
                'vector_rank': node.node.metadata.get('vector_rank'),
                'bm25_rank': node.node.metadata.get('bm25_rank'),
                'initial_score': node.node.metadata.get('initial_score', node.score)
            }
        
        # é‡æ’åº
        reranked_nodes = self.reranker.postprocess_nodes(
            retrieved_nodes,
            query_bundle=query_bundle
        )
        
        logger.info(f"é‡æ’åºåä¿ç•™ {len(reranked_nodes)} ä¸ªç»“æœ")
        
        # æ¢å¤åŸå§‹èŠ‚ç‚¹çš„æ£€ç´¢å…ƒæ•°æ®
        for node in reranked_nodes:
            node_id = node.node.node_id
            if node_id in original_metadata:
                metadata = original_metadata[node_id]
                node.node.metadata.update(metadata)
        
        logger.info(f"å·²æ¢å¤ {len([n for n in reranked_nodes if n.node.metadata.get('retrieval_sources')])} ä¸ªèŠ‚ç‚¹çš„æ£€ç´¢å…ƒæ•°æ®")
        
        # æ·»åŠ æ—¥å¿—ï¼šé‡æ’åºåçš„åˆ†æ•°
        if reranked_nodes:
            rerank_scores = [n.score for n in reranked_nodes[:5]]
            logger.info(f"é‡æ’åºé˜¶æ®µTop5å¾—åˆ†: {[f'{s:.4f}' for s in rerank_scores]}")
        
        # æ–¹æ¡ˆ1+3ç»„åˆï¼šæŒ‰å¾—åˆ†æ’åºåä¸¥æ ¼æˆªæ–­
        # ç¡®ä¿æŒ‰åˆ†æ•°ä»é«˜åˆ°ä½æ’åº
        reranked_nodes.sort(key=lambda x: x.score, reverse=True)
        
        # æ ¹æ®ç­–ç•¥å†³å®šè¿”å›æ•°é‡
        if strategy == "airline_visa_free":
            # ä¸‰åº“æ£€ç´¢ï¼šå›ºå®šè¿”å›20æ¡
            final_count = Settings.AIRLINE_VISA_FREE_RETURN_COUNT
            logger.info(f"[ä¸‰åº“æ£€ç´¢] ä½¿ç”¨å›ºå®šè¿”å›æ•°é‡: {final_count}æ¡ï¼ˆä¸å—å‰ç«¯å‚æ•°æ§åˆ¶ï¼‰")
        elif strategy == "visa_free":
            # å…ç­¾åº“+é€šç”¨åº“ï¼šå›ºå®šè¿”å›15æ¡
            final_count = Settings.VISA_FREE_STRATEGY_RETURN_COUNT
            logger.info(f"[å…ç­¾æ£€ç´¢] ä½¿ç”¨å›ºå®šè¿”å›æ•°é‡: {final_count}æ¡ï¼ˆä¸å—å‰ç«¯å‚æ•°æ§åˆ¶ï¼‰")
        elif strategy == "airline":
            # èˆªå¸åº“+é€šç”¨åº“ï¼šå›ºå®šè¿”å›15æ¡
            final_count = Settings.AIRLINE_STRATEGY_RETURN_COUNT
            logger.info(f"[èˆªå¸æ£€ç´¢] ä½¿ç”¨å›ºå®šè¿”å›æ•°é‡: {final_count}æ¡ï¼ˆä¸å—å‰ç«¯å‚æ•°æ§åˆ¶ï¼‰")
        else:
            # é€šç”¨é—®é¢˜ï¼šä½¿ç”¨å‰ç«¯å‚æ•°
            final_count = rerank_top_n
            logger.info(f"[é€šç”¨æ£€ç´¢] ä½¿ç”¨å‰ç«¯å‚æ•°: {final_count}æ¡")
        
        # æˆªæ–­åˆ°ç›®æ ‡æ•°é‡
        final_nodes = reranked_nodes[:final_count]
        
        if final_nodes:
            logger.info(
                f"æœ€ç»ˆè¿”å› {len(final_nodes)} ä¸ªæ–‡æ¡£ï¼ˆç­–ç•¥: {strategy}, ç›®æ ‡æ•°é‡: {final_count}ï¼‰ | "
                f"æœ€é«˜åˆ†: {final_nodes[0].score:.4f} | "
                f"æœ€ä½åˆ†: {final_nodes[-1].score:.4f}"
            )
        
        return final_nodes
    

    def debug_inspect_scores(
        self,
        question: str,
        *,
        retriever=None,
        match_substring: Optional[str] = None,
        match_node_id: Optional[str] = None,
        max_candidates: int = 50,
        include_full_text: bool = False,
        run_reranker: bool = True
    ) -> Dict[str, Any]:
        """
        è°ƒè¯•è¾…åŠ©ï¼šæŸ¥çœ‹æ£€ç´¢/é‡æ’åºé˜¶æ®µçš„èŠ‚ç‚¹å¾—åˆ†ã€‚
        ä»…åœ¨æ˜¾å¼è°ƒç”¨æ—¶æ‰§è¡Œï¼Œä¸å½±å“ç°æœ‰æµç¨‹ã€‚
        """
        if not question:
            raise ValueError("question ä¸èƒ½ä¸ºç©º")

        active_retriever = retriever or self.retriever
        if active_retriever is None:
            raise RuntimeError("æœªé…ç½®æ£€ç´¢å™¨ï¼Œæ— æ³•æ‰§è¡Œè°ƒè¯•")

        if run_reranker and self.reranker is None:
            raise RuntimeError("æœªé…ç½®é‡æ’å™¨ï¼Œæ— æ³•æ‰§è¡Œè°ƒè¯•")

        query_bundle = QueryBundle(query_str=question)

        def _execute_retriever() -> List[Any]:
            """å…¼å®¹å¤šçŸ¥è¯†åº“æ£€ç´¢å™¨çš„è°ƒç”¨æ–¹å¼"""
            try:
                from core.multi_kb_retriever import MultiKBRetriever
            except ImportError:
                MultiKBRetriever = None  # type: ignore

            if MultiKBRetriever and isinstance(active_retriever, MultiKBRetriever):
                return active_retriever.retrieve_from_all_three(question)

            return active_retriever.retrieve(query_bundle)

        def _serialize_nodes(nodes: List[Any], stage: str) -> List[Dict[str, Any]]:
            serialized = []
            for idx, node_score in enumerate(nodes[:max_candidates], start=1):
                node = node_score.node
                metadata = node.metadata or {}
                text = node.get_content()
                preview = text[:120].replace("\n", " ").strip()
                vector_rank = metadata.get("vector_rank")
                bm25_rank = metadata.get("bm25_rank")
                sources = metadata.get("retrieval_sources") or []
                source_label = "/".join(sources) if sources else "unknown"

                entry = {
                    "stage": stage,
                    "rank": idx,
                    "node_id": node.node_id,
                    "score": float(node_score.score or 0.0),
                    "vector_score": float(metadata.get("vector_score", 0.0)),
                    "bm25_score": float(metadata.get("bm25_score", 0.0)),
                    "vector_rank": vector_rank,
                    "bm25_rank": bm25_rank,
                    "sources": sources,
                    "source_label": source_label,
                    "file_name": metadata.get("file_name"),
                    "file_path": metadata.get("file_path"),
                    "text_preview": preview,
                    "metadata": metadata
                }

                if include_full_text:
                    entry["text"] = text

                serialized.append(entry)

            return serialized

        def _is_match(entry: Dict[str, Any]) -> bool:
            if not match_substring and not match_node_id:
                return False

            matched = True

            if match_substring:
                needle = match_substring.lower()
                haystack = [
                    (entry.get("text_preview") or "").lower(),
                    (entry.get("file_name") or "").lower(),
                ]
                if include_full_text:
                    haystack.append((entry.get("text") or "").lower())
                matched = any(needle in segment for segment in haystack)

            if matched and match_node_id:
                matched = match_node_id in (entry.get("node_id") or "")

            return matched

        retrieved_nodes = _execute_retriever() or []
        retrieval_serialized = _serialize_nodes(retrieved_nodes, stage="retrieval")

        rerank_serialized: List[Dict[str, Any]] = []
        if run_reranker and retrieved_nodes:
            reranked = self.reranker.postprocess_nodes(
                retrieved_nodes,
                query_bundle=query_bundle
            )
            reranked.sort(key=lambda x: x.score, reverse=True)
            rerank_serialized = _serialize_nodes(reranked, stage="rerank")

        matched_entries = []
        for entry in retrieval_serialized + rerank_serialized:
            if _is_match(entry):
                matched_entries.append(entry)

        return {
            "question": question,
            "retriever_type": type(active_retriever).__name__,
            "retrieval": retrieval_serialized,
            "rerank": rerank_serialized,
            "matches": matched_entries,
            "match_conditions": {
                "substring": match_substring,
                "node_id": match_node_id
            }
        }
