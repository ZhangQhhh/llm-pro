#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
éšè—çŸ¥è¯†åº“æ—¥å¿—åˆ†æè„šæœ¬
ç”¨äºåˆ†æéšè—çŸ¥è¯†åº“çš„æ£€ç´¢å’Œè°ƒç”¨æƒ…å†µ
"""
import os
import json
import datetime
from typing import Dict, List, Any
from collections import defaultdict, Counter


def load_json_logs(date_str: str = None) -> List[Dict]:
    """åŠ è½½æŒ‡å®šæ—¥æœŸçš„ JSON æ—¥å¿—"""
    if date_str is None:
        date_str = datetime.datetime.now().strftime("%Y-%m-%d")
    
    json_file = f"logs/hidden_logs/hidden_kb_{date_str}.json"
    
    if not os.path.exists(json_file):
        print(f"âŒ æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {json_file}")
        return []
    
    logs = []
    try:
        with open(json_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    logs.append(json.loads(line))
    except Exception as e:
        print(f"âŒ è¯»å–æ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
        return []
    
    return logs


def analyze_daily_stats(logs: List[Dict]) -> Dict[str, Any]:
    """åˆ†ææ¯æ—¥ç»Ÿè®¡"""
    stats = {
        "æ€»æ£€ç´¢æ¬¡æ•°": 0,
        "æˆåŠŸæ£€ç´¢æ¬¡æ•°": 0,
        "æ— ç»“æœæ¬¡æ•°": 0,
        "æ€»æ³¨å…¥æ¬¡æ•°": 0,
        "å¹³å‡æ£€ç´¢åˆ†æ•°": 0,
        "æœ€é«˜æ£€ç´¢åˆ†æ•°": 0,
        "æœ€å¸¸æŸ¥è¯¢": [],
        "æ£€ç´¢æ—¶é—´åˆ†å¸ƒ": defaultdict(int)
    }
    
    queries = []
    all_scores = []
    
    for log in logs:
        log_type = log.get("type", "")
        
        if log_type == "retrieval_start":
            stats["æ€»æ£€ç´¢æ¬¡æ•°"] += 1
            queries.append(log.get("query", ""))
            
            # æŒ‰å°æ—¶ç»Ÿè®¡
            timestamp = log.get("timestamp", "")
            if timestamp:
                try:
                    dt = datetime.datetime.fromisoformat(timestamp.replace('Z', '+00:00'))
                    hour = dt.hour
                    stats["æ£€ç´¢æ—¶é—´åˆ†å¸ƒ"][hour] += 1
                except:
                    pass
        
        elif log_type == "retrieval_result":
            result_count = log.get("result_count", 0)
            if result_count > 0:
                stats["æˆåŠŸæ£€ç´¢æ¬¡æ•°"] += 1
                # æå–åˆ†æ•°
                nodes = log.get("nodes", [])
                for node in nodes:
                    score = node.get("score", 0)
                    all_scores.append(score)
                    if score > stats["æœ€é«˜æ£€ç´¢åˆ†æ•°"]:
                        stats["æœ€é«˜æ£€ç´¢åˆ†æ•°"] = score
            else:
                stats["æ— ç»“æœæ¬¡æ•°"] += 1
        
        elif log_type == "context_injection":
            stats["æ€»æ³¨å…¥æ¬¡æ•°"] += 1
    
    # è®¡ç®—å¹³å‡åˆ†æ•°
    if all_scores:
        stats["å¹³å‡æ£€ç´¢åˆ†æ•°"] = sum(all_scores) / len(all_scores)
    
    # æœ€å¸¸æŸ¥è¯¢ï¼ˆå‰5ï¼‰
    query_counter = Counter(queries)
    stats["æœ€å¸¸æŸ¥è¯¢"] = query_counter.most_common(5)
    
    return stats


def analyze_query_details(logs: List[Dict], limit: int = 10) -> List[Dict]:
    """åˆ†ææŸ¥è¯¢è¯¦æƒ…"""
    query_details = []
    
    # æŒ‰æŸ¥è¯¢åˆ†ç»„
    query_groups = defaultdict(list)
    for log in logs:
        if log.get("type") == "retrieval_start":
            query = log.get("query", "")
            query_groups[query].append(log)
    
    # åˆ†ææ¯ä¸ªæŸ¥è¯¢
    for query, start_logs in query_groups.items():
        detail = {
            "æŸ¥è¯¢": query,
            "æ£€ç´¢æ¬¡æ•°": len(start_logs),
            "é¦–æ¬¡æ—¶é—´": start_logs[0].get("timestamp", ""),
            "æ£€ç´¢ç»“æœ": [],
            "æ³¨å…¥æƒ…å†µ": []
        }
        
        # æŸ¥æ‰¾å¯¹åº”çš„æ£€ç´¢ç»“æœ
        for log in logs:
            if (log.get("type") == "retrieval_result" and 
                log.get("query") == query):
                nodes = log.get("nodes", [])
                if nodes:
                    detail["æ£€ç´¢ç»“æœ"] = nodes[:3]  # åªä¿ç•™å‰3ä¸ªç»“æœ
                    break
        
        # æŸ¥æ‰¾æ³¨å…¥æƒ…å†µ
        for log in logs:
            if log.get("type") == "context_injection":
                detail["æ³¨å…¥æƒ…å†µ"].append({
                    "æ³¨å…¥æ•°é‡": log.get("injected_count", 0),
                    "ä¸Šä¸‹æ–‡é•¿åº¦": log.get("context_length", 0),
                    "å¹³å‡åˆ†æ•°": log.get("average_score", 0)
                })
        
        query_details.append(detail)
    
    # æŒ‰æ£€ç´¢æ¬¡æ•°æ’åº
    query_details.sort(key=lambda x: x["æ£€ç´¢æ¬¡æ•°"], reverse=True)
    
    return query_details[:limit]


def print_summary_report(stats: Dict[str, Any]):
    """æ‰“å°æ±‡æ€»æŠ¥å‘Š"""
    print("\n" + "="*60)
    print("ğŸ” éšè—çŸ¥è¯†åº“æ¯æ—¥ç»Ÿè®¡æŠ¥å‘Š")
    print("="*60)
    
    print(f"\nğŸ“Š åŸºç¡€ç»Ÿè®¡:")
    print(f"  â€¢ æ€»æ£€ç´¢æ¬¡æ•°: {stats['æ€»æ£€ç´¢æ¬¡æ•°']}")
    print(f"  â€¢ æˆåŠŸæ£€ç´¢æ¬¡æ•°: {stats['æˆåŠŸæ£€ç´¢æ¬¡æ•°']}")
    print(f"  â€¢ æ— ç»“æœæ¬¡æ•°: {stats['æ— ç»“æœæ¬¡æ•°']}")
    print(f"  â€¢ æ€»æ³¨å…¥æ¬¡æ•°: {stats['æ€»æ³¨å…¥æ¬¡æ•°']}")
    
    if stats['æ€»æ£€ç´¢æ¬¡æ•°'] > 0:
        success_rate = (stats['æˆåŠŸæ£€ç´¢æ¬¡æ•°'] / stats['æ€»æ£€ç´¢æ¬¡æ•°']) * 100
        print(f"  â€¢ æ£€ç´¢æˆåŠŸç‡: {success_rate:.1f}%")
    
    print(f"\nğŸ“ˆ åˆ†æ•°ç»Ÿè®¡:")
    print(f"  â€¢ å¹³å‡æ£€ç´¢åˆ†æ•°: {stats['å¹³å‡æ£€ç´¢åˆ†æ•°']:.4f}")
    print(f"  â€¢ æœ€é«˜æ£€ç´¢åˆ†æ•°: {stats['æœ€é«˜æ£€ç´¢åˆ†æ•°']:.4f}")
    
    print(f"\nğŸ”¥ çƒ­é—¨æŸ¥è¯¢ (å‰5):")
    for i, (query, count) in enumerate(stats['æœ€å¸¸æŸ¥è¯¢'], 1):
        print(f"  {i}. {query[:50]}... ({count}æ¬¡)")
    
    print(f"\nâ° æ£€ç´¢æ—¶é—´åˆ†å¸ƒ:")
    for hour in sorted(stats['æ£€ç´¢æ—¶é—´åˆ†å¸ƒ'].keys()):
        count = stats['æ£€ç´¢æ—¶é—´åˆ†å¸ƒ'][hour]
        print(f"  â€¢ {hour:02d}:00-{hour:02d}:59: {count}æ¬¡")


def print_query_details(query_details: List[Dict]):
    """æ‰“å°æŸ¥è¯¢è¯¦æƒ…"""
    print("\n" + "="*60)
    print("ğŸ” æŸ¥è¯¢è¯¦æƒ…æŠ¥å‘Š (å‰10)")
    print("="*60)
    
    for i, detail in enumerate(query_details, 1):
        print(f"\n{i}. æŸ¥è¯¢: {detail['æŸ¥è¯¢']}")
        print(f"   æ£€ç´¢æ¬¡æ•°: {detail['æ£€ç´¢æ¬¡æ•°']}")
        print(f"   é¦–æ¬¡æ—¶é—´: {detail['é¦–æ¬¡æ—¶é—´']}")
        
        if detail['æ£€ç´¢ç»“æœ']:
            print(f"   æ£€ç´¢ç»“æœ:")
            for j, node in enumerate(detail['æ£€ç´¢ç»“æœ'], 1):
                score = node.get('score', 0)
                preview = node.get('content_preview', '')[:50]
                print(f"     {j}. åˆ†æ•°: {score:.4f} | å†…å®¹: {preview}...")
        else:
            print(f"   æ£€ç´¢ç»“æœ: æ— ")
        
        if detail['æ³¨å…¥æƒ…å†µ']:
            injection = detail['æ³¨å…¥æƒ…å†µ'][0]  # å–ç¬¬ä¸€æ¬¡æ³¨å…¥
            print(f"   æ³¨å…¥æƒ…å†µ: {injection['æ³¨å…¥æ•°é‡']}æ¡ | "
                  f"ä¸Šä¸‹æ–‡: {injection['ä¸Šä¸‹æ–‡é•¿åº¦']}å­—ç¬¦ | "
                  f"å¹³å‡åˆ†: {injection['å¹³å‡åˆ†æ•°']:.4f}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="åˆ†æéšè—çŸ¥è¯†åº“æ—¥å¿—")
    parser.add_argument("--date", type=str, help="æŒ‡å®šæ—¥æœŸ (YYYY-MM-DD)ï¼Œé»˜è®¤ä»Šå¤©")
    parser.add_argument("--detail", action="store_true", help="æ˜¾ç¤ºè¯¦ç»†æŸ¥è¯¢ä¿¡æ¯")
    parser.add_argument("--json", action="store_true", help="è¾“å‡º JSON æ ¼å¼")
    
    args = parser.parse_args()
    
    # åŠ è½½æ—¥å¿—
    logs = load_json_logs(args.date)
    if not logs:
        return
    
    # åˆ†æç»Ÿè®¡
    stats = analyze_daily_stats(logs)
    
    if args.json:
        print(json.dumps(stats, ensure_ascii=False, indent=2))
    else:
        print_summary_report(stats)
        
        if args.detail:
            query_details = analyze_query_details(logs)
            print_query_details(query_details)


if __name__ == "__main__":
    main()
