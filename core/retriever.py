# -*- coding: utf-8 -*-
"""
æ£€ç´¢å™¨æ¨¡å—
å®ç°æ··åˆæ£€ç´¢ï¼ˆBM25 + å‘é‡æ£€ç´¢ + RRF èåˆï¼‰
"""
import jieba
import os
from typing import List
from llama_index.core import Document, QueryBundle
from llama_index.core.retrievers import AutoMergingRetriever, BaseRetriever
from llama_index.core.schema import NodeWithScore, TextNode
from llama_index.core import VectorStoreIndex
from llama_index.retrievers.bm25 import BM25Retriever as OfficialBM25
from utils.logger import logger
from utils.keyword_ranker import keyword_ranker

# åŠ è½½è‡ªå®šä¹‰è¯å…¸ï¼ˆä¿ç•™é»˜è®¤è¯å…¸ï¼Œåªå¢å¼ºè‡ªå®šä¹‰è¯ï¼‰
CUSTOM_DICT_PATH = os.path.join(os.path.dirname(os.path.dirname(__file__)), "dict", "custom_dict.txt")
CUSTOM_WORDS_SET = set()  # å…¨å±€å˜é‡ï¼Œç”¨äºæ ‡è®°è‡ªå®šä¹‰è¯

if os.path.exists(CUSTOM_DICT_PATH):
    # ä½¿ç”¨ jieba.load_userdict åŠ è½½è‡ªå®šä¹‰è¯å…¸ï¼ˆä¿ç•™é»˜è®¤è¯å…¸ï¼‰
    jieba.load_userdict(CUSTOM_DICT_PATH)
    
    # æå–è‡ªå®šä¹‰è¯å…¸çš„è¯å’Œæƒé‡
    custom_words = {}
    line_count = 0
    empty_lines = 0
    comment_lines = 0
    
    with open(CUSTOM_DICT_PATH, 'r', encoding='utf-8') as f:
        for line in f:
            line_count += 1
            line = line.strip()
            
            if not line:
                empty_lines += 1
                continue
                
            if line.startswith('#'):
                comment_lines += 1
                continue
            
            # è§£æè¯å’Œæƒé‡ï¼šæ ¼å¼ä¸º "è¯ æƒé‡ è¯æ€§" æˆ– "è¯\tæƒé‡\tè¯æ€§"
            parts = line.split()
            if len(parts) >= 2:
                word = parts[0]
                try:
                    freq = int(parts[1])
                    custom_words[word] = freq
                    CUSTOM_WORDS_SET.add(word)
                except ValueError:
                    custom_words[word] = 100000
                    CUSTOM_WORDS_SET.add(word)
                    logger.warning(f"ç¬¬ {line_count} è¡Œæƒé‡è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: '{line}'")
            elif len(parts) == 1:
                custom_words[parts[0]] = 100000
                CUSTOM_WORDS_SET.add(parts[0])
            else:
                logger.warning(f"ç¬¬ {line_count} è¡Œè§£æå¤±è´¥: '{line}'")
    
    # å¯¹è‡ªå®šä¹‰è¯èµ‹äºˆæ›´é«˜æƒé‡ï¼ˆä¸æ¸…ç©ºé»˜è®¤è¯å…¸ï¼‰
    for word, freq in custom_words.items():
        jieba.dt.FREQ[word] = freq
    
    # é‡æ–°è®¡ç®—æ€»è¯é¢‘
    jieba.dt.total = sum(jieba.dt.FREQ.values())
    
    logger.info(f"âœ… å·²åŠ è½½è‡ªå®šä¹‰è¯å…¸ï¼ˆä¿ç•™é»˜è®¤è¯å…¸ï¼‰: {CUSTOM_DICT_PATH}")
    logger.info(f"ğŸ“Š è¯å…¸ç»Ÿè®¡: æ€»è¡Œæ•°={line_count}, ç©ºè¡Œ={empty_lines}, æ³¨é‡Šè¡Œ={comment_lines}")
    logger.info(f"âœ… è‡ªå®šä¹‰è¯æ¡æ•°: {len(custom_words)}")
    logger.info(f"âœ… jieba æ€»è¯æ¡æ•°: {len(jieba.dt.FREQ)}")
    logger.info(f"âœ… è‡ªå®šä¹‰è¯ç¤ºä¾‹ï¼ˆå‰10ä¸ªï¼‰: {list(custom_words.keys())[:10]}")
else:
    logger.warning(f"âš ï¸ è‡ªå®šä¹‰è¯å…¸ä¸å­˜åœ¨: {CUSTOM_DICT_PATH}")


class CleanBM25Retriever(BaseRetriever):
    """æ¸…ç†åçš„ BM25 æ£€ç´¢å™¨ï¼ˆä½¿ç”¨ jieba åˆ†è¯ï¼‰"""

    def __init__(self, nodes: List[TextNode], similarity_top_k: int = 2):
        self._id_to_original_node = {node.node_id: node for node in nodes}

        # ä½¿ç”¨ jieba åˆ†è¯ï¼Œå¹¶è¿‡æ»¤å¼‚å¸¸èŠ‚ç‚¹
        tokenized_corpus = []
        valid_nodes = []
        
        for node in nodes:
            # è·å–èŠ‚ç‚¹å†…å®¹
            content = node.get_content() if hasattr(node, 'get_content') else (node.text or "")
            
            # éªŒè¯å†…å®¹æ˜¯å¦æœ‰æ•ˆï¼ˆä¸æ˜¯JSONæ ¼å¼çš„å…ƒæ•°æ®ï¼‰
            # æ£€æŸ¥æ˜¯å¦æ˜¯ JSON åºåˆ—åŒ–çš„èŠ‚ç‚¹å¯¹è±¡
            content_stripped = content.strip()
            is_json_node = (
                content_stripped.startswith('{"id_"') or 
                content_stripped.startswith('{"class_name"') or
                (content_stripped.startswith('{') and '"text":' in content_stripped and '"metadata":' in content_stripped)
            )
            
            if not content or is_json_node:
                logger.warning(f"è·³è¿‡å¼‚å¸¸èŠ‚ç‚¹ {node.node_id[:8]}...: å†…å®¹ä¸ºç©ºæˆ–ä¸ºå…ƒæ•°æ®æ ¼å¼")
                logger.debug(f"  å†…å®¹é¢„è§ˆ: {content[:100]}...")
                continue
            
            # åˆ†è¯ï¼Œè¿‡æ»¤å•å­—å³å¯ï¼ˆä¿è¯ç´¢å¼•å®Œæ•´æ€§ï¼‰
            all_tokens = jieba.lcut(content)
            filtered_tokens = [token for token in all_tokens if len(token) > 1]
            tokenized_text = " ".join(filtered_tokens)
            
            # è°ƒè¯•ï¼šè®°å½•ç¬¬ä¸€ä¸ªèŠ‚ç‚¹çš„åˆ†è¯æƒ…å†µ
            if len(valid_nodes) == 0:
                logger.info(f"[BM25ç´¢å¼•æ„å»º-ç¤ºä¾‹] åŸå§‹tokensæ•°: {len(all_tokens)}, è¿‡æ»¤å: {len(filtered_tokens)}")
                logger.info(f"[BM25ç´¢å¼•æ„å»º-ç¤ºä¾‹] è¿‡æ»¤åtokensç¤ºä¾‹: {filtered_tokens[:20]}")  # åªæ˜¾ç¤ºå‰20ä¸ª
            
            tokenized_corpus.append(tokenized_text)
            valid_nodes.append(node)
        
        # æ›´æ–°æ˜ å°„ï¼ŒåªåŒ…å«æœ‰æ•ˆèŠ‚ç‚¹
        self._id_to_original_node = {node.node_id: node for node in valid_nodes}
        
        logger.info(f"BM25æ£€ç´¢å™¨åˆå§‹åŒ–: æ€»èŠ‚ç‚¹{len(nodes)}ä¸ª, æœ‰æ•ˆèŠ‚ç‚¹{len(valid_nodes)}ä¸ª, è·³è¿‡{len(nodes)-len(valid_nodes)}ä¸ªå¼‚å¸¸èŠ‚ç‚¹")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆèŠ‚ç‚¹
        if len(valid_nodes) == 0:
            logger.error("âŒ æ‰€æœ‰èŠ‚ç‚¹éƒ½æ— æ•ˆï¼BM25æ£€ç´¢å™¨æ— æ³•åˆå§‹åŒ–")
            logger.error("è¯·æ£€æŸ¥ Qdrant ä¸­çš„æ•°æ®æ˜¯å¦æ­£ç¡®ï¼Œå¯èƒ½éœ€è¦é‡å»ºç´¢å¼•")
            raise ValueError(f"BM25æ£€ç´¢å™¨åˆå§‹åŒ–å¤±è´¥: {len(nodes)}ä¸ªèŠ‚ç‚¹å…¨éƒ¨æ— æ•ˆï¼Œè¯·é‡å»ºçŸ¥è¯†åº“ç´¢å¼•")
        
        tokenized_docs = [
            Document(text=text, id_=node.id_)
            for text, node in zip(tokenized_corpus, valid_nodes)
        ]

        self._bm25_retriever = OfficialBM25(
            nodes=tokenized_docs,
            similarity_top_k=similarity_top_k
        )
        super().__init__()

    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:
        """æ‰§è¡Œæ£€ç´¢"""
        # å¯¹æŸ¥è¯¢è¿›è¡Œåˆ†è¯
        all_keywords = jieba.lcut(query_bundle.query_str)
        
        # æ£€ç´¢é˜¶æ®µï¼šä½¿ç”¨åœç”¨è¯è¿‡æ»¤
        query_keywords_for_retrieval = keyword_ranker.filter_keywords(all_keywords)
        
     
        
        # â­ æ–°å¢ï¼šæ£€æŸ¥æ˜¯å¦è¿‡åº¦è¿‡æ»¤
        if len(query_keywords_for_retrieval) == 0:
            logger.warning(
                f"[BM25æ£€ç´¢-è­¦å‘Š] åœç”¨è¯è¿‡æ»¤åæŸ¥è¯¢ä¸ºç©ºï¼\n"
                f"  åŸå§‹æŸ¥è¯¢: {query_bundle.query_str}\n"
                f"  åŸå§‹åˆ†è¯: {all_keywords}\n"
                f"  å»ºè®®: æ£€æŸ¥åœç”¨è¯è¡¨æˆ–ä½¿ç”¨åŸå§‹åˆ†è¯"
            )
            # å›é€€åˆ°åŸå§‹åˆ†è¯ï¼ˆåªè¿‡æ»¤å•å­—ï¼‰
            query_keywords_for_retrieval = [kw for kw in all_keywords if len(kw) > 1]
          
        
        tokenized_query = " ".join(query_keywords_for_retrieval)
        tokenized_bundle = QueryBundle(query_str=tokenized_query)

        # æ£€ç´¢
        retrieved_nodes = self._bm25_retriever.retrieve(tokenized_bundle)
        
        # â­ æ–°å¢ï¼šè®°å½•æ£€ç´¢ç»“æœåˆ†æ•°
        if retrieved_nodes:
            bm25_scores = [f"{n.score:.4f}" for n in retrieved_nodes[:5]]
            logger.info(f"[BM25æ£€ç´¢-ç»“æœ] è¿”å› {len(retrieved_nodes)} ä¸ªèŠ‚ç‚¹ | Top5åˆ†æ•°: {', '.join(bm25_scores)}")
        else:
            logger.warning(f"[BM25æ£€ç´¢-ç»“æœ] æœªæ‰¾åˆ°ä»»ä½•åŒ¹é…èŠ‚ç‚¹")

        # æ›¿æ¢å›åŸå§‹èŠ‚ç‚¹ï¼Œå¹¶æ·»åŠ åŒ¹é…å…³é”®è¯ä¿¡æ¯
        clean_nodes = []
        for node_with_score in retrieved_nodes:
            original_node = self._id_to_original_node.get(
                node_with_score.node.node_id
            )
            if original_node:
                # æ‰¾å‡ºæ–‡æ¡£ä¸­åŒ¹é…çš„å…³é”®è¯ï¼ˆä½¿ç”¨æ‰€æœ‰æ£€ç´¢å…³é”®è¯ï¼‰
                doc_content = original_node.get_content() if hasattr(original_node, 'get_content') else (original_node.text or "")
                matched_keywords_raw = [kw for kw in query_keywords_for_retrieval if kw in doc_content]
                
                # ä½¿ç”¨ keyword_ranker è¿‡æ»¤åœç”¨è¯ï¼ˆé»‘åå•ï¼‰
                matched_keywords = keyword_ranker.filter_keywords(matched_keywords_raw)
                
                
                # å°†åŒ¹é…çš„å…³é”®è¯æ·»åŠ åˆ°èŠ‚ç‚¹å…ƒæ•°æ®
                original_node.metadata['bm25_matched_keywords'] = matched_keywords
                original_node.metadata['bm25_query_keywords'] = query_keywords_for_retrieval
                
                # Add a new metadata field 'bm25_relevance_score'
                original_node.metadata['bm25_relevance_score'] = node_with_score.score
                
                clean_nodes.append(
                    NodeWithScore(node=original_node, score=node_with_score.score)
                )

        return clean_nodes


class HybridRetriever(BaseRetriever):
    """æ··åˆæ£€ç´¢å™¨ï¼ˆå‘é‡ + BM25 + RRF èåˆï¼‰"""

    def __init__(
        self,
        automerging_retriever: AutoMergingRetriever,
        bm25_retriever: CleanBM25Retriever,
        rrf_k: float = 60.0,
        vector_weight: float = 0.7,
        bm25_weight: float = 0.3
    ):
        self._automerging = automerging_retriever
        self._bm25 = bm25_retriever
        self._rrf_k = rrf_k
        self._vector_weight = vector_weight
        self._bm25_weight = bm25_weight
        super().__init__()

    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:
        """
        ä½¿ç”¨ Reciprocal Rank Fusion (RRF) ç®—æ³•èåˆæ£€ç´¢ç»“æœ

        Args:
            query_bundle: æŸ¥è¯¢å†…å®¹

        Returns:
            èåˆåçš„æ£€ç´¢ç»“æœ
        """
        # 1. åˆ†åˆ«æ‰§è¡Œä¸¤ç§æ£€ç´¢
        automerging_nodes = self._automerging.retrieve(query_bundle)
        bm25_nodes = self._bm25.retrieve(query_bundle)
        
        #  æ–°å¢ï¼šè®°å½•å‘é‡æ£€ç´¢ç»“æœ
        if automerging_nodes:
            vector_scores_display = [f"{n.score:.4f}" for n in automerging_nodes[:5]]
        else:
            logger.warning(f"[å‘é‡æ£€ç´¢-ç»“æœ] æœªæ‰¾åˆ°ä»»ä½•åŒ¹é…èŠ‚ç‚¹")

        # 2. æ”¶é›†æ‰€æœ‰å”¯ä¸€èŠ‚ç‚¹
        all_nodes = {n.node.node_id: n.node for n in automerging_nodes}
        all_nodes.update({n.node.node_id: n.node for n in bm25_nodes})

        # 3. è®¡ç®—æ’åå’ŒåŸå§‹åˆ†æ•°
        vector_ranks = {
            node.node.node_id: rank
            for rank, node in enumerate(automerging_nodes, 1)
        }
        bm25_ranks = {
            node.node.node_id: rank
            for rank, node in enumerate(bm25_nodes, 1)
        }
        vector_scores = {n.node.node_id: n.score for n in automerging_nodes}
        bm25_scores = {n.node.node_id: n.score for n in bm25_nodes}

        # 4. è®¡ç®—åŠ æƒ RRF åˆ†æ•°
        fused_scores = {}
        #  ä¿®å¤1: é™ä½å‘é‡åˆ†æ•°é˜ˆå€¼ï¼Œé¿å…è¿‡åº¦è¿‡æ»¤ï¼ˆä» 0.01 é™åˆ° 0.001ï¼‰
        vector_score_threshold = 0.001  # å‘é‡åˆ†æ•°é˜ˆå€¼ï¼Œä½äºæ­¤å€¼è§†ä¸ºæ— æ•ˆ
        bm25_only_count = 0  # ç»Ÿè®¡çº¯BM25ç»“æœæ•°é‡
        
        for node_id in all_nodes:
            score = 0.0
            vector_score = vector_scores.get(node_id, 0.0)
            bm25_score = bm25_scores.get(node_id, 0.0)
            
            # åˆ¤æ–­å‘é‡æ£€ç´¢æ˜¯å¦æœ‰æ•ˆï¼ˆåˆ†æ•° > é˜ˆå€¼ï¼‰
            vector_valid = node_id in vector_ranks and vector_score > vector_score_threshold
            bm25_valid = node_id in bm25_ranks
            
            #  ä¿®å¤2: æ”¹è¿›çº¯BM25ç»“æœçš„åˆ†æ•°è®¡ç®—ï¼Œä½¿ç”¨ RRF è€ŒéåŸå§‹åˆ†æ•°
            if not vector_valid and bm25_valid:
                # çº¯BM25ç»“æœï¼šä½¿ç”¨ RRF å…¬å¼è®¡ç®—ï¼Œç¡®ä¿åˆ†æ•°åœ¨åˆç†èŒƒå›´
                # ä½¿ç”¨ BM25 æ’åè®¡ç®— RRF åˆ†æ•°ï¼Œå¹¶ä¹˜ä»¥æƒé‡
                score = self._bm25_weight * (1.0 / (self._rrf_k + bm25_ranks[node_id]))
                # æ·»åŠ ä¸€ä¸ªåŸºç¡€åˆ†æ•°ï¼Œé¿å…åˆ†æ•°è¿‡ä½
                score = max(score, bm25_score * 0.1)  # è‡³å°‘ä¿ç•™ BM25 åˆ†æ•°çš„ 10%
                bm25_only_count += 1
            else:
                # æ ‡å‡†RRFèåˆ
                if vector_valid:
                    score += self._vector_weight * (1.0 / (self._rrf_k + vector_ranks[node_id]))
                if bm25_valid:
                    score += self._bm25_weight * (1.0 / (self._rrf_k + bm25_ranks[node_id]))
            
            fused_scores[node_id] = score
        
        # è®°å½•çº¯BM25ç»“æœç»Ÿè®¡
        if bm25_only_count > 0:
            logger.info(
                f"[RRFèåˆ] æ£€æµ‹åˆ° {bm25_only_count} ä¸ªçº¯BM25ç»“æœï¼ˆå‘é‡åˆ†æ•° < {vector_score_threshold}ï¼‰ï¼Œ"
                f"ä½¿ç”¨æ”¹è¿›çš„ RRF åˆ†æ•°è®¡ç®—"
            )

        # 5. æ„å»ºç»“æœå¹¶é™„åŠ å…ƒæ•°æ®
        fused_results = []
        for node_id, score in fused_scores.items():
            node_obj = all_nodes[node_id]
            vector_rank = vector_ranks.get(node_id)
            bm25_rank = bm25_ranks.get(node_id)
            sources = []
            if vector_rank is not None:
                sources.append("vector")
            if bm25_rank is not None:
                sources.append("keyword")

            node_obj.metadata['vector_score'] = vector_scores.get(node_id, 0.0)
            node_obj.metadata['bm25_score'] = bm25_scores.get(node_id, 0.0)
            node_obj.metadata['vector_rank'] = vector_rank
            node_obj.metadata['bm25_rank'] = bm25_rank
            node_obj.metadata['retrieval_sources'] = sources
            node_obj.metadata['initial_score'] = score

            fused_results.append(NodeWithScore(node=node_obj, score=score))

        # 6. æŒ‰ RRF åˆ†æ•°é™åºæ’åº
        sorted_results = sorted(
            fused_results,
            key=lambda x: x.score,
            reverse=True
        )

        return sorted_results


class RetrieverFactory:
    """æ£€ç´¢å™¨å·¥å‚"""

    @staticmethod
    def create_hybrid_retriever(
        index: VectorStoreIndex,
        all_nodes: List[TextNode],
        similarity_top_k: int,
        similarity_top_k_bm25: int
    ) -> HybridRetriever:
        """
        åˆ›å»ºæ··åˆæ£€ç´¢å™¨

        Args:
            index: å‘é‡ç´¢å¼•
            all_nodes: æ‰€æœ‰èŠ‚ç‚¹
            similarity_top_k: æ£€ç´¢æ•°é‡

        Returns:
            æ··åˆæ£€ç´¢å™¨å®ä¾‹
        """
        logger.info("åˆ›å»ºæ··åˆæ£€ç´¢å™¨ï¼ˆå‘é‡ + BM25 + RRFï¼‰...")

        # å‘é‡æ£€ç´¢å™¨
        vector_retriever = index.as_retriever(similarity_top_k=similarity_top_k)

        # è‡ªåŠ¨åˆå¹¶æ£€ç´¢å™¨
        automerging_retriever = AutoMergingRetriever(
            vector_retriever,
            index.storage_context,
            verbose=False
        )

        # BM25 æ£€ç´¢å™¨
        bm25_retriever = CleanBM25Retriever(
            all_nodes,
            similarity_top_k=similarity_top_k_bm25
        )

        # æ··åˆæ£€ç´¢å™¨ï¼ˆä½¿ç”¨é…ç½®çš„æƒé‡ï¼‰
        from config.settings import Settings as AppSettings
        return HybridRetriever(
            automerging_retriever, 
            bm25_retriever,
            rrf_k=AppSettings.RRF_K,
            vector_weight=AppSettings.RRF_VECTOR_WEIGHT,
            bm25_weight=AppSettings.RRF_BM25_WEIGHT
        )

